{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83897, 18)\n",
      "(47516, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pandas.read_parquet('../data/to_send.pq')\n",
    "print(data.shape)\n",
    "data = data[['description', 'products']][pandas.notnull(data['products'])].copy().reset_index(drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Welch allyn combines its practical understandi...</td>\n",
       "      <td>[power supply, body sub assy, medical, valve b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In  line  with  the  company  s intention  to ...</td>\n",
       "      <td>[imo, advertising materials, point, imo label,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Services redaelli ricambi offers the ability t...</td>\n",
       "      <td>[auto spare parts, tie rod, tie rod end, auto ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STROTHMANN not only delivers suitable mechanic...</td>\n",
       "      <td>[covers non automated, demurrage rules form, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Established\\nin 1991, tien jiang enterprise co...</td>\n",
       "      <td>[rubber, polyester, nylon, boot, support]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  Welch allyn combines its practical understandi...   \n",
       "1  In  line  with  the  company  s intention  to ...   \n",
       "2  Services redaelli ricambi offers the ability t...   \n",
       "3  STROTHMANN not only delivers suitable mechanic...   \n",
       "4  Established\\nin 1991, tien jiang enterprise co...   \n",
       "\n",
       "                                            products  \n",
       "0  [power supply, body sub assy, medical, valve b...  \n",
       "1  [imo, advertising materials, point, imo label,...  \n",
       "2  [auto spare parts, tie rod, tie rod end, auto ...  \n",
       "3  [covers non automated, demurrage rules form, r...  \n",
       "4          [rubber, polyester, nylon, boot, support]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Welch allyn combines its practical understanding of clinical needs with its visionary spirit to develop solutions that assess, diagnose, treat, and manage a variety of illnesses and diseases.as a leading global manufacturer of medical diagnostic equipment, we offer a range of connected solutions. with nearly 2,500 employees working in 26 countries, we focus on the customer and imagine how healthcare will be delivered in the future to develop tools and future-proof technologies.our professional customers include physicians’ practices, community clinics, skilled nursing facilities, and emergency departments—places where 95% of patients first seek medical treatment.our welch allyn home division delivers solutions that make home monitoring of blood pressure readings accurate and easy. with the same attention to quality that we offer to our professional customers, we’re now bringing physician-trusted solutions to the home, helping people better manage their health.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['power supply', 'body sub assy', 'medical', 'valve body sub assy',\n",
       "       'digital blood pressure'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0]['products']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique categories 71111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35442, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "\n",
    "data['products'] = data['products'].apply(lambda x: np.array([' '.join(tokenizer.tokenize(product)).lower()\n",
    "                                                              for product in x.tolist()]))\n",
    "\n",
    "all_products = []\n",
    "for prod_list in data['products'][pandas.notnull(data['products'])].values:\n",
    "    all_products += [' '.join(tokenizer.tokenize(product))\n",
    "                     for product in prod_list.tolist()]\n",
    "    \n",
    "counter = Counter(all_products)\n",
    "print('unique categories', len(counter.most_common()))\n",
    "\n",
    "most_common = [product[0] for product in counter.most_common(600)]\n",
    "\n",
    "def filter_categories(x):\n",
    "    new_categories = np.array([product \n",
    "                               for product in x.tolist()\n",
    "                               if product in most_common])\n",
    "    if new_categories.shape[0] == 0:\n",
    "        return np.nan\n",
    "    return new_categories\n",
    "\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "stopwords_cached = stopwords.words('english')\n",
    "\n",
    "def filter_descriptions(text):\n",
    "    cleaned_text = [token#lemma.lemmatize(token)\n",
    "                      for token in tokenizer.tokenize(text.lower())\n",
    "                      if token not in stopwords_cached]\n",
    "    if len(cleaned_text) == 0:\n",
    "        return np.nan\n",
    "    return ' '.join(cleaned_text)\n",
    "\n",
    "data['products'] = data['products'].apply(filter_categories)\n",
    "data['description'] = data['description'].apply(filter_descriptions)\n",
    "\n",
    "cleaned_data = data[(data['products'].notnull()) & (data['description'].notnull())].copy().reset_index(drop=True)\n",
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welch allyn combines practical understanding c...</td>\n",
       "      <td>[power supply, medical]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>line company intention support international g...</td>\n",
       "      <td>[imo, point, marine pollutant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>services redaelli ricambi offers ability produ...</td>\n",
       "      <td>[auto spare parts, auto spare, spare parts]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strothmann delivers suitable mechanical system...</td>\n",
       "      <td>[line]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>established tien jiang enterprise co ltd one s...</td>\n",
       "      <td>[rubber, polyester, nylon, support]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  welch allyn combines practical understanding c...   \n",
       "1  line company intention support international g...   \n",
       "2  services redaelli ricambi offers ability produ...   \n",
       "3  strothmann delivers suitable mechanical system...   \n",
       "4  established tien jiang enterprise co ltd one s...   \n",
       "\n",
       "                                      products  \n",
       "0                      [power supply, medical]  \n",
       "1               [imo, point, marine pollutant]  \n",
       "2  [auto spare parts, auto spare, spare parts]  \n",
       "3                                       [line]  \n",
       "4          [rubber, polyester, nylon, support]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers.fasttext import FastText\n",
    "\n",
    "model = FastText.load_fasttext_format('../models/wiki.simple.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in word zc\n",
      "error in word zc\n",
      "error in word vz\n",
      "error in word vz\n",
      "error in word vz\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer = RegexpTokenizer(r'[a-z]+')\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "stopwords_cached = stopwords.words('english')\n",
    "\n",
    "def get_embedding_matrix(text, first_n=50):\n",
    "#     print(text)\n",
    "    matrix = []\n",
    "    text = [token#lemma.lemmatize(token)\n",
    "            for token in tokenizer.tokenize(text)]\n",
    "    for word in text[:first_n]:\n",
    "        try:\n",
    "            word_embedding = model.wv[word]\n",
    "            matrix.append(model.wv[word])\n",
    "        except KeyError:\n",
    "            print(\"error in word\", word)\n",
    "            matrix.append(np.zeros(300))\n",
    "    \n",
    "    matrix = np.array(matrix)\n",
    "    # fill text embeddings with seq_len < first_n with zeros\n",
    "#     print(matrix.shape[0])\n",
    "    if matrix.shape[0] < first_n:\n",
    "        matrix = np.vstack((matrix, np.zeros((first_n - matrix.shape[0], 300))))\n",
    "    return matrix\n",
    "\n",
    "embeddings = []\n",
    "for text in cleaned_data['description'].values:\n",
    "    embeddings.append(get_embedding_matrix(text))\n",
    "#     print(embeddings[-1].shape)\n",
    "    \n",
    "# onehot_y_df = pandas.get_dummies(data['severity'])\n",
    "\n",
    "train_data = np.array(embeddings)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([item.tolist() for item in cleaned_data['products'].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBatcher():\n",
    "    def __init__(self, X_, y_, batch_size_=30):\n",
    "        self.X_ = X_\n",
    "        self.y_ = y_\n",
    "        self.batch_size_ = batch_size_\n",
    "        self.from_ = 0\n",
    "        self.to = self.from_ + self.batch_size_\n",
    "\n",
    "    def next_batch(self):\n",
    "        if self.to == len(self.X_):\n",
    "            f = self.from_\n",
    "            t = self.to\n",
    "            self.from_ = 0\n",
    "            self.to = self.to = self.from_ + self.batch_size_\n",
    "            return self.X_[f:t], self.y_[f:t]\n",
    "        elif self.to > len(self.X_):\n",
    "            f = self.from_\n",
    "            t = self.to - len(self.X_)\n",
    "            self.from_ = self.to - len(self.X_)\n",
    "            self.to = self.from_ + self.batch_size_\n",
    "            return np.append(self.X_[f:], self.X_[:t], axis=0), np.append(self.y_[f:], self.y_[:t], axis=0)\n",
    "        else:\n",
    "            f = self.from_\n",
    "            t = self.to\n",
    "            self.from_ = self.to\n",
    "            self.to = self.from_ + self.batch_size_\n",
    "            return self.X_[f:t], self.y_[f:t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss= 2.7806, Train Acc= 0.004, Test Acc = 0.002\n",
      "Step 500, Loss= 0.5469, Train Acc= 0.006, Test Acc = 0.005\n",
      "Step 1000, Loss= 0.3122, Train Acc= 0.003, Test Acc = 0.004\n",
      "Step 1500, Loss= 0.1509, Train Acc= 0.003, Test Acc = 0.004\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Training Parameters\n",
    "    learning_rate = 0.0001\n",
    "    training_steps = 150000\n",
    "    batch_size = 80\n",
    "    display_step = 1000\n",
    "    checkpoint_step = 20000\n",
    "    \n",
    "    # Network Parameters\n",
    "    num_input = 300 # MNIST data input (img shape: 28*28)\n",
    "    timesteps = 50 # timesteps\n",
    "    num_hidden = 128 # hidden layer num of features\n",
    "    num_classes = 600 # MNIST total classes (0-9 digits)\n",
    "    num_layers = 2\n",
    "    input_keep_prob = 0.75\n",
    "    output_keep_prob = 0.75\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([2 * num_hidden, num_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "    }\n",
    "\n",
    "    def RNN(x, weights, biases):\n",
    "\n",
    "        # Prepare data shape to match `rnn` function requirements\n",
    "        # Current data input shape: (batch_size, timesteps, n_input)\n",
    "        # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "        # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "        x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "        # Cerate forward and backward cells\n",
    "        cells_fw = [cell(num_hidden, activation=tf.sigmoid) for _ in range(num_layers)]\n",
    "        cells_bw = [cell(num_hidden, activation=tf.sigmoid) for _ in range(num_layers)]\n",
    "        \n",
    "        # Add dropout\n",
    "        cells_fw = [tf.contrib.rnn.DropoutWrapper(cell, \n",
    "                                                  input_keep_prob=input_keep_prob,\n",
    "                                                  output_keep_prob=output_keep_prob\n",
    "                                                 ) for cell in cells_fw]\n",
    "        cells_bw = [tf.contrib.rnn.DropoutWrapper(cell, \n",
    "                                                  input_keep_prob=input_keep_prob,\n",
    "                                                  output_keep_prob=output_keep_prob\n",
    "                                                 ) for cell in cells_bw]\n",
    "        \n",
    "        outputs, _, _ = tf.contrib.rnn.stack_bidirectional_rnn(\n",
    "            cells_fw=cells_fw,\n",
    "            cells_bw=cells_bw,\n",
    "            inputs=x,\n",
    "            dtype=tf.float32)\n",
    "        \n",
    "#         rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "        \n",
    "        return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "        \n",
    "\n",
    "    logits = RNN(X, weights, biases)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    \n",
    "\n",
    "# tf.nn.sigmoid_cross_entropy_with_logits \n",
    "#     loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "#         logits=logits, labels=Y))\n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=logits, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    # Evaluate model (with test logits, for dropout to be disabled)\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "def evaluate_multilabel(y_pred, y_true):\n",
    "    acc = []\n",
    "    for y_pred_tmp, y_true_tmp in zip(y_pred, y_true):\n",
    "#         print('y_true_tmp', y_true_tmp, 'y_pred_tmp', y_pred_tmp)\n",
    "        real = np.nonzero(y_true_tmp)[0].tolist()\n",
    "        right_num = len(real)\n",
    "        pred = np.argpartition(y_pred_tmp, -right_num)[-right_num:]\n",
    "#         print('real', real, 'pred', pred)\n",
    "        if len(real) == 0:\n",
    "            #means 0 right answers\n",
    "            acc.append(0.0)\n",
    "            continue\n",
    "        acc.append(len(set(real).intersection(set(pred))) / len(real))\n",
    "#     print(acc)\n",
    "    return(np.array(acc).mean())\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4)\n",
    "sess = tf.InteractiveSession(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "# with tf.Session(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "batcher = DataBatcher(X_train, y_train, batch_size_=batch_size)\n",
    "\n",
    "for step in range(1, training_steps+1):\n",
    "    batch_x, batch_y = batcher.next_batch()\n",
    "    # Reshape data to get 28 seq of 28 elements\n",
    "#         batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "    # Run optimization op (backprop)\n",
    "    sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        # Calculate batch loss and accuracy\n",
    "        loss, pred = sess.run([loss_op, prediction], feed_dict={X: batch_x, Y: batch_y})\n",
    "        \n",
    "        test_out = sess.run(prediction, feed_dict={X: X_test})\n",
    "        print(\"Step \" + str(step) + \", Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss) + \", Train Acc= \" + \\\n",
    "              \"{:.3f}\".format(evaluate_multilabel(y_pred=pred, y_true=batch_y)) +  \", Test Acc = \" + \\\n",
    "              \"{:.3f}\".format(evaluate_multilabel(y_pred=test_out, y_true=y_test)))\n",
    "        \n",
    "    if step % checkpoint_step == 0:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, '../models/lstm_good/lstm_good', global_step=step)\n",
    "        \n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "#     # Calculate accuracy for 128 mnist test images\n",
    "#     print(\"Testing Accuracy:\", \\\n",
    "#         sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.18454562e-03, 1.15506053e-02, 2.30523201e-05, 3.32070363e-06,\n",
       "        1.50353546e-04, 1.20563403e-04, 3.98125849e-06, 8.15389285e-05,\n",
       "        2.25681234e-07, 9.43000373e-07, 1.05230291e-04, 4.86507815e-07,\n",
       "        6.37763333e-06, 3.74643096e-05, 3.62851188e-06, 5.10727914e-05,\n",
       "        2.91291758e-06, 1.54119061e-05, 1.75035275e-05, 1.34396760e-06,\n",
       "        3.55263023e-06, 2.84224562e-03, 3.24832101e-04, 7.74649307e-05,\n",
       "        1.01956475e-05, 4.77190333e-05, 4.19327553e-05, 4.11308829e-05,\n",
       "        1.23437552e-04, 2.74339000e-05, 6.19934872e-06, 3.74953379e-05,\n",
       "        4.49305105e-07, 7.40281030e-05, 7.03206868e-04, 1.16717001e-05,\n",
       "        1.17967669e-04, 2.97253905e-07, 3.12561133e-06, 5.46272622e-06,\n",
       "        8.61753833e-08, 2.12355371e-05, 6.49193581e-03, 3.89328704e-07,\n",
       "        2.87322928e-06, 4.24616746e-06, 1.85999197e-05, 7.84599222e-04,\n",
       "        3.65358716e-07, 1.68811653e-06, 2.68506426e-08, 3.66821109e-06,\n",
       "        9.85488441e-06, 2.47749158e-05, 2.33883708e-04, 4.11713045e-05,\n",
       "        3.15119069e-05, 7.19734680e-06, 1.90818158e-04, 4.05682513e-04,\n",
       "        5.67975221e-04, 4.97710921e-08, 6.99112934e-05, 1.51146858e-04,\n",
       "        2.63549131e-03, 4.25489270e-05, 1.50645399e-04, 1.74920376e-06,\n",
       "        7.88468984e-04, 1.15833272e-08, 1.87605474e-04, 1.03645348e-07,\n",
       "        1.23906371e-04, 2.18247573e-04, 4.25257487e-03, 4.33089263e-05,\n",
       "        3.65477608e-05, 7.30887259e-05, 7.03117030e-06, 1.46650025e-04,\n",
       "        7.75878143e-05, 2.10935195e-05, 4.94449637e-07, 5.18211380e-07,\n",
       "        1.36304952e-04, 7.18755882e-06, 3.04571516e-03, 2.95487530e-06,\n",
       "        9.98564879e-04, 2.57357588e-05, 2.79632630e-04, 8.02006014e-03,\n",
       "        2.48557467e-06, 1.13825754e-05, 6.53982624e-07, 1.72199707e-05,\n",
       "        2.25741633e-06, 4.38471744e-03, 2.37177274e-11, 1.21708181e-05,\n",
       "        6.58698625e-08, 1.44632125e-07, 4.09868626e-05, 5.17148237e-06,\n",
       "        9.10169604e-07, 3.26967012e-04, 3.98657812e-06, 3.35560322e-01,\n",
       "        1.23594089e-06, 1.04759838e-05, 5.24085008e-05, 2.91723381e-07,\n",
       "        4.50313837e-06, 4.64572920e-04, 2.13640561e-07, 1.50754713e-04,\n",
       "        6.05129935e-05, 3.45855195e-04, 7.43396640e-07, 9.59262343e-06,\n",
       "        1.78075599e-04, 2.25128373e-03, 1.77990069e-06, 1.68320389e-06,\n",
       "        2.25628119e-05, 8.95803225e-07, 1.95567118e-05, 2.61522969e-03,\n",
       "        5.20401409e-05, 1.22481561e-05, 4.53976418e-06, 1.71287189e-04,\n",
       "        8.83224374e-08, 2.66849298e-07, 3.19213910e-07, 1.63472054e-04,\n",
       "        1.95240827e-05, 7.77480238e-07, 2.27225813e-04, 2.36150867e-04,\n",
       "        9.16657591e-05, 1.11546989e-07, 1.07006086e-02, 6.18725608e-05,\n",
       "        2.02252679e-07, 1.04241808e-05, 8.46806003e-08, 2.74278045e-05,\n",
       "        1.60522177e-05, 1.83873883e-04, 1.27798930e-05, 2.29320387e-04,\n",
       "        1.19272436e-05, 5.44571435e-07, 1.80086026e-05, 5.09576312e-05,\n",
       "        8.85520967e-06, 9.83215796e-06, 1.53478308e-04, 7.07986910e-05,\n",
       "        2.28706558e-05, 3.38608443e-05, 8.90417839e-04, 1.91692811e-06,\n",
       "        1.00828720e-05, 5.53695543e-07, 6.34095108e-04, 5.73730387e-04,\n",
       "        1.19395118e-05, 4.11851070e-04, 7.46213686e-07, 4.60682685e-07,\n",
       "        1.22401747e-03, 3.48575231e-05, 2.87149533e-06, 4.73642349e-02,\n",
       "        1.85705296e-09, 7.90301710e-05, 4.66254889e-04, 1.81304531e-05,\n",
       "        6.02224803e-09, 5.40192937e-04, 1.06921843e-05, 6.12800359e-05,\n",
       "        1.80947842e-04, 9.92978039e-06, 2.97683691e-05, 3.75703053e-06,\n",
       "        2.28401320e-03, 4.53013456e-07, 4.41062148e-05, 2.12479222e-06,\n",
       "        6.20611652e-04, 1.06240832e-05, 6.77069556e-06, 2.24421942e-03,\n",
       "        4.36096743e-04, 1.33991455e-06, 1.72978581e-03, 8.64467365e-05,\n",
       "        9.83293517e-04, 8.58428166e-07, 2.53510716e-05, 5.26223971e-07,\n",
       "        1.15740359e-05, 1.64148957e-03, 4.51483902e-06, 7.51888956e-06,\n",
       "        1.71377042e-06, 3.32263974e-03, 1.59914343e-05, 1.83094759e-03,\n",
       "        5.35170102e-06, 4.81233428e-07, 1.31013376e-05, 8.88541854e-06,\n",
       "        9.45459236e-04, 5.03131162e-07, 2.25893469e-04, 7.66128069e-03,\n",
       "        4.42822142e-07, 7.94746593e-05, 9.07913927e-05, 3.57478918e-08,\n",
       "        3.84311752e-05, 6.28019823e-03, 3.66264780e-04, 5.28735458e-04,\n",
       "        1.81109681e-06, 8.32134930e-08, 1.24244489e-05, 2.35232528e-06,\n",
       "        8.31802172e-05, 3.14551662e-03, 2.84019834e-03, 2.60198590e-07,\n",
       "        2.32851698e-06, 2.92004515e-05, 1.83607099e-05, 1.38736516e-06,\n",
       "        7.90298684e-04, 4.92019951e-03, 1.01507430e-06, 1.68509505e-05,\n",
       "        3.03391629e-04, 5.87211922e-04, 3.45128865e-05, 6.97674523e-08,\n",
       "        1.16735384e-04, 3.12037999e-03, 1.13332353e-05, 1.01611931e-06,\n",
       "        2.90891603e-05, 1.66653411e-03, 3.05011436e-05, 5.46723641e-07,\n",
       "        4.96781297e-07, 3.41306149e-05, 4.02258192e-05, 4.23222346e-08,\n",
       "        4.60092991e-07, 6.48338813e-04, 8.83732820e-09, 3.84233908e-06,\n",
       "        7.45829311e-04, 2.89925424e-06, 2.65106974e-05, 1.04940627e-02,\n",
       "        8.04079173e-04, 1.14985625e-07, 5.44382026e-03, 1.85782881e-07,\n",
       "        4.61508262e-06, 5.81161294e-05, 8.45620962e-05, 3.05135240e-04,\n",
       "        4.86604279e-07, 4.30428481e-04, 1.09563325e-05, 3.60031445e-05,\n",
       "        4.97545348e-03, 1.80721645e-06, 2.09287915e-04, 4.44719008e-05,\n",
       "        1.81453890e-06, 2.76788294e-01, 1.34984555e-06, 5.16396676e-06,\n",
       "        9.13949307e-08, 8.38299748e-05, 4.66474594e-05, 6.58037316e-05,\n",
       "        7.60558614e-05, 1.16179566e-04, 4.71417025e-06, 1.56384851e-06,\n",
       "        3.10330979e-05, 1.14526927e-06, 3.67844550e-05, 9.60855505e-06,\n",
       "        3.56358032e-06, 2.09286038e-04, 2.50901678e-04, 1.09441660e-03,\n",
       "        1.65686700e-07, 1.13029004e-04, 1.10299292e-03, 2.22420786e-04,\n",
       "        5.02039512e-08, 1.71236868e-04, 8.61655280e-06, 5.18010324e-07,\n",
       "        3.27733287e-05, 2.54559109e-06, 2.39598776e-05, 7.52731194e-06,\n",
       "        8.76770355e-05, 1.81445131e-07, 5.98458755e-05, 1.37008901e-04,\n",
       "        5.44646580e-04, 6.37553467e-06, 4.93068819e-06, 3.34581164e-05,\n",
       "        1.45354701e-04, 5.27333486e-06, 2.53863236e-05, 2.06204280e-08,\n",
       "        5.58454894e-06, 5.64561342e-05, 1.79611345e-06, 5.48083881e-06,\n",
       "        4.68616199e-05, 6.72923974e-08, 1.32120724e-04, 1.07669842e-03,\n",
       "        2.84823636e-06, 5.42053836e-04, 2.51570873e-06, 9.14940472e-07,\n",
       "        8.61314358e-04, 2.27138935e-05, 5.77712563e-05, 2.90174829e-03,\n",
       "        2.06134882e-06, 5.12555707e-04, 3.94195922e-06, 8.33340891e-07,\n",
       "        7.17129093e-04, 3.61278057e-06, 6.60005071e-07, 1.00959347e-04,\n",
       "        1.03402533e-06, 6.31950214e-04, 1.50136257e-04, 2.93862672e-06,\n",
       "        1.09543318e-04, 4.57299257e-06, 9.21474784e-05, 9.27448269e-08,\n",
       "        1.57313087e-04, 6.75356190e-04, 7.46060721e-07, 1.73843972e-07,\n",
       "        2.51331803e-04, 3.56579875e-03, 2.71794352e-06, 4.09021668e-05,\n",
       "        7.18425945e-05, 1.29789805e-05, 1.45294145e-03, 1.28949605e-05,\n",
       "        3.32055919e-07, 8.89807561e-05, 4.52472023e-07, 8.30620644e-04,\n",
       "        4.45244723e-07, 1.50984772e-06, 6.54674581e-09, 8.44590948e-04,\n",
       "        1.07897513e-05, 5.46183623e-03, 1.88482738e-06, 5.50994673e-06,\n",
       "        1.06425650e-04, 4.27046007e-06, 1.84901786e-04, 1.19767049e-06,\n",
       "        8.22947044e-08, 8.82121240e-05, 1.56860228e-03, 1.47125189e-04,\n",
       "        1.94328558e-02, 3.90531886e-06, 6.59176658e-05, 2.83021632e-06,\n",
       "        1.51144544e-04, 6.17536261e-06, 2.06548830e-05, 1.19220458e-04,\n",
       "        3.29905655e-04, 9.06038167e-06, 9.25111635e-06, 5.00326836e-03,\n",
       "        2.89340573e-03, 2.12173546e-07, 5.91476346e-05, 8.19092566e-06,\n",
       "        2.69120915e-06, 1.47944986e-04, 1.74848514e-03, 1.94066206e-06,\n",
       "        1.05513886e-04, 8.29291705e-04, 1.52636304e-07, 2.40066976e-04,\n",
       "        2.60764617e-04, 6.63823521e-05, 2.47494700e-06, 4.29728352e-05,\n",
       "        4.45592887e-04, 9.45058946e-06, 4.23887695e-06, 7.46272854e-05,\n",
       "        1.12042026e-05, 7.00431855e-08, 1.46189441e-05, 9.33233940e-04,\n",
       "        1.13406641e-04, 7.66330022e-09, 3.43855419e-07, 9.95877713e-07,\n",
       "        4.12214717e-07, 1.01764326e-05, 6.77569176e-07, 1.27794538e-05,\n",
       "        1.45068925e-05, 7.89897001e-07, 1.57228351e-05, 1.96738067e-04,\n",
       "        4.67109612e-06, 5.86536264e-10, 2.28732819e-08, 1.13720000e-06,\n",
       "        7.61274190e-04, 1.48411871e-07, 8.87061833e-06, 1.29744558e-06,\n",
       "        1.92321331e-05, 8.60865111e-06, 3.86389411e-06, 9.72961821e-03,\n",
       "        5.06375557e-07, 1.46392922e-04, 8.47840420e-06, 4.87167796e-04,\n",
       "        2.97562248e-04, 5.44114679e-04, 7.55252177e-06, 7.16508293e-06,\n",
       "        5.65800510e-05, 2.64315020e-07, 1.02217639e-08, 2.26815883e-03,\n",
       "        1.34597883e-06, 2.12081410e-08, 3.68163426e-04, 2.65603205e-07,\n",
       "        7.64718512e-04, 3.98283264e-05, 2.80123873e-04, 6.10187456e-08,\n",
       "        4.99762837e-06, 1.44280339e-08, 1.94441272e-05, 3.39939049e-03,\n",
       "        1.27991398e-05, 1.20145727e-04, 4.36698519e-06, 3.38079076e-06,\n",
       "        5.76615491e-07, 8.88740033e-08, 8.61615874e-04, 2.51483812e-04,\n",
       "        4.03970480e-05, 5.69185754e-03, 4.82407486e-05, 9.69158373e-06,\n",
       "        3.12313347e-07, 8.73600366e-05, 5.10125483e-06, 4.18513373e-05,\n",
       "        4.20047814e-04, 6.90031971e-04, 1.08715403e-06, 2.86946005e-07,\n",
       "        1.64887554e-03, 5.57058047e-06, 3.36117555e-05, 4.60564988e-05,\n",
       "        1.14377227e-03, 4.09311610e-07, 2.11344286e-06, 3.18157322e-06,\n",
       "        3.37934500e-04, 6.46641944e-03, 3.19907691e-08, 7.17223747e-09,\n",
       "        9.36395679e-07, 3.82363595e-07, 7.24632218e-06, 2.19740514e-07,\n",
       "        4.73307009e-05, 9.89997759e-04, 4.72794527e-05, 1.02889295e-07,\n",
       "        5.78715344e-06, 2.02372394e-06, 9.81614576e-04, 3.83507373e-04,\n",
       "        1.93852484e-05, 6.09432072e-06, 6.93094407e-05, 9.74629188e-07,\n",
       "        5.22574555e-06, 3.04307287e-06, 1.46636676e-05, 4.55528607e-06,\n",
       "        3.24137873e-05, 1.12992677e-06, 4.61881253e-04, 2.26401227e-07,\n",
       "        2.70194469e-05, 6.81427809e-06, 2.14079339e-02, 3.75893054e-04,\n",
       "        3.45635590e-06, 1.67731321e-06, 1.46654768e-08, 4.86713816e-07,\n",
       "        1.19061166e-04, 1.30183998e-05, 8.06239550e-05, 6.76905038e-04,\n",
       "        2.71001172e-05, 1.40374113e-05, 3.29838917e-06, 4.36062037e-05,\n",
       "        1.38819843e-04, 4.84278899e-05, 1.02618200e-04, 1.41154826e-02,\n",
       "        2.42592956e-04, 8.62408243e-08, 8.18623602e-03, 1.93712021e-05,\n",
       "        2.05150296e-04, 3.73566172e-06, 3.76929506e-07, 1.80315192e-06,\n",
       "        2.18501953e-07, 2.47583492e-04, 1.60936033e-05, 2.20064840e-06,\n",
       "        7.72261558e-07, 1.14803007e-02, 8.99567603e-05, 4.39506257e-03,\n",
       "        7.91504397e-04, 2.98697341e-06, 1.18032931e-05, 1.21217439e-04,\n",
       "        2.99695621e-05, 1.39927701e-03, 4.66196155e-07, 1.16064111e-04,\n",
       "        1.34669797e-04, 3.33862736e-05, 1.49735229e-06, 7.69982307e-06,\n",
       "        2.12199666e-05, 7.04910974e-08, 1.67030634e-09, 2.36849118e-07,\n",
       "        4.82978976e-06, 1.24738726e-04, 6.65887666e-04, 2.91808919e-06,\n",
       "        6.82870768e-06, 6.85058907e-03, 4.81729074e-08, 1.09339796e-03,\n",
       "        1.10573685e-02, 8.97845257e-06, 2.32486241e-06, 3.12830252e-07,\n",
       "        2.10854359e-05, 3.48791218e-08, 1.47398014e-03, 2.73708545e-04]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ = sess.run(prediction, feed_dict={X: [batch_x[3]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred)\n",
    "# pred\n",
    "# batch_y[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real [224, 339]\n",
      "pred [285 107]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real = np.nonzero(batch_y[3])[0].tolist()\n",
    "# batch_y[1]\n",
    "pred = np.argpartition(predict_[0], -len(real))[-len(real):]\n",
    "print('real', real)\n",
    "print('pred', pred)\n",
    "\n",
    "len(set(real).intersection(set(pred))) / len(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multilabel(y_pred, y_true):\n",
    "    acc = []\n",
    "    for y_pred_tmp, y_true_tmp in zip(y_pred, y_true):\n",
    "        print('y_true_tmp', y_true_tmp, 'y_pred_tmp', y_pred_tmp)\n",
    "        real = np.nonzero(y_true_tmp)[0].tolist()\n",
    "        right_num = len(real)\n",
    "        pred = np.argpartition(y_pred_tmp, -right_num)[-right_num:]\n",
    "        print('real', real, 'pred', pred)\n",
    "        if len(real) == 0:\n",
    "            #means 0 right answers\n",
    "            acc.append(0.0)\n",
    "            continue\n",
    "        acc.append(len(set(real).intersection(set(pred))) / len(real))\n",
    "    print(acc)\n",
    "    return(np.array(acc).mean())\n",
    "        \n",
    "evaluate_multilabel([[0,0,1], [0,0,0]], [[0,0,1], [1,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nonzero([0,1,1])[0].tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
