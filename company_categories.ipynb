{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83897, 18)\n",
      "(47516, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pandas.read_parquet('../data/to_send.pq')\n",
    "print(data.shape)\n",
    "data = data[['description', 'products']][pandas.notnull(data['products'])].copy().reset_index(drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Welch allyn combines its practical understandi...</td>\n",
       "      <td>[power supply, body sub assy, medical, valve b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In  line  with  the  company  s intention  to ...</td>\n",
       "      <td>[imo, advertising materials, point, imo label,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Services redaelli ricambi offers the ability t...</td>\n",
       "      <td>[auto spare parts, tie rod, tie rod end, auto ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STROTHMANN not only delivers suitable mechanic...</td>\n",
       "      <td>[covers non automated, demurrage rules form, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Established\\nin 1991, tien jiang enterprise co...</td>\n",
       "      <td>[rubber, polyester, nylon, boot, support]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  Welch allyn combines its practical understandi...   \n",
       "1  In  line  with  the  company  s intention  to ...   \n",
       "2  Services redaelli ricambi offers the ability t...   \n",
       "3  STROTHMANN not only delivers suitable mechanic...   \n",
       "4  Established\\nin 1991, tien jiang enterprise co...   \n",
       "\n",
       "                                            products  \n",
       "0  [power supply, body sub assy, medical, valve b...  \n",
       "1  [imo, advertising materials, point, imo label,...  \n",
       "2  [auto spare parts, tie rod, tie rod end, auto ...  \n",
       "3  [covers non automated, demurrage rules form, r...  \n",
       "4          [rubber, polyester, nylon, boot, support]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Welch allyn combines its practical understanding of clinical needs with its visionary spirit to develop solutions that assess, diagnose, treat, and manage a variety of illnesses and diseases.as a leading global manufacturer of medical diagnostic equipment, we offer a range of connected solutions. with nearly 2,500 employees working in 26 countries, we focus on the customer and imagine how healthcare will be delivered in the future to develop tools and future-proof technologies.our professional customers include physicians’ practices, community clinics, skilled nursing facilities, and emergency departments—places where 95% of patients first seek medical treatment.our welch allyn home division delivers solutions that make home monitoring of blood pressure readings accurate and easy. with the same attention to quality that we offer to our professional customers, we’re now bringing physician-trusted solutions to the home, helping people better manage their health.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['power supply', 'body sub assy', 'medical', 'valve body sub assy',\n",
       "       'digital blood pressure'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0]['products']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique categories 71111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35475, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "\n",
    "data['products'] = data['products'].apply(lambda x: np.array([' '.join(tokenizer.tokenize(product)).lower()\n",
    "                                                              for product in x.tolist()]))\n",
    "\n",
    "all_products = []\n",
    "for prod_list in data['products'][pandas.notnull(data['products'])].values:\n",
    "    all_products += [' '.join(tokenizer.tokenize(product))\n",
    "                     for product in prod_list.tolist()]\n",
    "    \n",
    "counter = Counter(all_products)\n",
    "print('unique categories', len(counter.most_common()))\n",
    "\n",
    "most_common = [product[0] for product in counter.most_common(600)]\n",
    "\n",
    "def filter_categories(x):\n",
    "    new_categories = np.array([product \n",
    "                               for product in x.tolist()\n",
    "                               if product in most_common])\n",
    "    if new_categories.shape[0] == 0:\n",
    "        return np.nan\n",
    "    return new_categories\n",
    "\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "stopwords_cached = stopwords.words('english')\n",
    "\n",
    "def filter_descriptions(text):\n",
    "    cleaned_text = [token#lemma.lemmatize(token)\n",
    "                      for token in tokenizer.tokenize(text.lower())\n",
    "                      if token not in stopwords_cached]\n",
    "    if len(cleaned_text) == 0:\n",
    "        return np.nan\n",
    "    return ' '.join(cleaned_text)\n",
    "\n",
    "data['products'] = data['products'].apply(filter_categories)\n",
    "data['description'] = data['description'].apply(filter_descriptions)\n",
    "\n",
    "cleaned_data = data[(data['products'].notnull()) & (data['description'].notnull())].copy().reset_index(drop=True)\n",
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welch allyn combines practical understanding c...</td>\n",
       "      <td>[power supply, medical]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>line company intention support international g...</td>\n",
       "      <td>[imo, point, marine pollutant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>services redaelli ricambi offers ability produ...</td>\n",
       "      <td>[auto spare parts, auto spare, spare parts]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strothmann delivers suitable mechanical system...</td>\n",
       "      <td>[line]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>established tien jiang enterprise co ltd one s...</td>\n",
       "      <td>[rubber, polyester, nylon, support]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  welch allyn combines practical understanding c...   \n",
       "1  line company intention support international g...   \n",
       "2  services redaelli ricambi offers ability produ...   \n",
       "3  strothmann delivers suitable mechanical system...   \n",
       "4  established tien jiang enterprise co ltd one s...   \n",
       "\n",
       "                                      products  \n",
       "0                      [power supply, medical]  \n",
       "1               [imo, point, marine pollutant]  \n",
       "2  [auto spare parts, auto spare, spare parts]  \n",
       "3                                       [line]  \n",
       "4          [rubber, polyester, nylon, support]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers.fasttext import FastText\n",
    "\n",
    "model = FastText.load_fasttext_format('../models/wiki.simple.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in word zc\n",
      "error in word zc\n",
      "error in word vz\n",
      "error in word vz\n",
      "error in word vz\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer = RegexpTokenizer(r'[a-z]+')\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "stopwords_cached = stopwords.words('english')\n",
    "\n",
    "def get_embedding_matrix(text, first_n=50):\n",
    "#     print(text)\n",
    "    matrix = []\n",
    "    text = [token#lemma.lemmatize(token)\n",
    "            for token in tokenizer.tokenize(text)]\n",
    "    for word in text[:first_n]:\n",
    "        try:\n",
    "            word_embedding = model.wv[word]\n",
    "            matrix.append(model.wv[word])\n",
    "        except KeyError:\n",
    "            print(\"error in word\", word)\n",
    "            matrix.append(np.zeros(300))\n",
    "    \n",
    "    matrix = np.array(matrix)\n",
    "    # fill text embeddings with seq_len < first_n with zeros\n",
    "#     print(matrix.shape[0])\n",
    "    if matrix.shape[0] < first_n:\n",
    "        matrix = np.vstack((matrix, np.zeros((first_n - matrix.shape[0], 300))))\n",
    "    return matrix\n",
    "\n",
    "embeddings = []\n",
    "for text in cleaned_data['description'].values:\n",
    "    embeddings.append(get_embedding_matrix(text))\n",
    "#     print(embeddings[-1].shape)\n",
    "    \n",
    "# onehot_y_df = pandas.get_dummies(data['severity'])\n",
    "\n",
    "train_data = np.array(embeddings)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([item.tolist() for item in cleaned_data['products'].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class DataBatcher():\n",
    "    def __init__(self, _X, _y, _batch_size=30):\n",
    "        self._X = _X\n",
    "        self._y = _y\n",
    "        self._batch_size = _batch_size\n",
    "        self._resplit = True\n",
    "        self._num_examples = self._y.shape[0]\n",
    "    \n",
    "    def next_batch(self):\n",
    "        if self._resplit:\n",
    "#             print('splitting')\n",
    "            perm0 = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm0)\n",
    "            self._batches_indexes = np.array_split(perm0, math.ceil(perm0.shape[0] / self._batch_size))\n",
    "#             print(self._batches_indexes)\n",
    "            self._batch_counter = -1\n",
    "            self._resplit = False\n",
    "\n",
    "        self._batch_counter += 1\n",
    "        if self._batches_indexes[self._batch_counter].shape[0] < self._batch_size:\n",
    "#             print('hstacking')\n",
    "            self._resplit = True\n",
    "            ind = self._batch_counter\n",
    "#             self._batch_counter = -1\n",
    "            missing_num = self._batch_size - self._batches_indexes[ind].shape[0]\n",
    "            return self._X[np.hstack((self._batches_indexes[ind], self._batches_indexes[0][:missing_num]))],\\\n",
    "                   self._y[np.hstack((self._batches_indexes[ind], self._batches_indexes[0][:missing_num]))]\n",
    "        \n",
    "        return self._X[self._batches_indexes[self._batch_counter]], self._y[self._batches_indexes[self._batch_counter]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss= 2.8073, Train Acc= 0.003, Test Acc = 0.006\n",
      "Step 1000, Loss= 0.0260, Train Acc= 0.021, Test Acc = 0.010\n",
      "Step 2000, Loss= 0.0262, Train Acc= 0.000, Test Acc = 0.017\n",
      "Step 3000, Loss= 0.0223, Train Acc= 0.036, Test Acc = 0.020\n",
      "Step 4000, Loss= 0.0219, Train Acc= 0.037, Test Acc = 0.022\n",
      "Step 5000, Loss= 0.0209, Train Acc= 0.049, Test Acc = 0.026\n",
      "Step 6000, Loss= 0.0211, Train Acc= 0.015, Test Acc = 0.032\n",
      "Step 7000, Loss= 0.0210, Train Acc= 0.039, Test Acc = 0.040\n",
      "Step 8000, Loss= 0.0208, Train Acc= 0.050, Test Acc = 0.048\n",
      "Step 9000, Loss= 0.0204, Train Acc= 0.067, Test Acc = 0.049\n",
      "Step 10000, Loss= 0.0198, Train Acc= 0.030, Test Acc = 0.059\n",
      "Step 11000, Loss= 0.0180, Train Acc= 0.060, Test Acc = 0.062\n",
      "Step 12000, Loss= 0.0197, Train Acc= 0.067, Test Acc = 0.070\n",
      "Step 13000, Loss= 0.0187, Train Acc= 0.085, Test Acc = 0.074\n",
      "Step 14000, Loss= 0.0192, Train Acc= 0.085, Test Acc = 0.080\n",
      "Step 15000, Loss= 0.0172, Train Acc= 0.100, Test Acc = 0.087\n",
      "Step 16000, Loss= 0.0183, Train Acc= 0.110, Test Acc = 0.089\n",
      "Step 17000, Loss= 0.0173, Train Acc= 0.108, Test Acc = 0.092\n",
      "Step 18000, Loss= 0.0178, Train Acc= 0.176, Test Acc = 0.097\n",
      "Step 19000, Loss= 0.0193, Train Acc= 0.094, Test Acc = 0.097\n",
      "Step 20000, Loss= 0.0171, Train Acc= 0.110, Test Acc = 0.094\n",
      "Step 21000, Loss= 0.0175, Train Acc= 0.090, Test Acc = 0.095\n",
      "Step 22000, Loss= 0.0175, Train Acc= 0.152, Test Acc = 0.093\n",
      "Step 23000, Loss= 0.0171, Train Acc= 0.146, Test Acc = 0.096\n",
      "Step 24000, Loss= 0.0166, Train Acc= 0.154, Test Acc = 0.095\n",
      "Step 25000, Loss= 0.0183, Train Acc= 0.165, Test Acc = 0.096\n",
      "Step 26000, Loss= 0.0159, Train Acc= 0.161, Test Acc = 0.099\n",
      "Step 27000, Loss= 0.0144, Train Acc= 0.201, Test Acc = 0.097\n",
      "Step 28000, Loss= 0.0143, Train Acc= 0.192, Test Acc = 0.095\n",
      "Step 29000, Loss= 0.0146, Train Acc= 0.215, Test Acc = 0.096\n",
      "Step 30000, Loss= 0.0134, Train Acc= 0.275, Test Acc = 0.093\n",
      "Step 31000, Loss= 0.0149, Train Acc= 0.262, Test Acc = 0.093\n",
      "Step 32000, Loss= 0.0137, Train Acc= 0.280, Test Acc = 0.096\n",
      "Step 33000, Loss= 0.0139, Train Acc= 0.223, Test Acc = 0.094\n",
      "Step 34000, Loss= 0.0142, Train Acc= 0.207, Test Acc = 0.094\n",
      "Step 35000, Loss= 0.0143, Train Acc= 0.267, Test Acc = 0.092\n",
      "Step 36000, Loss= 0.0149, Train Acc= 0.268, Test Acc = 0.093\n",
      "Step 37000, Loss= 0.0136, Train Acc= 0.297, Test Acc = 0.097\n",
      "Step 38000, Loss= 0.0137, Train Acc= 0.204, Test Acc = 0.091\n",
      "Step 39000, Loss= 0.0130, Train Acc= 0.290, Test Acc = 0.094\n",
      "Step 40000, Loss= 0.0129, Train Acc= 0.223, Test Acc = 0.094\n",
      "Step 41000, Loss= 0.0107, Train Acc= 0.371, Test Acc = 0.093\n",
      "Step 42000, Loss= 0.0109, Train Acc= 0.358, Test Acc = 0.091\n",
      "Step 43000, Loss= 0.0126, Train Acc= 0.278, Test Acc = 0.092\n",
      "Step 44000, Loss= 0.0114, Train Acc= 0.359, Test Acc = 0.093\n",
      "Step 45000, Loss= 0.0129, Train Acc= 0.291, Test Acc = 0.093\n",
      "Step 46000, Loss= 0.0121, Train Acc= 0.420, Test Acc = 0.090\n",
      "Step 47000, Loss= 0.0117, Train Acc= 0.327, Test Acc = 0.092\n",
      "Step 48000, Loss= 0.0111, Train Acc= 0.430, Test Acc = 0.095\n",
      "Step 49000, Loss= 0.0098, Train Acc= 0.409, Test Acc = 0.089\n",
      "Step 50000, Loss= 0.0097, Train Acc= 0.385, Test Acc = 0.089\n",
      "Step 51000, Loss= 0.0107, Train Acc= 0.415, Test Acc = 0.089\n",
      "Step 52000, Loss= 0.0106, Train Acc= 0.332, Test Acc = 0.088\n",
      "Step 53000, Loss= 0.0099, Train Acc= 0.422, Test Acc = 0.088\n",
      "Step 54000, Loss= 0.0105, Train Acc= 0.376, Test Acc = 0.087\n",
      "Step 55000, Loss= 0.0100, Train Acc= 0.473, Test Acc = 0.090\n",
      "Step 56000, Loss= 0.0100, Train Acc= 0.365, Test Acc = 0.089\n",
      "Step 57000, Loss= 0.0089, Train Acc= 0.480, Test Acc = 0.086\n",
      "Step 58000, Loss= 0.0095, Train Acc= 0.490, Test Acc = 0.090\n",
      "Step 59000, Loss= 0.0088, Train Acc= 0.496, Test Acc = 0.086\n",
      "Step 60000, Loss= 0.0089, Train Acc= 0.423, Test Acc = 0.091\n",
      "Step 61000, Loss= 0.0089, Train Acc= 0.492, Test Acc = 0.084\n",
      "Step 62000, Loss= 0.0087, Train Acc= 0.512, Test Acc = 0.089\n",
      "Step 63000, Loss= 0.0093, Train Acc= 0.458, Test Acc = 0.088\n",
      "Step 64000, Loss= 0.0079, Train Acc= 0.535, Test Acc = 0.088\n",
      "Step 65000, Loss= 0.0084, Train Acc= 0.431, Test Acc = 0.084\n",
      "Step 66000, Loss= 0.0080, Train Acc= 0.547, Test Acc = 0.088\n",
      "Step 67000, Loss= 0.0090, Train Acc= 0.487, Test Acc = 0.084\n",
      "Step 68000, Loss= 0.0090, Train Acc= 0.456, Test Acc = 0.086\n",
      "Step 69000, Loss= 0.0088, Train Acc= 0.518, Test Acc = 0.088\n",
      "Step 70000, Loss= 0.0080, Train Acc= 0.545, Test Acc = 0.086\n",
      "Step 71000, Loss= 0.0084, Train Acc= 0.529, Test Acc = 0.085\n",
      "Step 72000, Loss= 0.0078, Train Acc= 0.535, Test Acc = 0.086\n",
      "Step 73000, Loss= 0.0088, Train Acc= 0.549, Test Acc = 0.087\n",
      "Step 74000, Loss= 0.0086, Train Acc= 0.509, Test Acc = 0.086\n",
      "Step 75000, Loss= 0.0065, Train Acc= 0.661, Test Acc = 0.087\n",
      "Step 76000, Loss= 0.0076, Train Acc= 0.546, Test Acc = 0.087\n",
      "Step 77000, Loss= 0.0079, Train Acc= 0.645, Test Acc = 0.089\n",
      "Step 78000, Loss= 0.0075, Train Acc= 0.568, Test Acc = 0.084\n",
      "Step 79000, Loss= 0.0073, Train Acc= 0.595, Test Acc = 0.086\n",
      "Step 80000, Loss= 0.0067, Train Acc= 0.652, Test Acc = 0.087\n",
      "Step 81000, Loss= 0.0075, Train Acc= 0.566, Test Acc = 0.084\n",
      "Step 82000, Loss= 0.0074, Train Acc= 0.611, Test Acc = 0.083\n",
      "Step 83000, Loss= 0.0073, Train Acc= 0.577, Test Acc = 0.087\n",
      "Step 84000, Loss= 0.0081, Train Acc= 0.578, Test Acc = 0.085\n",
      "Step 85000, Loss= 0.0069, Train Acc= 0.609, Test Acc = 0.086\n",
      "Step 86000, Loss= 0.0066, Train Acc= 0.680, Test Acc = 0.087\n",
      "Step 87000, Loss= 0.0069, Train Acc= 0.641, Test Acc = 0.085\n",
      "Step 88000, Loss= 0.0070, Train Acc= 0.618, Test Acc = 0.085\n",
      "Step 89000, Loss= 0.0059, Train Acc= 0.653, Test Acc = 0.084\n",
      "Step 90000, Loss= 0.0061, Train Acc= 0.610, Test Acc = 0.083\n",
      "Step 91000, Loss= 0.0063, Train Acc= 0.623, Test Acc = 0.081\n",
      "Step 92000, Loss= 0.0062, Train Acc= 0.631, Test Acc = 0.086\n",
      "Step 93000, Loss= 0.0071, Train Acc= 0.604, Test Acc = 0.085\n",
      "Step 94000, Loss= 0.0062, Train Acc= 0.609, Test Acc = 0.082\n",
      "Step 95000, Loss= 0.0062, Train Acc= 0.689, Test Acc = 0.085\n",
      "Step 96000, Loss= 0.0054, Train Acc= 0.635, Test Acc = 0.086\n",
      "Step 97000, Loss= 0.0058, Train Acc= 0.655, Test Acc = 0.087\n",
      "Step 98000, Loss= 0.0061, Train Acc= 0.652, Test Acc = 0.086\n",
      "Step 99000, Loss= 0.0062, Train Acc= 0.614, Test Acc = 0.083\n",
      "Step 100000, Loss= 0.0057, Train Acc= 0.729, Test Acc = 0.086\n",
      "Step 101000, Loss= 0.0048, Train Acc= 0.737, Test Acc = 0.083\n",
      "Step 102000, Loss= 0.0050, Train Acc= 0.676, Test Acc = 0.084\n",
      "Step 103000, Loss= 0.0056, Train Acc= 0.684, Test Acc = 0.084\n",
      "Step 104000, Loss= 0.0052, Train Acc= 0.699, Test Acc = 0.083\n",
      "Step 105000, Loss= 0.0047, Train Acc= 0.719, Test Acc = 0.083\n",
      "Step 106000, Loss= 0.0054, Train Acc= 0.718, Test Acc = 0.083\n",
      "Step 107000, Loss= 0.0051, Train Acc= 0.734, Test Acc = 0.084\n",
      "Step 108000, Loss= 0.0061, Train Acc= 0.680, Test Acc = 0.084\n",
      "Step 109000, Loss= 0.0061, Train Acc= 0.666, Test Acc = 0.087\n",
      "Step 110000, Loss= 0.0047, Train Acc= 0.739, Test Acc = 0.085\n",
      "Step 111000, Loss= 0.0044, Train Acc= 0.740, Test Acc = 0.086\n",
      "Step 112000, Loss= 0.0047, Train Acc= 0.767, Test Acc = 0.081\n",
      "Step 113000, Loss= 0.0045, Train Acc= 0.728, Test Acc = 0.084\n",
      "Step 114000, Loss= 0.0042, Train Acc= 0.766, Test Acc = 0.084\n",
      "Step 115000, Loss= 0.0046, Train Acc= 0.770, Test Acc = 0.086\n",
      "Step 116000, Loss= 0.0056, Train Acc= 0.690, Test Acc = 0.081\n",
      "Step 117000, Loss= 0.0046, Train Acc= 0.715, Test Acc = 0.082\n",
      "Step 118000, Loss= 0.0053, Train Acc= 0.697, Test Acc = 0.084\n",
      "Step 119000, Loss= 0.0046, Train Acc= 0.757, Test Acc = 0.084\n",
      "Step 120000, Loss= 0.0059, Train Acc= 0.708, Test Acc = 0.083\n",
      "Step 121000, Loss= 0.0043, Train Acc= 0.810, Test Acc = 0.085\n",
      "Step 122000, Loss= 0.0044, Train Acc= 0.780, Test Acc = 0.082\n",
      "Step 123000, Loss= 0.0046, Train Acc= 0.742, Test Acc = 0.085\n",
      "Step 124000, Loss= 0.0050, Train Acc= 0.722, Test Acc = 0.082\n",
      "Step 125000, Loss= 0.0040, Train Acc= 0.786, Test Acc = 0.084\n",
      "Step 126000, Loss= 0.0047, Train Acc= 0.710, Test Acc = 0.083\n",
      "Step 127000, Loss= 0.0045, Train Acc= 0.729, Test Acc = 0.083\n",
      "Step 128000, Loss= 0.0042, Train Acc= 0.761, Test Acc = 0.081\n",
      "Step 129000, Loss= 0.0044, Train Acc= 0.718, Test Acc = 0.086\n",
      "Step 130000, Loss= 0.0058, Train Acc= 0.658, Test Acc = 0.083\n",
      "Step 131000, Loss= 0.0043, Train Acc= 0.739, Test Acc = 0.080\n",
      "Step 132000, Loss= 0.0049, Train Acc= 0.719, Test Acc = 0.080\n",
      "Step 133000, Loss= 0.0050, Train Acc= 0.741, Test Acc = 0.085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 134000, Loss= 0.0053, Train Acc= 0.735, Test Acc = 0.082\n",
      "Step 135000, Loss= 0.0047, Train Acc= 0.757, Test Acc = 0.082\n",
      "Step 136000, Loss= 0.0040, Train Acc= 0.809, Test Acc = 0.083\n",
      "Step 137000, Loss= 0.0047, Train Acc= 0.724, Test Acc = 0.083\n",
      "Step 138000, Loss= 0.0042, Train Acc= 0.769, Test Acc = 0.078\n",
      "Step 139000, Loss= 0.0048, Train Acc= 0.721, Test Acc = 0.082\n",
      "Step 140000, Loss= 0.0048, Train Acc= 0.729, Test Acc = 0.082\n",
      "Step 141000, Loss= 0.0051, Train Acc= 0.745, Test Acc = 0.085\n",
      "Step 142000, Loss= 0.0047, Train Acc= 0.768, Test Acc = 0.081\n",
      "Step 143000, Loss= 0.0037, Train Acc= 0.800, Test Acc = 0.086\n",
      "Step 144000, Loss= 0.0045, Train Acc= 0.787, Test Acc = 0.081\n",
      "Step 145000, Loss= 0.0048, Train Acc= 0.709, Test Acc = 0.076\n",
      "Step 146000, Loss= 0.0048, Train Acc= 0.759, Test Acc = 0.085\n",
      "Step 147000, Loss= 0.0042, Train Acc= 0.792, Test Acc = 0.082\n",
      "Step 148000, Loss= 0.0041, Train Acc= 0.780, Test Acc = 0.083\n",
      "Step 149000, Loss= 0.0032, Train Acc= 0.883, Test Acc = 0.081\n",
      "Step 150000, Loss= 0.0042, Train Acc= 0.797, Test Acc = 0.078\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Training Parameters\n",
    "    learning_rate = 0.001\n",
    "    training_steps = 150000\n",
    "    batch_size = 80\n",
    "    display_step = 1000\n",
    "    checkpoint_step = 20000\n",
    "    \n",
    "    # Network Parameters\n",
    "    num_input = 300 # MNIST data input (img shape: 28*28)\n",
    "    timesteps = 50 # timesteps\n",
    "    num_hidden = 400 # hidden layer num of features\n",
    "    num_classes = 600 # MNIST total classes (0-9 digits)\n",
    "    num_layers = 3\n",
    "    input_keep_prob = 0.75\n",
    "    output_keep_prob = 0.75\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([2 * num_hidden, num_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "    }\n",
    "\n",
    "    def RNN(x, weights, biases):\n",
    "\n",
    "        # Prepare data shape to match `rnn` function requirements\n",
    "        # Current data input shape: (batch_size, timesteps, n_input)\n",
    "        # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "        # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "        x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "        # Cerate forward and backward cells\n",
    "        cells_fw = [cell(num_hidden, activation=tf.sigmoid) for _ in range(num_layers)]\n",
    "        cells_bw = [cell(num_hidden, activation=tf.sigmoid) for _ in range(num_layers)]\n",
    "        \n",
    "        # Add dropout\n",
    "        cells_fw = [tf.contrib.rnn.DropoutWrapper(cell, \n",
    "                                                  input_keep_prob=input_keep_prob,\n",
    "                                                  output_keep_prob=output_keep_prob\n",
    "                                                 ) for cell in cells_fw]\n",
    "        cells_bw = [tf.contrib.rnn.DropoutWrapper(cell, \n",
    "                                                  input_keep_prob=input_keep_prob,\n",
    "                                                  output_keep_prob=output_keep_prob\n",
    "                                                 ) for cell in cells_bw]\n",
    "        \n",
    "        outputs, _, _ = tf.contrib.rnn.stack_bidirectional_rnn(\n",
    "            cells_fw=cells_fw,\n",
    "            cells_bw=cells_bw,\n",
    "            inputs=x,\n",
    "            dtype=tf.float32)\n",
    "        \n",
    "#         rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "        \n",
    "        return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "        \n",
    "\n",
    "    logits = RNN(X, weights, biases)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    \n",
    "\n",
    "# tf.nn.sigmoid_cross_entropy_with_logits \n",
    "#     loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "#         logits=logits, labels=Y))\n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=logits, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    # Evaluate model (with test logits, for dropout to be disabled)\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "def evaluate_multilabel(y_pred, y_true):\n",
    "    acc = []\n",
    "    for y_pred_tmp, y_true_tmp in zip(y_pred, y_true):\n",
    "#         print('y_true_tmp', y_true_tmp, 'y_pred_tmp', y_pred_tmp)\n",
    "        real = np.nonzero(y_true_tmp)[0].tolist()\n",
    "        right_num = len(real)\n",
    "        pred = np.argpartition(y_pred_tmp, -right_num)[-right_num:]\n",
    "#         print('real', real, 'pred', pred)\n",
    "        if len(real) == 0:\n",
    "            #means 0 right answers\n",
    "            acc.append(0.0)\n",
    "            continue\n",
    "        acc.append(len(set(real).intersection(set(pred))) / len(real))\n",
    "#     print(acc)\n",
    "    return(np.array(acc).mean())\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4)\n",
    "sess = tf.InteractiveSession(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "# with tf.Session(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "batcher = DataBatcher(X_train, y_train, _batch_size=batch_size)\n",
    "\n",
    "for step in range(1, training_steps+1):\n",
    "    batch_x, batch_y = batcher.next_batch()\n",
    "    # Reshape data to get 28 seq of 28 elements\n",
    "#         batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "    # Run optimization op (backprop)\n",
    "    sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        # Calculate batch loss and accuracy\n",
    "        loss, pred = sess.run([loss_op, prediction], feed_dict={X: batch_x, Y: batch_y})\n",
    "        \n",
    "        test_out = sess.run(prediction, feed_dict={X: X_test})\n",
    "        print(\"Step \" + str(step) + \", Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss) + \", Train Acc= \" + \\\n",
    "              \"{:.3f}\".format(evaluate_multilabel(y_pred=pred, y_true=batch_y)) +  \", Test Acc = \" + \\\n",
    "              \"{:.3f}\".format(evaluate_multilabel(y_pred=test_out, y_true=y_test)))\n",
    "        \n",
    "    if step % checkpoint_step == 0:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, '../models/lstm_3_layers/lstm_3_layers', global_step=step)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, '../models/lstm_3_layers/lstm_3_layers', global_step=step)\n",
    "        \n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "#     # Calculate accuracy for 128 mnist test images\n",
    "#     print(\"Testing Accuracy:\", \\\n",
    "#         sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Training Parameters\n",
    "    learning_rate = 0.001\n",
    "    training_steps = 10000\n",
    "    batch_size = 128\n",
    "    display_step = 200\n",
    "\n",
    "    # Network Parameters\n",
    "    num_input = 28 # MNIST data input (img shape: 28*28)\n",
    "    timesteps = 28 # timesteps\n",
    "    num_hidden = 128 # hidden layer num of features\n",
    "    num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "    }\n",
    "\n",
    "    def RNN(x, weights, biases):\n",
    "\n",
    "        # Prepare data shape to match `rnn` function requirements\n",
    "        # Current data input shape: (batch_size, timesteps, n_input)\n",
    "        # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "        # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "        x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "        # Define a lstm cell with tensorflow\n",
    "        lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "        # Get lstm cell output\n",
    "        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "        # Linear activation, using rnn inner loop last output\n",
    "        return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "    logits = RNN(X, weights, biases)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    # Evaluate model (with test logits, for dropout to be disabled)\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4)\n",
    "sess = tf.InteractiveSession(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "# Start training\n",
    "# with tf.Session() as sess:\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(1, training_steps+1):\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "#     batcher = DataBatcher(mnist.train.images, mnist.train.labels, _batch_size=batch_size)\n",
    "#     batch_x, batch_y = batcher.next_batch()\n",
    "    # Reshape data to get 28 seq of 28 elements\n",
    "    batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "    # Run optimization op (backprop)\n",
    "    sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        # Calculate batch loss and accuracy\n",
    "        loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                             Y: batch_y})\n",
    "        print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "              \"{:.3f}\".format(acc))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Calculate accuracy for 128 mnist test images\n",
    "test_len = 128\n",
    "test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "test_label = mnist.test.labels[:test_len]\n",
    "print(\"Testing Accuracy:\", \\\n",
    "    sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batcher = DataBatcher(X_train, y_train, batch_size_=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.train.images.shape)\n",
    "print(mnist.train.labels.shape)\n",
    "print(mnist.test.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ = sess.run(prediction, feed_dict={X: [batch_x[3]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(predict_)\n",
    "# pred\n",
    "# batch_y[3]\n",
    "predict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = 5\n",
    "predict_ = sess.run(prediction, feed_dict={X: [batch_x[obj]]})\n",
    "real = np.nonzero(batch_y[obj])[0].tolist()\n",
    "# batch_y[1]\n",
    "pred = np.argpartition(predict_[0], -len(real))[-len(real):]\n",
    "print('real', real)\n",
    "print('pred', pred)\n",
    "print('classes real', mlb.classes_[real])\n",
    "print('classes predicted', mlb.classes_[pred])\n",
    "\n",
    "len(set(real).intersection(set(pred))) / len(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.classes_[512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multilabel(y_pred, y_true):\n",
    "    acc = []\n",
    "    for y_pred_tmp, y_true_tmp in zip(y_pred, y_true):\n",
    "        print('y_true_tmp', y_true_tmp, 'y_pred_tmp', y_pred_tmp)\n",
    "        real = np.nonzero(y_true_tmp)[0].tolist()\n",
    "        right_num = len(real)\n",
    "        pred = np.argpartition(y_pred_tmp, -right_num)[-right_num:]\n",
    "        print('real', real, 'pred', pred)\n",
    "        if len(real) == 0:\n",
    "            #means 0 right answers\n",
    "            acc.append(0.0)\n",
    "            continue\n",
    "        acc.append(len(set(real).intersection(set(pred))) / len(real))\n",
    "    print(acc)\n",
    "    return(np.array(acc).mean())\n",
    "        \n",
    "evaluate_multilabel([[0,0,1], [0,0,0]], [[0,0,1], [1,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nonzero([0,1,1])[0].tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
