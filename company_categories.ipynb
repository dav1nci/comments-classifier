{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83897, 18)\n",
      "(47516, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pandas.read_parquet('../data/to_send.pq')\n",
    "print(data.shape)\n",
    "data = data[['description', 'products']][pandas.notnull(data['products'])].copy().reset_index(drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Welch allyn combines its practical understandi...</td>\n",
       "      <td>[power supply, body sub assy, medical, valve b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In  line  with  the  company  s intention  to ...</td>\n",
       "      <td>[imo, advertising materials, point, imo label,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Services redaelli ricambi offers the ability t...</td>\n",
       "      <td>[auto spare parts, tie rod, tie rod end, auto ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STROTHMANN not only delivers suitable mechanic...</td>\n",
       "      <td>[covers non automated, demurrage rules form, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Established\\nin 1991, tien jiang enterprise co...</td>\n",
       "      <td>[rubber, polyester, nylon, boot, support]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  Welch allyn combines its practical understandi...   \n",
       "1  In  line  with  the  company  s intention  to ...   \n",
       "2  Services redaelli ricambi offers the ability t...   \n",
       "3  STROTHMANN not only delivers suitable mechanic...   \n",
       "4  Established\\nin 1991, tien jiang enterprise co...   \n",
       "\n",
       "                                            products  \n",
       "0  [power supply, body sub assy, medical, valve b...  \n",
       "1  [imo, advertising materials, point, imo label,...  \n",
       "2  [auto spare parts, tie rod, tie rod end, auto ...  \n",
       "3  [covers non automated, demurrage rules form, r...  \n",
       "4          [rubber, polyester, nylon, boot, support]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique categories 71111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35461, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "\n",
    "data['products'] = data['products'].apply(lambda x: np.array([' '.join(tokenizer.tokenize(product)).lower()\n",
    "                                                              for product in x.tolist()]))\n",
    "\n",
    "all_products = []\n",
    "for prod_list in data['products'][pandas.notnull(data['products'])].values:\n",
    "    all_products += [' '.join(tokenizer.tokenize(product))\n",
    "                     for product in prod_list.tolist()]\n",
    "    \n",
    "counter = Counter(all_products)\n",
    "print('unique categories', len(counter.most_common()))\n",
    "\n",
    "most_common = [product[0] for product in counter.most_common(600)]\n",
    "\n",
    "def filter_categories(x):\n",
    "    new_categories = np.array([product \n",
    "                               for product in x.tolist()\n",
    "                               if product in most_common])\n",
    "    if new_categories.shape[0] == 0:\n",
    "        return np.nan\n",
    "    return new_categories\n",
    "\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "stopwords_cached = stopwords.words('english')\n",
    "\n",
    "def filter_descriptions(text):\n",
    "    cleaned_text = [token#lemma.lemmatize(token)\n",
    "                      for token in tokenizer.tokenize(text.lower())\n",
    "                      if token not in stopwords_cached]\n",
    "    if len(cleaned_text) == 0:\n",
    "        return np.nan\n",
    "    return ' '.join(cleaned_text)\n",
    "\n",
    "data['products'] = data['products'].apply(filter_categories)\n",
    "data['description'] = data['description'].apply(filter_descriptions)\n",
    "\n",
    "cleaned_data = data[(data['products'].notnull()) & (data['description'].notnull())].copy().reset_index(drop=True)\n",
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['products'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welch allyn combines practical understanding c...</td>\n",
       "      <td>[power supply, medical]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>line company intention support international g...</td>\n",
       "      <td>[imo, point, marine pollutant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>services redaelli ricambi offers ability produ...</td>\n",
       "      <td>[auto spare parts, auto spare, spare parts]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strothmann delivers suitable mechanical system...</td>\n",
       "      <td>[line]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>established tien jiang enterprise co ltd one s...</td>\n",
       "      <td>[rubber, polyester, nylon, support]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  welch allyn combines practical understanding c...   \n",
       "1  line company intention support international g...   \n",
       "2  services redaelli ricambi offers ability produ...   \n",
       "3  strothmann delivers suitable mechanical system...   \n",
       "4  established tien jiang enterprise co ltd one s...   \n",
       "\n",
       "                                      products  \n",
       "0                      [power supply, medical]  \n",
       "1               [imo, point, marine pollutant]  \n",
       "2  [auto spare parts, auto spare, spare parts]  \n",
       "3                                       [line]  \n",
       "4          [rubber, polyester, nylon, support]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers.fasttext import FastText\n",
    "\n",
    "model = FastText.load_fasttext_format('../models/wiki.simple.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dimaStolpakov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "error in word zc\n",
      "error in word zc\n",
      "error in word vz\n",
      "error in word vz\n",
      "error in word vz\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer = RegexpTokenizer(r'[a-z]+')\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "stopwords_cached = stopwords.words('english')\n",
    "\n",
    "def get_embedding_matrix(text, first_n=50):\n",
    "#     print(text)\n",
    "    matrix = []\n",
    "    text = [token#lemma.lemmatize(token)\n",
    "            for token in tokenizer.tokenize(text)]\n",
    "    for word in text[:first_n]:\n",
    "        try:\n",
    "            word_embedding = model.wv[word]\n",
    "            matrix.append(model.wv[word])\n",
    "        except KeyError:\n",
    "            print(\"error in word\", word)\n",
    "            matrix.append(np.zeros(300))\n",
    "    \n",
    "    matrix = np.array(matrix)\n",
    "    # fill text embeddings with seq_len < first_n with zeros\n",
    "#     print(matrix.shape[0])\n",
    "    if matrix.shape[0] < first_n:\n",
    "        matrix = np.vstack((matrix, np.zeros((first_n - matrix.shape[0], 300))))\n",
    "    return matrix\n",
    "\n",
    "embeddings = []\n",
    "for text in cleaned_data['description'].values:\n",
    "    embeddings.append(get_embedding_matrix(text))\n",
    "#     print(embeddings[-1].shape)\n",
    "    \n",
    "# onehot_y_df = pandas.get_dummies(data['severity'])\n",
    "\n",
    "train_data = np.array(embeddings)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([item.tolist() for item in cleaned_data['products'].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBatcher():\n",
    "    def __init__(self, X_, y_, batch_size_=30):\n",
    "        self.X_ = X_\n",
    "        self.y_ = y_\n",
    "        self.batch_size_ = batch_size_\n",
    "        self.from_ = 0\n",
    "        self.to = self.from_ + self.batch_size_\n",
    "\n",
    "    def next_batch(self):\n",
    "        if self.to == len(self.X_):\n",
    "            f = self.from_\n",
    "            t = self.to\n",
    "            self.from_ = 0\n",
    "            self.to = self.to = self.from_ + self.batch_size_\n",
    "            return self.X_[f:t], self.y_[f:t]\n",
    "        elif self.to > len(self.X_):\n",
    "            f = self.from_\n",
    "            t = self.to - len(self.X_)\n",
    "            self.from_ = self.to - len(self.X_)\n",
    "            self.to = self.from_ + self.batch_size_\n",
    "            return np.append(self.X_[f:], self.X_[:t], axis=0), np.append(self.y_[f:], self.y_[:t], axis=0)\n",
    "        else:\n",
    "            f = self.from_\n",
    "            t = self.to\n",
    "            self.from_ = self.to\n",
    "            self.to = self.from_ + self.batch_size_\n",
    "            return self.X_[f:t], self.y_[f:t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss= 14.3337, Train Acc= 0.000, Test Acc = 0.001\n",
      "Step 500, Loss= 13.2921, Train Acc= 0.020, Test Acc = 0.012\n",
      "Step 1000, Loss= 13.0609, Train Acc= 0.000, Test Acc = 0.012\n",
      "Step 1500, Loss= 11.9343, Train Acc= 0.000, Test Acc = 0.013\n",
      "Step 2000, Loss= 12.4185, Train Acc= 0.000, Test Acc = 0.012\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Training Parameters\n",
    "    learning_rate = 0.001\n",
    "    training_steps = 300000\n",
    "    batch_size = 50\n",
    "    display_step = 500\n",
    "    \n",
    "    # Network Parameters\n",
    "    num_input = 300 # MNIST data input (img shape: 28*28)\n",
    "    timesteps = 50 # timesteps\n",
    "    num_hidden = 128 # hidden layer num of features\n",
    "    num_classes = 600 # MNIST total classes (0-9 digits)\n",
    "    num_layers = 2\n",
    "    input_keep_prob = 0.6\n",
    "    output_keep_prob = 1\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([2 * num_hidden, num_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "    }\n",
    "\n",
    "    def RNN(x, weights, biases):\n",
    "\n",
    "        # Prepare data shape to match `rnn` function requirements\n",
    "        # Current data input shape: (batch_size, timesteps, n_input)\n",
    "        # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "        # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "        x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "        # Cerate forward and backward cells\n",
    "        cells_fw = [cell(num_hidden) for _ in range(num_layers)]\n",
    "        cells_bw = [cell(num_hidden) for _ in range(num_layers)]\n",
    "        \n",
    "        # Add dropout\n",
    "        cells_fw = [tf.contrib.rnn.DropoutWrapper(cell, \n",
    "                                                  input_keep_prob=input_keep_prob,\n",
    "                                                  output_keep_prob=output_keep_prob) for cell in cells_fw]\n",
    "        cells_bw = [tf.contrib.rnn.DropoutWrapper(cell, \n",
    "                                                  input_keep_prob=input_keep_prob,\n",
    "                                                  output_keep_prob=output_keep_prob) for cell in cells_bw]\n",
    "        \n",
    "        outputs, _, _ = tf.contrib.rnn.stack_bidirectional_rnn(\n",
    "            cells_fw=cells_fw,\n",
    "            cells_bw=cells_bw,\n",
    "            inputs=x,\n",
    "            dtype=tf.float32)\n",
    "        \n",
    "#         rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "        \n",
    "        return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "        \n",
    "\n",
    "    logits = RNN(X, weights, biases)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=Y))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    # Evaluate model (with test logits, for dropout to be disabled)\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    batcher = DataBatcher(X_train, y_train, batch_size_=batch_size)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = batcher.next_batch()\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "#         batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Train Acc= \" + \\\n",
    "                  \"{:.3f}\".format(acc) +  \", Test Acc = \" + \\\n",
    "                  \"{:.3f}\".format(sess.run(accuracy, feed_dict={X: X_test, Y: y_test})))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "#     # Calculate accuracy for 128 mnist test images\n",
    "#     print(\"Testing Accuracy:\", \\\n",
    "#         sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'steell' in most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(['qwe', '', 'rew'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sets = []\n",
    "\n",
    "for prod_list in data['products']:\n",
    "    if set(prod_list) not in unique_sets:\n",
    "        unique_sets.append(set(prod_list))\n",
    "        \n",
    "len(unique_sets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
