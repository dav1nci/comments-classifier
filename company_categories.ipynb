{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83897, 18)\n",
      "(47516, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pandas.read_parquet('../data/data.csv')\n",
    "print(data.shape)\n",
    "data = data[['description', 'products']][pandas.notnull(data['products'])].copy().reset_index(drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique categories 71111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35452, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "\n",
    "data['products'] = data['products'].apply(lambda x: np.array([' '.join(tokenizer.tokenize(product)).lower()\n",
    "                                                              for product in x.tolist()]))\n",
    "\n",
    "all_products = []\n",
    "for prod_list in data['products'][pandas.notnull(data['products'])].values:\n",
    "    all_products += [' '.join(tokenizer.tokenize(product))\n",
    "                     for product in prod_list.tolist()]\n",
    "    \n",
    "counter = Counter(all_products)\n",
    "print('unique categories', len(counter.most_common()))\n",
    "\n",
    "most_common = [product[0] for product in counter.most_common(600)]\n",
    "\n",
    "def filter_categories(x):\n",
    "    new_categories = np.array([product \n",
    "                               for product in x.tolist()\n",
    "                               if product in most_common])\n",
    "    if new_categories.shape[0] == 0:\n",
    "        return np.nan\n",
    "    return new_categories\n",
    "\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "stopwords_cached = stopwords.words('english')\n",
    "\n",
    "def filter_descriptions(text):\n",
    "    cleaned_text = [token#lemma.lemmatize(token)\n",
    "                      for token in tokenizer.tokenize(text.lower())\n",
    "                      if token not in stopwords_cached]\n",
    "    if len(cleaned_text) == 0:\n",
    "        return np.nan\n",
    "    return ' '.join(cleaned_text)\n",
    "\n",
    "data['products'] = data['products'].apply(filter_categories)\n",
    "data['description'] = data['description'].apply(filter_descriptions)\n",
    "\n",
    "cleaned_data = data[(data['products'].notnull()) & (data['description'].notnull())].copy().reset_index(drop=True)\n",
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welch allyn combines practical understanding c...</td>\n",
       "      <td>[power supply, medical]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>line company intention support international g...</td>\n",
       "      <td>[imo, point, marine pollutant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>services redaelli ricambi offers ability produ...</td>\n",
       "      <td>[auto spare parts, auto spare, spare parts]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strothmann delivers suitable mechanical system...</td>\n",
       "      <td>[line]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>established tien jiang enterprise co ltd one s...</td>\n",
       "      <td>[rubber, polyester, nylon, support]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  welch allyn combines practical understanding c...   \n",
       "1  line company intention support international g...   \n",
       "2  services redaelli ricambi offers ability produ...   \n",
       "3  strothmann delivers suitable mechanical system...   \n",
       "4  established tien jiang enterprise co ltd one s...   \n",
       "\n",
       "                                      products  \n",
       "0                      [power supply, medical]  \n",
       "1               [imo, point, marine pollutant]  \n",
       "2  [auto spare parts, auto spare, spare parts]  \n",
       "3                                       [line]  \n",
       "4          [rubber, polyester, nylon, support]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers.fasttext import FastText\n",
    "\n",
    "model = FastText.load_fasttext_format('../models/wiki.simple.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in word zc\n",
      "error in word zc\n",
      "error in word vz\n",
      "error in word vz\n",
      "error in word vz\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer = RegexpTokenizer(r'[a-z]+')\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "stopwords_cached = stopwords.words('english')\n",
    "\n",
    "def get_embedding_matrix(text, first_n=50):\n",
    "#     print(text)\n",
    "    matrix = []\n",
    "    text = [token#lemma.lemmatize(token)\n",
    "            for token in tokenizer.tokenize(text)]\n",
    "    for word in text[:first_n]:\n",
    "        try:\n",
    "            word_embedding = model.wv[word]\n",
    "            matrix.append(model.wv[word])\n",
    "        except KeyError:\n",
    "            print(\"error in word\", word)\n",
    "            matrix.append(np.zeros(300))\n",
    "    \n",
    "    matrix = np.array(matrix)\n",
    "    # fill text embeddings with seq_len < first_n with zeros\n",
    "#     print(matrix.shape[0])\n",
    "    if matrix.shape[0] < first_n:\n",
    "        matrix = np.vstack((matrix, np.zeros((first_n - matrix.shape[0], 300))))\n",
    "    return matrix\n",
    "\n",
    "embeddings = []\n",
    "for text in cleaned_data['description'].values:\n",
    "    embeddings.append(get_embedding_matrix(text))\n",
    "#     print(embeddings[-1].shape)\n",
    "    \n",
    "# onehot_y_df = pandas.get_dummies(data['severity'])\n",
    "\n",
    "train_data = np.array(embeddings)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([item.tolist() for item in cleaned_data['products'].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class DataBatcher():\n",
    "    def __init__(self, _X, _y, _batch_size=30):\n",
    "        self._X = _X\n",
    "        self._y = _y\n",
    "        self._batch_size = _batch_size\n",
    "        self._resplit = True\n",
    "        self._num_examples = self._y.shape[0]\n",
    "    \n",
    "    def next_batch(self):\n",
    "        if self._resplit:\n",
    "#             print('splitting')\n",
    "            perm0 = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm0)\n",
    "            self._batches_indexes = np.array_split(perm0, math.ceil(perm0.shape[0] / self._batch_size))\n",
    "#             print(self._batches_indexes)\n",
    "            self._batch_counter = -1\n",
    "            self._resplit = False\n",
    "\n",
    "        self._batch_counter += 1\n",
    "        if self._batches_indexes[self._batch_counter].shape[0] < self._batch_size:\n",
    "#             print('hstacking')\n",
    "            self._resplit = True\n",
    "            ind = self._batch_counter\n",
    "#             self._batch_counter = -1\n",
    "            missing_num = self._batch_size - self._batches_indexes[ind].shape[0]\n",
    "            return self._X[np.hstack((self._batches_indexes[ind], self._batches_indexes[0][:missing_num]))],\\\n",
    "                   self._y[np.hstack((self._batches_indexes[ind], self._batches_indexes[0][:missing_num]))]\n",
    "        \n",
    "        return self._X[self._batches_indexes[self._batch_counter]], self._y[self._batches_indexes[self._batch_counter]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss= 0.9152, Train Acc= 0.551, Test Acc = 0.417\n",
      "Step 1000, Loss= 0.0243, Train Acc= 0.000, Test Acc = 0.000\n",
      "Step 2000, Loss= 0.0240, Train Acc= 0.000, Test Acc = 0.001\n",
      "Step 3000, Loss= 0.0203, Train Acc= 0.003, Test Acc = 0.005\n",
      "Step 4000, Loss= 0.0223, Train Acc= 0.000, Test Acc = 0.012\n",
      "Step 5000, Loss= 0.0227, Train Acc= 0.000, Test Acc = 0.001\n",
      "Step 6000, Loss= 0.0217, Train Acc= 0.000, Test Acc = 0.013\n",
      "Step 7000, Loss= 0.0201, Train Acc= 0.006, Test Acc = 0.013\n",
      "Step 8000, Loss= 0.0182, Train Acc= 0.017, Test Acc = 0.020\n",
      "Step 9000, Loss= 0.0216, Train Acc= 0.025, Test Acc = 0.023\n",
      "Step 10000, Loss= 0.0192, Train Acc= 0.019, Test Acc = 0.024\n",
      "Step 11000, Loss= 0.0171, Train Acc= 0.020, Test Acc = 0.024\n",
      "Step 12000, Loss= 0.0200, Train Acc= 0.042, Test Acc = 0.025\n",
      "Step 13000, Loss= 0.0175, Train Acc= 0.019, Test Acc = 0.027\n",
      "Step 14000, Loss= 0.0164, Train Acc= 0.054, Test Acc = 0.028\n",
      "Step 15000, Loss= 0.0172, Train Acc= 0.040, Test Acc = 0.036\n",
      "Step 16000, Loss= 0.0166, Train Acc= 0.068, Test Acc = 0.035\n",
      "Step 17000, Loss= 0.0167, Train Acc= 0.028, Test Acc = 0.038\n",
      "Step 18000, Loss= 0.0165, Train Acc= 0.062, Test Acc = 0.037\n",
      "Step 19000, Loss= 0.0170, Train Acc= 0.074, Test Acc = 0.043\n",
      "Step 20000, Loss= 0.0170, Train Acc= 0.109, Test Acc = 0.045\n",
      "Step 21000, Loss= 0.0157, Train Acc= 0.035, Test Acc = 0.047\n",
      "Step 22000, Loss= 0.0126, Train Acc= 0.179, Test Acc = 0.043\n",
      "Step 23000, Loss= 0.0147, Train Acc= 0.090, Test Acc = 0.048\n",
      "Step 24000, Loss= 0.0139, Train Acc= 0.128, Test Acc = 0.050\n",
      "Step 25000, Loss= 0.0130, Train Acc= 0.166, Test Acc = 0.050\n",
      "Step 26000, Loss= 0.0134, Train Acc= 0.190, Test Acc = 0.050\n",
      "Step 27000, Loss= 0.0130, Train Acc= 0.198, Test Acc = 0.053\n",
      "Step 28000, Loss= 0.0114, Train Acc= 0.214, Test Acc = 0.053\n",
      "Step 29000, Loss= 0.0114, Train Acc= 0.113, Test Acc = 0.054\n",
      "Step 30000, Loss= 0.0126, Train Acc= 0.145, Test Acc = 0.055\n",
      "Step 31000, Loss= 0.0110, Train Acc= 0.183, Test Acc = 0.055\n",
      "Step 32000, Loss= 0.0105, Train Acc= 0.266, Test Acc = 0.055\n",
      "Step 33000, Loss= 0.0095, Train Acc= 0.298, Test Acc = 0.056\n",
      "Step 34000, Loss= 0.0093, Train Acc= 0.311, Test Acc = 0.057\n",
      "Step 35000, Loss= 0.0101, Train Acc= 0.351, Test Acc = 0.058\n",
      "Step 36000, Loss= 0.0081, Train Acc= 0.371, Test Acc = 0.059\n",
      "Step 37000, Loss= 0.0090, Train Acc= 0.245, Test Acc = 0.056\n",
      "Step 38000, Loss= 0.0075, Train Acc= 0.380, Test Acc = 0.062\n",
      "Step 39000, Loss= 0.0094, Train Acc= 0.323, Test Acc = 0.055\n",
      "Step 40000, Loss= 0.0072, Train Acc= 0.383, Test Acc = 0.059\n",
      "Step 41000, Loss= 0.0084, Train Acc= 0.364, Test Acc = 0.061\n",
      "Step 42000, Loss= 0.0098, Train Acc= 0.286, Test Acc = 0.061\n",
      "Step 43000, Loss= 0.0082, Train Acc= 0.329, Test Acc = 0.066\n",
      "Step 44000, Loss= 0.0088, Train Acc= 0.352, Test Acc = 0.064\n",
      "Step 45000, Loss= 0.0074, Train Acc= 0.396, Test Acc = 0.062\n",
      "Step 46000, Loss= 0.0071, Train Acc= 0.464, Test Acc = 0.066\n",
      "Step 47000, Loss= 0.0074, Train Acc= 0.462, Test Acc = 0.062\n",
      "Step 48000, Loss= 0.0073, Train Acc= 0.467, Test Acc = 0.064\n",
      "Step 49000, Loss= 0.0074, Train Acc= 0.481, Test Acc = 0.064\n",
      "Step 50000, Loss= 0.0070, Train Acc= 0.451, Test Acc = 0.066\n",
      "Step 51000, Loss= 0.0075, Train Acc= 0.482, Test Acc = 0.061\n",
      "Step 52000, Loss= 0.0083, Train Acc= 0.397, Test Acc = 0.067\n",
      "Step 53000, Loss= 0.0060, Train Acc= 0.490, Test Acc = 0.066\n",
      "Step 54000, Loss= 0.0056, Train Acc= 0.579, Test Acc = 0.065\n",
      "Step 55000, Loss= 0.0066, Train Acc= 0.507, Test Acc = 0.064\n",
      "Step 56000, Loss= 0.0058, Train Acc= 0.552, Test Acc = 0.065\n",
      "Step 57000, Loss= 0.0071, Train Acc= 0.443, Test Acc = 0.066\n",
      "Step 58000, Loss= 0.0049, Train Acc= 0.608, Test Acc = 0.061\n",
      "Step 59000, Loss= 0.0050, Train Acc= 0.555, Test Acc = 0.067\n",
      "Step 60000, Loss= 0.0049, Train Acc= 0.622, Test Acc = 0.066\n",
      "Step 61000, Loss= 0.0050, Train Acc= 0.596, Test Acc = 0.065\n",
      "Step 62000, Loss= 0.0040, Train Acc= 0.632, Test Acc = 0.067\n",
      "Step 63000, Loss= 0.0046, Train Acc= 0.598, Test Acc = 0.067\n",
      "Step 64000, Loss= 0.0053, Train Acc= 0.600, Test Acc = 0.065\n",
      "Step 65000, Loss= 0.0047, Train Acc= 0.664, Test Acc = 0.065\n",
      "Step 66000, Loss= 0.0039, Train Acc= 0.645, Test Acc = 0.063\n",
      "Step 67000, Loss= 0.0042, Train Acc= 0.648, Test Acc = 0.069\n",
      "Step 68000, Loss= 0.0044, Train Acc= 0.683, Test Acc = 0.063\n",
      "Step 69000, Loss= 0.0047, Train Acc= 0.671, Test Acc = 0.065\n",
      "Step 70000, Loss= 0.0045, Train Acc= 0.636, Test Acc = 0.069\n",
      "Step 71000, Loss= 0.0039, Train Acc= 0.635, Test Acc = 0.069\n",
      "Step 72000, Loss= 0.0037, Train Acc= 0.708, Test Acc = 0.069\n",
      "Step 73000, Loss= 0.0046, Train Acc= 0.675, Test Acc = 0.068\n",
      "Step 74000, Loss= 0.0036, Train Acc= 0.709, Test Acc = 0.066\n",
      "Step 75000, Loss= 0.0045, Train Acc= 0.685, Test Acc = 0.069\n",
      "Step 76000, Loss= 0.0038, Train Acc= 0.614, Test Acc = 0.069\n",
      "Step 77000, Loss= 0.0043, Train Acc= 0.667, Test Acc = 0.068\n",
      "Step 78000, Loss= 0.0038, Train Acc= 0.704, Test Acc = 0.070\n",
      "Step 79000, Loss= 0.0042, Train Acc= 0.721, Test Acc = 0.066\n",
      "Step 80000, Loss= 0.0035, Train Acc= 0.711, Test Acc = 0.067\n",
      "Step 81000, Loss= 0.0040, Train Acc= 0.680, Test Acc = 0.069\n",
      "Step 82000, Loss= 0.0045, Train Acc= 0.700, Test Acc = 0.069\n",
      "Step 83000, Loss= 0.0035, Train Acc= 0.758, Test Acc = 0.068\n",
      "Step 84000, Loss= 0.0036, Train Acc= 0.733, Test Acc = 0.073\n",
      "Step 85000, Loss= 0.0027, Train Acc= 0.780, Test Acc = 0.070\n",
      "Step 86000, Loss= 0.0034, Train Acc= 0.658, Test Acc = 0.067\n",
      "Step 87000, Loss= 0.0033, Train Acc= 0.704, Test Acc = 0.068\n",
      "Step 88000, Loss= 0.0033, Train Acc= 0.729, Test Acc = 0.070\n",
      "Step 89000, Loss= 0.0041, Train Acc= 0.739, Test Acc = 0.069\n",
      "Step 90000, Loss= 0.0039, Train Acc= 0.760, Test Acc = 0.070\n",
      "Step 91000, Loss= 0.0025, Train Acc= 0.831, Test Acc = 0.069\n",
      "Step 92000, Loss= 0.0031, Train Acc= 0.724, Test Acc = 0.068\n",
      "Step 93000, Loss= 0.0030, Train Acc= 0.785, Test Acc = 0.067\n",
      "Step 94000, Loss= 0.0033, Train Acc= 0.783, Test Acc = 0.067\n",
      "Step 95000, Loss= 0.0030, Train Acc= 0.761, Test Acc = 0.068\n",
      "Step 96000, Loss= 0.0029, Train Acc= 0.726, Test Acc = 0.069\n",
      "Step 97000, Loss= 0.0035, Train Acc= 0.741, Test Acc = 0.067\n",
      "Step 98000, Loss= 0.0032, Train Acc= 0.789, Test Acc = 0.068\n",
      "Step 99000, Loss= 0.0025, Train Acc= 0.794, Test Acc = 0.069\n",
      "Step 100000, Loss= 0.0026, Train Acc= 0.845, Test Acc = 0.072\n",
      "Step 101000, Loss= 0.0036, Train Acc= 0.694, Test Acc = 0.072\n",
      "Step 102000, Loss= 0.0027, Train Acc= 0.765, Test Acc = 0.071\n",
      "Step 103000, Loss= 0.0026, Train Acc= 0.850, Test Acc = 0.071\n",
      "Step 104000, Loss= 0.0022, Train Acc= 0.858, Test Acc = 0.066\n",
      "Step 105000, Loss= 0.0022, Train Acc= 0.841, Test Acc = 0.070\n",
      "Step 106000, Loss= 0.0030, Train Acc= 0.752, Test Acc = 0.071\n",
      "Step 107000, Loss= 0.0022, Train Acc= 0.814, Test Acc = 0.074\n",
      "Step 108000, Loss= 0.0028, Train Acc= 0.809, Test Acc = 0.072\n",
      "Step 109000, Loss= 0.0026, Train Acc= 0.831, Test Acc = 0.073\n",
      "Step 110000, Loss= 0.0017, Train Acc= 0.872, Test Acc = 0.070\n",
      "Step 111000, Loss= 0.0025, Train Acc= 0.767, Test Acc = 0.074\n",
      "Step 112000, Loss= 0.0027, Train Acc= 0.809, Test Acc = 0.071\n",
      "Step 113000, Loss= 0.0024, Train Acc= 0.829, Test Acc = 0.070\n",
      "Step 114000, Loss= 0.0032, Train Acc= 0.820, Test Acc = 0.071\n",
      "Step 115000, Loss= 0.0028, Train Acc= 0.750, Test Acc = 0.073\n",
      "Step 116000, Loss= 0.0030, Train Acc= 0.847, Test Acc = 0.071\n",
      "Step 117000, Loss= 0.0028, Train Acc= 0.804, Test Acc = 0.072\n",
      "Step 118000, Loss= 0.0023, Train Acc= 0.877, Test Acc = 0.071\n",
      "Step 119000, Loss= 0.0019, Train Acc= 0.818, Test Acc = 0.070\n",
      "Step 120000, Loss= 0.0019, Train Acc= 0.860, Test Acc = 0.072\n",
      "Step 121000, Loss= 0.0022, Train Acc= 0.824, Test Acc = 0.069\n",
      "Step 122000, Loss= 0.0025, Train Acc= 0.812, Test Acc = 0.070\n",
      "Step 123000, Loss= 0.0024, Train Acc= 0.785, Test Acc = 0.072\n",
      "Step 124000, Loss= 0.0021, Train Acc= 0.852, Test Acc = 0.072\n",
      "Step 125000, Loss= 0.0024, Train Acc= 0.820, Test Acc = 0.070\n",
      "Step 126000, Loss= 0.0023, Train Acc= 0.848, Test Acc = 0.069\n",
      "Step 127000, Loss= 0.0022, Train Acc= 0.867, Test Acc = 0.073\n",
      "Step 128000, Loss= 0.0022, Train Acc= 0.860, Test Acc = 0.074\n",
      "Step 129000, Loss= 0.0027, Train Acc= 0.822, Test Acc = 0.071\n",
      "Step 130000, Loss= 0.0028, Train Acc= 0.750, Test Acc = 0.071\n",
      "Step 131000, Loss= 0.0028, Train Acc= 0.815, Test Acc = 0.068\n",
      "Step 132000, Loss= 0.0023, Train Acc= 0.877, Test Acc = 0.072\n",
      "Step 133000, Loss= 0.0025, Train Acc= 0.821, Test Acc = 0.076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 134000, Loss= 0.0025, Train Acc= 0.791, Test Acc = 0.070\n",
      "Step 135000, Loss= 0.0015, Train Acc= 0.887, Test Acc = 0.073\n",
      "Step 136000, Loss= 0.0022, Train Acc= 0.866, Test Acc = 0.070\n",
      "Step 137000, Loss= 0.0021, Train Acc= 0.841, Test Acc = 0.070\n",
      "Step 138000, Loss= 0.0021, Train Acc= 0.804, Test Acc = 0.071\n",
      "Step 139000, Loss= 0.0019, Train Acc= 0.897, Test Acc = 0.067\n",
      "Step 140000, Loss= 0.0016, Train Acc= 0.904, Test Acc = 0.071\n",
      "Step 141000, Loss= 0.0015, Train Acc= 0.926, Test Acc = 0.071\n",
      "Step 142000, Loss= 0.0022, Train Acc= 0.823, Test Acc = 0.069\n",
      "Step 143000, Loss= 0.0013, Train Acc= 0.864, Test Acc = 0.069\n",
      "Step 144000, Loss= 0.0012, Train Acc= 0.914, Test Acc = 0.073\n",
      "Step 145000, Loss= 0.0020, Train Acc= 0.805, Test Acc = 0.071\n",
      "Step 146000, Loss= 0.0022, Train Acc= 0.863, Test Acc = 0.070\n",
      "Step 147000, Loss= 0.0018, Train Acc= 0.876, Test Acc = 0.069\n",
      "Step 148000, Loss= 0.0023, Train Acc= 0.871, Test Acc = 0.072\n",
      "Step 149000, Loss= 0.0021, Train Acc= 0.890, Test Acc = 0.071\n",
      "Step 150000, Loss= 0.0029, Train Acc= 0.807, Test Acc = 0.069\n",
      "Step 151000, Loss= 0.0023, Train Acc= 0.818, Test Acc = 0.070\n",
      "Step 152000, Loss= 0.0029, Train Acc= 0.833, Test Acc = 0.070\n",
      "Step 153000, Loss= 0.0019, Train Acc= 0.901, Test Acc = 0.074\n",
      "Step 154000, Loss= 0.0015, Train Acc= 0.920, Test Acc = 0.073\n",
      "Step 155000, Loss= 0.0018, Train Acc= 0.870, Test Acc = 0.072\n",
      "Step 156000, Loss= 0.0013, Train Acc= 0.891, Test Acc = 0.074\n",
      "Step 157000, Loss= 0.0026, Train Acc= 0.841, Test Acc = 0.071\n",
      "Step 158000, Loss= 0.0020, Train Acc= 0.851, Test Acc = 0.071\n",
      "Step 159000, Loss= 0.0017, Train Acc= 0.877, Test Acc = 0.072\n",
      "Step 160000, Loss= 0.0014, Train Acc= 0.906, Test Acc = 0.072\n",
      "Step 161000, Loss= 0.0027, Train Acc= 0.875, Test Acc = 0.069\n",
      "Step 162000, Loss= 0.0020, Train Acc= 0.890, Test Acc = 0.073\n",
      "Step 163000, Loss= 0.0016, Train Acc= 0.854, Test Acc = 0.070\n",
      "Step 164000, Loss= 0.0017, Train Acc= 0.919, Test Acc = 0.074\n",
      "Step 165000, Loss= 0.0017, Train Acc= 0.901, Test Acc = 0.070\n",
      "Step 166000, Loss= 0.0019, Train Acc= 0.862, Test Acc = 0.075\n",
      "Step 167000, Loss= 0.0019, Train Acc= 0.884, Test Acc = 0.073\n",
      "Step 168000, Loss= 0.0016, Train Acc= 0.856, Test Acc = 0.072\n",
      "Step 169000, Loss= 0.0013, Train Acc= 0.901, Test Acc = 0.075\n",
      "Step 170000, Loss= 0.0012, Train Acc= 0.916, Test Acc = 0.074\n",
      "Step 171000, Loss= 0.0016, Train Acc= 0.933, Test Acc = 0.071\n",
      "Step 172000, Loss= 0.0013, Train Acc= 0.902, Test Acc = 0.071\n",
      "Step 173000, Loss= 0.0024, Train Acc= 0.857, Test Acc = 0.073\n",
      "Step 174000, Loss= 0.0014, Train Acc= 0.860, Test Acc = 0.070\n",
      "Step 175000, Loss= 0.0018, Train Acc= 0.899, Test Acc = 0.071\n",
      "Step 176000, Loss= 0.0016, Train Acc= 0.898, Test Acc = 0.071\n",
      "Step 177000, Loss= 0.0018, Train Acc= 0.883, Test Acc = 0.070\n",
      "Step 178000, Loss= 0.0013, Train Acc= 0.896, Test Acc = 0.070\n",
      "Step 179000, Loss= 0.0017, Train Acc= 0.906, Test Acc = 0.073\n",
      "Step 180000, Loss= 0.0017, Train Acc= 0.873, Test Acc = 0.070\n",
      "Step 181000, Loss= 0.0017, Train Acc= 0.862, Test Acc = 0.068\n",
      "Step 182000, Loss= 0.0017, Train Acc= 0.896, Test Acc = 0.072\n",
      "Step 183000, Loss= 0.0015, Train Acc= 0.930, Test Acc = 0.074\n",
      "Step 184000, Loss= 0.0023, Train Acc= 0.852, Test Acc = 0.074\n",
      "Step 185000, Loss= 0.0009, Train Acc= 0.936, Test Acc = 0.072\n",
      "Step 186000, Loss= 0.0024, Train Acc= 0.868, Test Acc = 0.074\n",
      "Step 187000, Loss= 0.0014, Train Acc= 0.897, Test Acc = 0.071\n",
      "Step 188000, Loss= 0.0012, Train Acc= 0.925, Test Acc = 0.070\n",
      "Step 189000, Loss= 0.0019, Train Acc= 0.829, Test Acc = 0.070\n",
      "Step 190000, Loss= 0.0022, Train Acc= 0.854, Test Acc = 0.073\n",
      "Step 191000, Loss= 0.0019, Train Acc= 0.906, Test Acc = 0.074\n",
      "Step 192000, Loss= 0.0025, Train Acc= 0.830, Test Acc = 0.070\n",
      "Step 193000, Loss= 0.0020, Train Acc= 0.842, Test Acc = 0.069\n",
      "Step 194000, Loss= 0.0018, Train Acc= 0.865, Test Acc = 0.071\n",
      "Step 195000, Loss= 0.0020, Train Acc= 0.852, Test Acc = 0.071\n",
      "Step 196000, Loss= 0.0014, Train Acc= 0.928, Test Acc = 0.073\n",
      "Step 197000, Loss= 0.0010, Train Acc= 0.941, Test Acc = 0.073\n",
      "Step 198000, Loss= 0.0021, Train Acc= 0.826, Test Acc = 0.074\n",
      "Step 199000, Loss= 0.0012, Train Acc= 0.872, Test Acc = 0.073\n",
      "Step 200000, Loss= 0.0024, Train Acc= 0.852, Test Acc = 0.073\n",
      "Step 201000, Loss= 0.0014, Train Acc= 0.865, Test Acc = 0.073\n",
      "Step 202000, Loss= 0.0018, Train Acc= 0.887, Test Acc = 0.070\n",
      "Step 203000, Loss= 0.0018, Train Acc= 0.862, Test Acc = 0.074\n",
      "Step 204000, Loss= 0.0017, Train Acc= 0.883, Test Acc = 0.073\n",
      "Step 205000, Loss= 0.0009, Train Acc= 0.960, Test Acc = 0.071\n",
      "Step 206000, Loss= 0.0014, Train Acc= 0.870, Test Acc = 0.072\n",
      "Step 207000, Loss= 0.0014, Train Acc= 0.876, Test Acc = 0.073\n",
      "Step 208000, Loss= 0.0013, Train Acc= 0.948, Test Acc = 0.071\n",
      "Step 209000, Loss= 0.0012, Train Acc= 0.906, Test Acc = 0.072\n",
      "Step 210000, Loss= 0.0011, Train Acc= 0.907, Test Acc = 0.073\n",
      "Step 211000, Loss= 0.0018, Train Acc= 0.885, Test Acc = 0.074\n",
      "Step 212000, Loss= 0.0014, Train Acc= 0.929, Test Acc = 0.073\n",
      "Step 213000, Loss= 0.0011, Train Acc= 0.914, Test Acc = 0.073\n",
      "Step 214000, Loss= 0.0017, Train Acc= 0.910, Test Acc = 0.074\n",
      "Step 215000, Loss= 0.0016, Train Acc= 0.817, Test Acc = 0.075\n",
      "Step 216000, Loss= 0.0017, Train Acc= 0.879, Test Acc = 0.072\n",
      "Step 217000, Loss= 0.0010, Train Acc= 0.909, Test Acc = 0.073\n",
      "Step 218000, Loss= 0.0014, Train Acc= 0.927, Test Acc = 0.077\n",
      "Step 219000, Loss= 0.0021, Train Acc= 0.883, Test Acc = 0.072\n",
      "Step 220000, Loss= 0.0012, Train Acc= 0.906, Test Acc = 0.074\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Training Parameters\n",
    "    learning_rate = 0.001\n",
    "    training_steps = 220000\n",
    "    batch_size = 80\n",
    "    display_step = 1000\n",
    "    checkpoint_step = 20000\n",
    "    \n",
    "    # Network Parameters\n",
    "    num_input = 300 # MNIST data input (img shape: 28*28)\n",
    "    timesteps = 50 # timesteps\n",
    "    num_hidden = 400 # hidden layer num of features\n",
    "    num_classes = 600 # MNIST total classes (0-9 digits)\n",
    "    num_layers = 3\n",
    "    input_keep_prob = 0.5\n",
    "    output_keep_prob = 0.5\n",
    "    cell = tf.contrib.rnn.LSTMCell\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([2 * num_hidden, num_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "    }\n",
    "\n",
    "    def RNN(x, weights, biases):\n",
    "\n",
    "        # Prepare data shape to match `rnn` function requirements\n",
    "        # Current data input shape: (batch_size, timesteps, n_input)\n",
    "        # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "        # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "        x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "        # Cerate forward and backward cells\n",
    "        cells_fw = [cell(num_hidden, activation=tf.tanh) for _ in range(num_layers)]\n",
    "        cells_bw = [cell(num_hidden, activation=tf.tanh) for _ in range(num_layers)]\n",
    "#         cells_fw.append(cell(num_hidden, activation=tf.tanh))\n",
    "#         cells_bw.append(cell(num_hidden, activation=tf.tanh))\n",
    "        \n",
    "        # Add dropout\n",
    "        cells_fw = [tf.contrib.rnn.DropoutWrapper(cell, \n",
    "                                                  input_keep_prob=input_keep_prob,\n",
    "                                                  output_keep_prob=output_keep_prob\n",
    "                                                 ) for cell in cells_fw]\n",
    "        cells_bw = [tf.contrib.rnn.DropoutWrapper(cell, \n",
    "                                                  input_keep_prob=input_keep_prob,\n",
    "                                                  output_keep_prob=output_keep_prob\n",
    "                                                 ) for cell in cells_bw]\n",
    "        \n",
    "        outputs, _, _ = tf.contrib.rnn.stack_bidirectional_rnn(\n",
    "            cells_fw=cells_fw,\n",
    "            cells_bw=cells_bw,\n",
    "            inputs=x,\n",
    "            dtype=tf.float32)\n",
    "        \n",
    "#         rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "        \n",
    "        return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "        \n",
    "\n",
    "    logits = RNN(X, weights, biases)\n",
    "    prediction = tf.nn.sigmoid(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    \n",
    "\n",
    "# tf.nn.sigmoid_cross_entropy_with_logits \n",
    "#     loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "#         logits=logits, labels=Y))\n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=logits, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    # Evaluate model (with test logits, for dropout to be disabled)\n",
    "#     correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "def evaluate_multilabel(y_pred, y_true):\n",
    "    acc = []\n",
    "    for y_pred_tmp, y_true_tmp in zip(y_pred, y_true):\n",
    "#         print('y_true_tmp', y_true_tmp, 'y_pred_tmp', y_pred_tmp)\n",
    "        real_ = np.nonzero(y_true_tmp)[0].tolist()\n",
    "#         right_num = len(real)\n",
    "        pred_ = np.nonzero(y_pred_tmp)[0].tolist()\n",
    "#         print('real', real, 'pred', pred)\n",
    "        if len(real_) == 0:\n",
    "            #means 0 right answers\n",
    "            acc.append(0.0)\n",
    "            continue\n",
    "        acc.append(len(set(real_).intersection(set(pred_))) / len(real_))\n",
    "#     print(acc)\n",
    "    return(np.array(acc).mean())\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4)\n",
    "sess = tf.InteractiveSession(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "# with tf.Session(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "batcher = DataBatcher(X_train, y_train, _batch_size=batch_size)\n",
    "\n",
    "for step in range(1, training_steps+1):\n",
    "    batch_x, batch_y = batcher.next_batch()\n",
    "    # Reshape data to get 28 seq of 28 elements\n",
    "#         batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "    # Run optimization op (backprop)\n",
    "    sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        # Calculate batch loss and accuracy\n",
    "        loss, pred = sess.run([loss_op, tf.round(prediction)], feed_dict={X: batch_x, Y: batch_y})\n",
    "        \n",
    "        test_out = sess.run(tf.round(prediction), feed_dict={X: X_test})\n",
    "        print(\"Step \" + str(step) + \", Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss) + \", Train Acc= \" + \\\n",
    "              \"{:.3f}\".format(evaluate_multilabel(y_pred=pred, y_true=batch_y)) +  \", Test Acc = \" + \\\n",
    "              \"{:.3f}\".format(evaluate_multilabel(y_pred=test_out, y_true=y_test)))\n",
    "        \n",
    "    if step % checkpoint_step == 0:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, '../models/lstm_big_dropout/lstm_big_dropout', global_step=step)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, '../models/lstm_big_dropout/lstm_big_dropout', global_step=step)\n",
    "        \n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "#     # Calculate accuracy for 128 mnist test images\n",
    "#     print(\"Testing Accuracy:\", \\\n",
    "#         sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 1\n",
    "pred = tf.round(tf.nn.sigmoid(logits))\n",
    "\n",
    "def evaluate_multilabel(y_pred, y_true):\n",
    "    acc = []\n",
    "    for y_pred_tmp, y_true_tmp in zip(y_pred, y_true):\n",
    "#         print('y_true_tmp', y_true_tmp, 'y_pred_tmp', y_pred_tmp)\n",
    "        real_ = np.nonzero(y_true_tmp)[0].tolist()\n",
    "#         right_num = len(real)\n",
    "        pred_ = np.nonzero(y_pred_tmp)[0].tolist()\n",
    "#         print('real', real, 'pred', pred)\n",
    "        if len(real) == 0:\n",
    "            #means 0 right answers\n",
    "            acc.append(0.0)\n",
    "            continue\n",
    "        acc.append(len(set(real_).intersection(set(pred_))) / len(real_))\n",
    "#     print(acc)\n",
    "    return(np.array(acc).mean())\n",
    "\n",
    "# correct_predictions = tf.equal(tf.round(tf.nn.sigmoid(logits)), Y)\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "# pred = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "pred = sess.run(pred, feed_dict={X: X_test, Y: y_test})\n",
    "evaluate_multilabel(pred, y_test)\n",
    "# real = y_test[num]\n",
    "# tf.equal(tf.round(tf.nn.sigmoid(pred)), tf.round(y_))\n",
    "# pred > 0.5\n",
    "\n",
    "# real = np.nonzero(real)[0].tolist()\n",
    "# right_num = len(real)\n",
    "# pred = np.argpartition(pred, -right_num)[-right_num:]\n",
    "# print(real, pred)\n",
    "# print('real_classes', mlb.classes_[np.array(real)])\n",
    "# print('predicted classes', mlb.classes_[np.array(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    save = tf.train.Saver()\n",
    "    save.restore(session, '../models/lstm_relu/lstm_relu-150000')\n",
    "    pred = tf.round(tf.nn.sigmoid(logits))\n",
    "    print(session.run(pred, feed_dict={X: X_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['dthc']\n",
    "def filter_categories(cat_list):\n",
    "    return set(cat_list).intersection(query) == set(query)\n",
    "data[data['products'].apply(filter_categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(['qwe', 'asd']).intersection(['qwe', 'asd']) == set(['asd', 'qwe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batcher = DataBatcher(X_train, y_train, batch_size_=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.train.images.shape)\n",
    "print(mnist.train.labels.shape)\n",
    "print(mnist.test.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ = sess.run(prediction, feed_dict={X: [batch_x[3]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(predict_)\n",
    "# pred\n",
    "# batch_y[3]\n",
    "predict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = 5\n",
    "predict_ = sess.run(prediction, feed_dict={X: [batch_x[obj]]})\n",
    "real = np.nonzero(batch_y[obj])[0].tolist()\n",
    "# batch_y[1]\n",
    "pred = np.argpartition(predict_[0], -len(real))[-len(real):]\n",
    "print('real', real)\n",
    "print('pred', pred)\n",
    "print('classes real', mlb.classes_[real])\n",
    "print('classes predicted', mlb.classes_[pred])\n",
    "\n",
    "len(set(real).intersection(set(pred))) / len(real)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
