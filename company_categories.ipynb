{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83897, 18)\n",
      "(47516, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pandas.read_parquet('../data/to_send.pq')\n",
    "print(data.shape)\n",
    "data = data[['description', 'products']][pandas.notnull(data['products'])].copy().reset_index(drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Welch allyn combines its practical understandi...</td>\n",
       "      <td>[power supply, body sub assy, medical, valve b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In  line  with  the  company  s intention  to ...</td>\n",
       "      <td>[imo, advertising materials, point, imo label,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Services redaelli ricambi offers the ability t...</td>\n",
       "      <td>[auto spare parts, tie rod, tie rod end, auto ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STROTHMANN not only delivers suitable mechanic...</td>\n",
       "      <td>[covers non automated, demurrage rules form, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Established\\nin 1991, tien jiang enterprise co...</td>\n",
       "      <td>[rubber, polyester, nylon, boot, support]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  Welch allyn combines its practical understandi...   \n",
       "1  In  line  with  the  company  s intention  to ...   \n",
       "2  Services redaelli ricambi offers the ability t...   \n",
       "3  STROTHMANN not only delivers suitable mechanic...   \n",
       "4  Established\\nin 1991, tien jiang enterprise co...   \n",
       "\n",
       "                                            products  \n",
       "0  [power supply, body sub assy, medical, valve b...  \n",
       "1  [imo, advertising materials, point, imo label,...  \n",
       "2  [auto spare parts, tie rod, tie rod end, auto ...  \n",
       "3  [covers non automated, demurrage rules form, r...  \n",
       "4          [rubber, polyester, nylon, boot, support]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Welch allyn combines its practical understanding of clinical needs with its visionary spirit to develop solutions that assess, diagnose, treat, and manage a variety of illnesses and diseases.as a leading global manufacturer of medical diagnostic equipment, we offer a range of connected solutions. with nearly 2,500 employees working in 26 countries, we focus on the customer and imagine how healthcare will be delivered in the future to develop tools and future-proof technologies.our professional customers include physicians’ practices, community clinics, skilled nursing facilities, and emergency departments—places where 95% of patients first seek medical treatment.our welch allyn home division delivers solutions that make home monitoring of blood pressure readings accurate and easy. with the same attention to quality that we offer to our professional customers, we’re now bringing physician-trusted solutions to the home, helping people better manage their health.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['power supply', 'body sub assy', 'medical', 'valve body sub assy',\n",
       "       'digital blood pressure'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0]['products']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique categories 70713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22403, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "stopwords_cached = stopwords.words('english')\n",
    "\n",
    "def clean_categories(x):\n",
    "    result = []\n",
    "    for category in x:\n",
    "        tmp_ = ' '.join([word \n",
    "                         for word in tokenizer.tokenize(category.lower())\n",
    "                         if word not in stopwords_cached])\n",
    "        if tmp_ != '':\n",
    "            result.append(tmp_)\n",
    "    if len(result) == 0:\n",
    "        return np.nan\n",
    "    return np.array(result)\n",
    "\n",
    "data['products'] = data['products'].apply(clean_categories)\n",
    "data = data[data['products'].notnull()].copy().reset_index(drop=True)\n",
    "\n",
    "all_products = []\n",
    "for prod_list in data['products'].values:\n",
    "    all_products += [' '.join(tokenizer.tokenize(product))\n",
    "                     for product in prod_list.tolist()]\n",
    "    \n",
    "counter = Counter(all_products)\n",
    "print('unique categories', len(counter.most_common()))\n",
    "\n",
    "most_common = [product[0] for product in counter.most_common(100)]\n",
    "\n",
    "def filter_categories(x):\n",
    "    new_categories = np.array([product \n",
    "                               for product in x.tolist()\n",
    "                               if product in most_common])\n",
    "    if new_categories.shape[0] == 0:\n",
    "        return np.nan\n",
    "    return new_categories\n",
    "\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "stopwords_cached = stopwords.words('english')\n",
    "\n",
    "def filter_descriptions(text):\n",
    "    cleaned_text = [lemma.lemmatize(token)\n",
    "                      for token in tokenizer.tokenize(text.lower())\n",
    "                      if token not in stopwords_cached]\n",
    "    if len(cleaned_text) == 0:\n",
    "        return np.nan\n",
    "    return ' '.join(cleaned_text)\n",
    "\n",
    "data['products'] = data['products'].apply(filter_categories)\n",
    "data['description'] = data['description'].apply(filter_descriptions)\n",
    "\n",
    "data = data[(data['products'].notnull()) & (data['description'].notnull())].reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welch allyn combine practical understanding cl...</td>\n",
       "      <td>[power supply, medical]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>line company intention support international g...</td>\n",
       "      <td>[imo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>service redaelli ricambi offer ability produce...</td>\n",
       "      <td>[auto spare parts, auto spare, spare parts]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>established tien jiang enterprise co ltd one s...</td>\n",
       "      <td>[rubber, polyester]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>songwei dedicate become benchmark manufacturin...</td>\n",
       "      <td>[electric]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  welch allyn combine practical understanding cl...   \n",
       "1  line company intention support international g...   \n",
       "2  service redaelli ricambi offer ability produce...   \n",
       "3  established tien jiang enterprise co ltd one s...   \n",
       "4  songwei dedicate become benchmark manufacturin...   \n",
       "\n",
       "                                      products  \n",
       "0                      [power supply, medical]  \n",
       "1                                        [imo]  \n",
       "2  [auto spare parts, auto spare, spare parts]  \n",
       "3                          [rubber, polyester]  \n",
       "4                                   [electric]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers.fasttext import FastText\n",
    "\n",
    "model = FastText.load_fasttext_format('../models/wiki.simple.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in word vz\n",
      "error in word vz\n",
      "error in word vz\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer = RegexpTokenizer(r'[a-z]+')\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "stopwords_cached = stopwords.words('english')\n",
    "\n",
    "def get_embedding_matrix(text, first_n=50):\n",
    "#     print(text)\n",
    "    matrix = []\n",
    "    text = [token#lemma.lemmatize(token)\n",
    "            for token in tokenizer.tokenize(text)]\n",
    "    for word in text[:first_n]:\n",
    "        try:\n",
    "            word_embedding = model.wv[word]\n",
    "            matrix.append(model.wv[word])\n",
    "        except KeyError:\n",
    "            print(\"error in word\", word)\n",
    "            matrix.append(np.zeros(300))\n",
    "    \n",
    "    matrix = np.array(matrix)\n",
    "    # fill text embeddings with seq_len < first_n with zeros\n",
    "#     print(matrix.shape[0])\n",
    "    if matrix.shape[0] < first_n:\n",
    "        matrix = np.vstack((matrix, np.zeros((first_n - matrix.shape[0], 300))))\n",
    "    return matrix\n",
    "\n",
    "embeddings = []\n",
    "for text in data['description'].values:\n",
    "    embeddings.append(get_embedding_matrix(text))\n",
    "#     print(embeddings[-1].shape)\n",
    "    \n",
    "# onehot_y_df = pandas.get_dummies(data['severity'])\n",
    "\n",
    "train_data = np.array(embeddings)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([item.tolist() for item in data['products'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aluminium', 'aluminum', 'assembly', 'auto parts', 'auto spare',\n",
       "       'auto spare parts', 'automobile', 'automotive parts',\n",
       "       'automotive spare', 'automotive spare parts', 'ball', 'battery',\n",
       "       'book', 'bottle', 'brake', 'cable', 'cap', 'car', 'ceramic',\n",
       "       'ceramic tile', 'chair', 'component', 'components', 'compressor',\n",
       "       'computer', 'copper', 'cotton', 'cover', 'cylinder', 'display',\n",
       "       'electric', 'electrical', 'engine', 'fabric', 'film', 'filter',\n",
       "       'furniture', 'gear', 'glass', 'granite', 'hand', 'hardware',\n",
       "       'hose', 'housing', 'hydraulic', 'imo', 'industrial', 'injection',\n",
       "       'leather', 'led', 'light', 'lighting', 'machinery',\n",
       "       'machinery parts', 'medical', 'mold', 'motor', 'motor vehicle',\n",
       "       'nut', 'oil', 'pipe', 'plastic parts', 'plate', 'polyester',\n",
       "       'powder', 'power', 'power supply', 'pump', 'pvc', 'ring', 'rubber',\n",
       "       'screw', 'seat', 'sheet', 'slab', 'software', 'solid wood',\n",
       "       'spare part', 'spare parts', 'stackable', 'stainless',\n",
       "       'stainless steel', 'steel', 'system', 'table', 'tile', 'tire',\n",
       "       'tool', 'tools', 'toys', 'truck', 'tube', 'valve', 'vehicle',\n",
       "       'water', 'wheel', 'wine', 'wire', 'wooden furniture', 'x'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Clusterize for example \"aluminium\" and \"aluminum\";  'spare part' and 'spare parts' and others into one category.\n",
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class DataBatcher():\n",
    "    def __init__(self, _X, _y, _batch_size=30):\n",
    "        self._X = _X\n",
    "        self._y = _y\n",
    "        self._batch_size = _batch_size\n",
    "        self._resplit = True\n",
    "        self._num_examples = self._y.shape[0]\n",
    "    \n",
    "    def next_batch(self):\n",
    "        if self._resplit:\n",
    "#             print('splitting')\n",
    "            perm0 = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm0)\n",
    "            self._batches_indexes = np.array_split(perm0, math.ceil(perm0.shape[0] / self._batch_size))\n",
    "#             print(self._batches_indexes)\n",
    "            self._batch_counter = -1\n",
    "            self._resplit = False\n",
    "\n",
    "        self._batch_counter += 1\n",
    "        if self._batches_indexes[self._batch_counter].shape[0] < self._batch_size:\n",
    "#             print('hstacking')\n",
    "            self._resplit = True\n",
    "            ind = self._batch_counter\n",
    "#             self._batch_counter = -1\n",
    "            missing_num = self._batch_size - self._batches_indexes[ind].shape[0]\n",
    "            return self._X[np.hstack((self._batches_indexes[ind], self._batches_indexes[0][:missing_num]))],\\\n",
    "                   self._y[np.hstack((self._batches_indexes[ind], self._batches_indexes[0][:missing_num]))]\n",
    "        \n",
    "        return self._X[self._batches_indexes[self._batch_counter]], self._y[self._batches_indexes[self._batch_counter]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss= 0.4753, Train Acc= 0.224, Test Acc = 0.204\n",
      "Step 1000, Loss= 0.0625, Train Acc= 0.062, Test Acc = 0.045\n",
      "Step 2000, Loss= 0.0615, Train Acc= 0.062, Test Acc = 0.056\n",
      "Step 3000, Loss= 0.0519, Train Acc= 0.122, Test Acc = 0.085\n",
      "Step 4000, Loss= 0.0480, Train Acc= 0.229, Test Acc = 0.102\n",
      "Step 5000, Loss= 0.0490, Train Acc= 0.175, Test Acc = 0.105\n",
      "Step 6000, Loss= 0.0270, Train Acc= 0.537, Test Acc = 0.126\n",
      "Step 7000, Loss= 0.0209, Train Acc= 0.617, Test Acc = 0.122\n",
      "Step 8000, Loss= 0.0168, Train Acc= 0.623, Test Acc = 0.127\n",
      "Step 9000, Loss= 0.0160, Train Acc= 0.754, Test Acc = 0.125\n",
      "Step 10000, Loss= 0.0122, Train Acc= 0.792, Test Acc = 0.124\n",
      "Step 11000, Loss= 0.0089, Train Acc= 0.850, Test Acc = 0.126\n",
      "Step 12000, Loss= 0.0088, Train Acc= 0.858, Test Acc = 0.125\n",
      "Step 13000, Loss= 0.0064, Train Acc= 0.915, Test Acc = 0.131\n",
      "Step 14000, Loss= 0.0048, Train Acc= 0.906, Test Acc = 0.135\n",
      "Step 15000, Loss= 0.0053, Train Acc= 0.884, Test Acc = 0.135\n",
      "Step 16000, Loss= 0.0029, Train Acc= 0.919, Test Acc = 0.129\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Training Parameters\n",
    "    learning_rate = 0.001\n",
    "    training_steps = 220000\n",
    "    batch_size = 80\n",
    "    display_step = 1000\n",
    "    checkpoint_step = 10000\n",
    "    \n",
    "    # Network Parameters\n",
    "    num_input = 300 \n",
    "    timesteps = 50 \n",
    "    num_hidden = 450 \n",
    "    num_classes = 100 \n",
    "    num_layers = 3\n",
    "#     input_keep_prob = 0.65\n",
    "#     output_keep_prob = 0.65\n",
    "    cell = tf.contrib.rnn.LSTMCell\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([2 * num_hidden, num_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "    }\n",
    "\n",
    "    def RNN(x, weights, biases):\n",
    "\n",
    "        # Prepare data shape to match `rnn` function requirements\n",
    "        # Current data input shape: (batch_size, timesteps, n_input)\n",
    "        # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "        # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "        x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "        # Cerate forward and backward cells\n",
    "        cells_fw = [cell(num_hidden, activation=tf.tanh) for _ in range(num_layers)]\n",
    "        cells_bw = [cell(num_hidden, activation=tf.tanh) for _ in range(num_layers)]\n",
    "        \n",
    "        keep_probs = [0.75, 0.65, 0.65]\n",
    "        # Add dropout\n",
    "        cells_fw = [tf.contrib.rnn.DropoutWrapper(cell, \n",
    "                                                  input_keep_prob=keep_prob_\n",
    "                                                 ) for cell, keep_prob_ in zip(cells_fw, keep_probs)]\n",
    "        cells_bw = [tf.contrib.rnn.DropoutWrapper(cell, \n",
    "                                                  input_keep_prob=keep_prob_\n",
    "                                                 ) for cell, keep_prob_ in zip(cells_bw, keep_probs)]\n",
    "        \n",
    "        outputs, _, _ = tf.contrib.rnn.stack_bidirectional_rnn(\n",
    "            cells_fw=cells_fw,\n",
    "            cells_bw=cells_bw,\n",
    "            inputs=x,\n",
    "            dtype=tf.float32)\n",
    "        \n",
    "        return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "    logits = RNN(X, weights, biases)\n",
    "    prediction = tf.nn.sigmoid(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=logits, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "def evaluate_multilabel(y_pred, y_true):\n",
    "    acc = []\n",
    "    for y_pred_tmp, y_true_tmp in zip(y_pred, y_true):\n",
    "        real_ = np.nonzero(y_true_tmp)[0].tolist()\n",
    "        pred_ = np.nonzero(y_pred_tmp)[0].tolist()\n",
    "        if len(real_) == 0:\n",
    "            #means 0 right answers\n",
    "            acc.append(0.0)\n",
    "            continue\n",
    "        acc.append(len(set(real_).intersection(set(pred_))) / len(real_))\n",
    "    return(np.array(acc).mean())\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4)\n",
    "sess = tf.InteractiveSession(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "# with tf.Session(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "batcher = DataBatcher(X_train, y_train, _batch_size=batch_size)\n",
    "\n",
    "try:\n",
    "for step in range(1, training_steps+1):\n",
    "    batch_x, batch_y = batcher.next_batch()\n",
    "    # Reshape data to get 28 seq of 28 elements\n",
    "#         batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "    # Run optimization op (backprop)\n",
    "    sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        # Calculate batch loss and accuracy\n",
    "        loss, pred = sess.run([loss_op, tf.round(prediction)], feed_dict={X: batch_x, Y: batch_y})\n",
    "        \n",
    "        test_out = sess.run(tf.round(prediction), feed_dict={X: X_test})\n",
    "        print(\"Step \" + str(step) + \", Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss) + \", Train Acc= \" + \\\n",
    "              \"{:.3f}\".format(evaluate_multilabel(y_pred=pred, y_true=batch_y)) +  \", Test Acc = \" + \\\n",
    "              \"{:.3f}\".format(evaluate_multilabel(y_pred=test_out, y_true=y_test)))\n",
    "        \n",
    "    if step % checkpoint_step == 0:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, '../models/lstm_big_dropout/lstm_100_classes', global_step=step)\n",
    "except \n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, '../models/lstm_big_dropout/lstm_100_classes', global_step=step)\n",
    "        \n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "#     # Calculate accuracy for 128 mnist test images\n",
    "#     print(\"Testing Accuracy:\", \\\n",
    "#         sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 1\n",
    "pred = tf.round(tf.nn.sigmoid(logits))\n",
    "\n",
    "def evaluate_multilabel(y_pred, y_true):\n",
    "    acc = []\n",
    "    for y_pred_tmp, y_true_tmp in zip(y_pred, y_true):\n",
    "#         print('y_true_tmp', y_true_tmp, 'y_pred_tmp', y_pred_tmp)\n",
    "        real_ = np.nonzero(y_true_tmp)[0].tolist()\n",
    "#         right_num = len(real)\n",
    "        pred_ = np.nonzero(y_pred_tmp)[0].tolist()\n",
    "#         print('real', real, 'pred', pred)\n",
    "        if len(real) == 0:\n",
    "            #means 0 right answers\n",
    "            acc.append(0.0)\n",
    "            continue\n",
    "        acc.append(len(set(real_).intersection(set(pred_))) / len(real_))\n",
    "#     print(acc)\n",
    "    return(np.array(acc).mean())\n",
    "\n",
    "# correct_predictions = tf.equal(tf.round(tf.nn.sigmoid(logits)), Y)\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "# pred = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "pred = sess.run(pred, feed_dict={X: X_test, Y: y_test})\n",
    "evaluate_multilabel(pred, y_test)\n",
    "# real = y_test[num]\n",
    "# tf.equal(tf.round(tf.nn.sigmoid(pred)), tf.round(y_))\n",
    "# pred > 0.5\n",
    "\n",
    "# real = np.nonzero(real)[0].tolist()\n",
    "# right_num = len(real)\n",
    "# pred = np.argpartition(pred, -right_num)[-right_num:]\n",
    "# print(real, pred)\n",
    "# print('real_classes', mlb.classes_[np.array(real)])\n",
    "# print('predicted classes', mlb.classes_[np.array(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    save = tf.train.Saver()\n",
    "    save.restore(session, '../models/lstm_relu/lstm_relu-150000')\n",
    "    pred = tf.round(tf.nn.sigmoid(logits))\n",
    "    print(session.run(pred, feed_dict={X: X_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['dthc']\n",
    "def filter_categories(cat_list):\n",
    "    return set(cat_list).intersection(query) == set(query)\n",
    "data[data['products'].apply(filter_categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(['qwe', 'asd']).intersection(['qwe', 'asd']) == set(['asd', 'qwe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batcher = DataBatcher(X_train, y_train, batch_size_=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.train.images.shape)\n",
    "print(mnist.train.labels.shape)\n",
    "print(mnist.test.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ = sess.run(prediction, feed_dict={X: [batch_x[3]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(predict_)\n",
    "# pred\n",
    "# batch_y[3]\n",
    "predict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = 5\n",
    "predict_ = sess.run(prediction, feed_dict={X: [batch_x[obj]]})\n",
    "real = np.nonzero(batch_y[obj])[0].tolist()\n",
    "# batch_y[1]\n",
    "pred = np.argpartition(predict_[0], -len(real))[-len(real):]\n",
    "print('real', real)\n",
    "print('pred', pred)\n",
    "print('classes real', mlb.classes_[real])\n",
    "print('classes predicted', mlb.classes_[pred])\n",
    "\n",
    "len(set(real).intersection(set(pred))) / len(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_log = None\n",
    "\n",
    "with open('../perf.log', 'r') as f:\n",
    "    performance_log = f.readlines()\n",
    "    \n",
    "ram = []\n",
    "cpu = []\n",
    "for i in range(7, len(performance_log), 9):\n",
    "#     print(performance_log[i][:-1].split(' '))\n",
    "    line = [i for i in performance_log[i][:-1].split(' ') if i != '' ]\n",
    "    if line == list():\n",
    "        continue\n",
    "#     print(line)\n",
    "    ram.append(line[5])\n",
    "    cpu.append(float(line[8].replace(',', '.')))\n",
    "\n",
    "ram = [float(i[:-1].replace(',', '.')) if 'g' in i else float(i) / (10**6) for i in ram]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(1, figsize=(15,6))\n",
    "plt.subplot(211)\n",
    "plt.plot(range(len(ram)),ram)\n",
    "plt.title(\"RAM\")\n",
    "plt.subplot(212)\n",
    "plt.plot(range(len(cpu)), cpu)\n",
    "plt.title(\"CPU\")\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "# performance_log[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([1,2,3,4,5,6,7,8,9,10])\n",
    "list(set([423,123,54,2341,3125,62,322]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
