{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.read_parquet('../data/to_send.pq')\n",
    "print(data.shape)\n",
    "data = data[['description', 'products']][pandas.notnull(data['products'])].copy().reset_index(drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "stopwords_cached = stopwords.words('english')\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "st = LancasterStemmer()\n",
    "\n",
    "# lemmatize categories to make 'detail' and 'details' as one category\n",
    "def clean_categories(x):\n",
    "    result = []\n",
    "    for category in x:\n",
    "        tmp_ = ' '.join([lemma.lemmatize(word)\n",
    "                         for word in tokenizer.tokenize(category.lower())\n",
    "                         if word not in stopwords_cached])\n",
    "        if tmp_ != '':\n",
    "            result.append(tmp_)\n",
    "    if len(result) == 0:\n",
    "        return np.nan\n",
    "    return np.array(result)\n",
    "\n",
    "data['products'] = data['products'].apply(clean_categories)\n",
    "data = data[data['products'].notnull()].copy().reset_index(drop=True)\n",
    "\n",
    "all_products = []\n",
    "for prod_list in data['products'].values:\n",
    "    all_products += [' '.join(tokenizer.tokenize(product))\n",
    "                     for product in prod_list.tolist()]\n",
    "    \n",
    "counter = Counter(all_products)\n",
    "print('unique categories', len(counter.most_common()))\n",
    "\n",
    "most_common = [product[0] for product in counter.most_common(100)]\n",
    "\n",
    "# remove all categories which is not in top 100\n",
    "def filter_categories(x):\n",
    "    new_categories = np.array([product \n",
    "                               for product in x.tolist()\n",
    "                               if product in most_common])\n",
    "    if new_categories.shape[0] == 0:\n",
    "        return np.nan\n",
    "    return new_categories\n",
    "\n",
    "stopwords_cached = stopwords.words('english')\n",
    "\n",
    "# basic text processing on descriptions\n",
    "def filter_descriptions(text):\n",
    "    cleaned_text = [lemma.lemmatize(token)\n",
    "                      for token in tokenizer.tokenize(text.lower())\n",
    "                      if token not in stopwords_cached]\n",
    "    if len(cleaned_text) == 0:\n",
    "        return np.nan\n",
    "    return ' '.join(cleaned_text)\n",
    "\n",
    "data['products'] = data['products'].apply(filter_categories)\n",
    "data['description'] = data['description'].apply(filter_descriptions)\n",
    "\n",
    "data = data[(data['products'].notnull()) & (data['description'].notnull())].reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers.fasttext import FastText\n",
    "\n",
    "model = FastText.load_fasttext_format('../models/wiki.simple.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['description'])\n",
    "sequences = tokenizer.texts_to_sequences(data['description'])\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform([item.tolist() for item in data['products'].values])\n",
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape, len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index), 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= len(tokenizer.word_index):\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = model.wv[word]\n",
    "    except KeyError:\n",
    "        print(\"error in word \" + word)\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "def evaluate_multilabel(y_pred, y_true):\n",
    "    y_pred, y_true = y_true, y_pred\n",
    "    acc = []\n",
    "    for y_pred_tmp, y_true_tmp in zip(y_pred, y_true):\n",
    "        real_ = np.nonzero(y_true_tmp)[0].tolist()\n",
    "        pred_ = np.nonzero(y_pred_tmp)[0].tolist()\n",
    "        if len(real_) == 0:\n",
    "            #means 0 right answers\n",
    "            acc.append(0.0)\n",
    "            continue\n",
    "        acc.append(len(set(real_).intersection(set(pred_))) / len(real_))\n",
    "    return(np.array(acc).mean())\n",
    "\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=50,\n",
    "                            trainable=False)\n",
    "sequence_input = Input(shape=(50,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Bidirectional(LSTM(64))(embedded_sequences)\n",
    "x = Dropout(0.5)(x)\n",
    "# x = Bidirectional(LSTM(64))(x)\n",
    "# x = Dense(1, activation='tanh')(x)\n",
    "preds = Dense(len(y[0]), activation='sigmoid')(x)\n",
    "\n",
    "nn = Model(sequence_input, preds)\n",
    "nn.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "nn.fit(X_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=100,\n",
    "validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.fit(X_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=100,\n",
    "validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(evaluate_multilabel(preds, y_test))\n",
    "for num in range(0, preds.shape[0]):\n",
    "    real = y_test[num]\n",
    "\n",
    "    real = np.nonzero(real)[0].tolist()\n",
    "    right_num = len(real)\n",
    "    pred = np.round(preds[num])\n",
    "    pred = np.nonzero(pred)[0].tolist()\n",
    "    print(real, pred)\n",
    "    print('real_classes', mlb.classes_[np.array(real)])\n",
    "    if len(pred) > 0:\n",
    "        print('predicted classes', mlb.classes_[np.array(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nn.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(evaluate_multilabel(preds, y_train))\n",
    "for num in range(0, preds.shape[0]):\n",
    "    real = y_train[num]\n",
    "\n",
    "    real = np.nonzero(real)[0].tolist()\n",
    "    right_num = len(real)\n",
    "    pred = np.round(preds[num])\n",
    "    pred = np.nonzero(pred)[0].tolist()\n",
    "    print(real, pred)\n",
    "    print('real_classes', mlb.classes_[np.array(real)])\n",
    "    if len(pred) > 0:\n",
    "        print('predicted classes', mlb.classes_[np.array(pred)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
