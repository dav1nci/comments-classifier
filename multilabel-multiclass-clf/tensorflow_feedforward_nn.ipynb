{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83897, 18)\n",
      "(47516, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pandas.read_parquet('../data/to_send.pq')\n",
    "print(data.shape)\n",
    "data = data[['description', 'products']][pandas.notnull(data['products'])].copy().reset_index(drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Welch allyn combines its practical understandi...</td>\n",
       "      <td>[power supply, body sub assy, medical, valve b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In  line  with  the  company  s intention  to ...</td>\n",
       "      <td>[imo, advertising materials, point, imo label,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Services redaelli ricambi offers the ability t...</td>\n",
       "      <td>[auto spare parts, tie rod, tie rod end, auto ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STROTHMANN not only delivers suitable mechanic...</td>\n",
       "      <td>[covers non automated, demurrage rules form, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Established\\nin 1991, tien jiang enterprise co...</td>\n",
       "      <td>[rubber, polyester, nylon, boot, support]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  Welch allyn combines its practical understandi...   \n",
       "1  In  line  with  the  company  s intention  to ...   \n",
       "2  Services redaelli ricambi offers the ability t...   \n",
       "3  STROTHMANN not only delivers suitable mechanic...   \n",
       "4  Established\\nin 1991, tien jiang enterprise co...   \n",
       "\n",
       "                                            products  \n",
       "0  [power supply, body sub assy, medical, valve b...  \n",
       "1  [imo, advertising materials, point, imo label,...  \n",
       "2  [auto spare parts, tie rod, tie rod end, auto ...  \n",
       "3  [covers non automated, demurrage rules form, r...  \n",
       "4          [rubber, polyester, nylon, boot, support]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "stopwords_cached = stopwords.words('english')\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "st = LancasterStemmer()\n",
    "\n",
    "# lemmatize categories to make 'detail' and 'details' as one category\n",
    "def clean_categories(x):\n",
    "    result = []\n",
    "    for category in x:\n",
    "        tmp_ = ' '.join([lemma.lemmatize(word)\n",
    "                         for word in tokenizer.tokenize(category.lower())\n",
    "                         if word not in stopwords_cached])\n",
    "        if tmp_ != '':\n",
    "            result.append(tmp_)\n",
    "    if len(result) == 0:\n",
    "        return np.nan\n",
    "    return np.array(result)\n",
    "\n",
    "data['products'] = data['products'].apply(clean_categories)\n",
    "data = data[data['products'].notnull()].copy().reset_index(drop=True)\n",
    "\n",
    "all_products = []\n",
    "for prod_list in data['products'].values:\n",
    "    all_products += [' '.join(tokenizer.tokenize(product))\n",
    "                     for product in prod_list.tolist()]\n",
    "    \n",
    "counter = Counter(all_products)\n",
    "print('unique categories', len(counter.most_common()))\n",
    "\n",
    "most_common = [product[0] for product in counter.most_common(100)]\n",
    "\n",
    "# remove all categories which is not in top 100\n",
    "def filter_categories(x):\n",
    "    new_categories = np.array([product \n",
    "                               for product in x.tolist()\n",
    "                               if product in most_common])\n",
    "    if new_categories.shape[0] == 0:\n",
    "        return np.nan\n",
    "    return new_categories\n",
    "\n",
    "stopwords_cached = stopwords.words('english')\n",
    "\n",
    "# basic text processing on descriptions\n",
    "def filter_descriptions(text):\n",
    "    cleaned_text = [lemma.lemmatize(token)\n",
    "                      for token in tokenizer.tokenize(text.lower())\n",
    "                      if token not in stopwords_cached]\n",
    "    if len(cleaned_text) == 0:\n",
    "        return np.nan\n",
    "    return ' '.join(cleaned_text)\n",
    "\n",
    "data['products'] = data['products'].apply(filter_categories)\n",
    "data['description'] = data['description'].apply(filter_descriptions)\n",
    "\n",
    "data = data[(data['products'].notnull()) & (data['description'].notnull())].reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim\n",
    "\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "stopwords_cached = stopwords.words('english')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def read_corpus(tokens_only=False):\n",
    "#     with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "    for line, id_ in zip(data['description'].values, \n",
    "                         data.index.values):\n",
    "        \n",
    "        # For training data, add tags\n",
    "        yield gensim.models.doc2vec.TaggedDocument([token\n",
    "                                                    for token in tokenizer.tokenize(line)], \n",
    "                                                   ['_*' + str(id_)])\n",
    "        \n",
    "train_corpus = list(read_corpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PV-DBOW \n",
    "\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(dm=0, dbow_words=1, size=200, window=8, min_count=19, iter=10, workers=7)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "model.save('../models/company_descriptions.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.doc2vec.Doc2Vec.load('../models/company_descriptions.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.docvecs['_*1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = []\n",
    "y_ = []\n",
    "\n",
    "for i in data.index.values:\n",
    "    X_.append(model.docvecs['_*' + str(i)])\n",
    "    y_.append(data.loc[i]['products'])\n",
    "X_ = np.array(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_ = mlb.fit_transform(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.docvecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class DataBatcher():\n",
    "    def __init__(self, _X, _y, _batch_size=30):\n",
    "        self._X = _X\n",
    "        self._y = _y\n",
    "        self._batch_size = _batch_size\n",
    "        self._resplit = True\n",
    "        self._num_examples = self._y.shape[0]\n",
    "    \n",
    "    def next_batch(self):\n",
    "        if self._resplit:\n",
    "            perm0 = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm0)\n",
    "            self._batches_indexes = np.array_split(perm0, math.ceil(perm0.shape[0] / self._batch_size))\n",
    "            self._batch_counter = -1\n",
    "            self._resplit = False\n",
    "\n",
    "        self._batch_counter += 1\n",
    "        if self._batches_indexes[self._batch_counter].shape[0] < self._batch_size:\n",
    "            self._resplit = True\n",
    "            ind = self._batch_counter\n",
    "            missing_num = self._batch_size - self._batches_indexes[ind].shape[0]\n",
    "            return self._X[np.hstack((self._batches_indexes[ind], self._batches_indexes[0][:missing_num]))],\\\n",
    "                   self._y[np.hstack((self._batches_indexes[ind], self._batches_indexes[0][:missing_num]))]\n",
    "        \n",
    "        return self._X[self._batches_indexes[self._batch_counter]], self._y[self._batches_indexes[self._batch_counter]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Parameters\n",
    "    learn_rate = 0.001\n",
    "    num_steps = 200000\n",
    "    batch_size = 128\n",
    "    display_step = 1000\n",
    "    checkpoint_step = 20000\n",
    "\n",
    "    # Network Parameters\n",
    "    n_hidden = 70 \n",
    "    num_input = 200 \n",
    "    num_classes = 100 \n",
    "\n",
    "    # Graph input\n",
    "    X = tf.placeholder(\"float\", [None, num_input])\n",
    "    y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    lr = tf.placeholder(\"float\")\n",
    "\n",
    "    # Define the neural network\n",
    "    def simple_nn(x):\n",
    "        initializer = tf.truncated_normal_initializer(stddev=0.18632568, dtype=tf.float32)\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=0.01)\n",
    "        layer_1 = tf.layers.dense(inputs=x, units=n_hidden, activation=tf.tanh, \n",
    "                                  kernel_initializer=initializer, kernel_regularizer=regularizer)\n",
    "        layer_2 = tf.layers.dense(inputs=layer_1, units=n_hidden, activation=tf.tanh, \n",
    "                                  kernel_initializer=initializer, kernel_regularizer=regularizer)\n",
    "        layer_3 = tf.layers.dense(inputs=layer_2, units=n_hidden, activation=tf.tanh, \n",
    "                                  kernel_initializer=initializer, kernel_regularizer=regularizer)\n",
    "        layer_4 = tf.layers.dense(inputs=layer_3, units=n_hidden, activation=tf.tanh, \n",
    "                                  kernel_initializer=initializer, kernel_regularizer=regularizer)\n",
    "        # Output fully connected layer with a neuron for each class\n",
    "        out_layer = tf.layers.dense(layer_4, num_classes, activation=None, \n",
    "                                  kernel_initializer=initializer, kernel_regularizer=regularizer)\n",
    "        return out_layer\n",
    "\n",
    "    logits = simple_nn(X)\n",
    "    prediction = tf.nn.sigmoid(logits)\n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "def evaluate_multilabel(y_pred, y_true):\n",
    "    acc = []\n",
    "    for y_pred_tmp, y_true_tmp in zip(y_pred, y_true):\n",
    "        real_ = np.nonzero(y_true_tmp)[0].tolist()\n",
    "        pred_ = np.nonzero(y_pred_tmp)[0].tolist()\n",
    "        if len(real_) == 0:\n",
    "            acc.append(0.0)\n",
    "            continue\n",
    "        acc.append(len(set(real_).intersection(set(pred_))) / len(real_))\n",
    "    return(np.array(acc).mean())\n",
    "\n",
    "# Start training\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4)\n",
    "sess = tf.InteractiveSession(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.2, random_state=42)\n",
    "batcher = DataBatcher(X_train, y_train, _batch_size=batch_size)\n",
    "\n",
    "\n",
    "for step in range(1, num_steps + 1):\n",
    "    batch_x, batch_y = batcher.next_batch()\n",
    "    if step == int(num_steps * 0.75):\n",
    "        learn_rate /= 2\n",
    "        print(\"lr now \" + str(learn_rate))\n",
    "    if step == int(num_steps * 0.90):\n",
    "        learn_rate /= 2\n",
    "        print(\"lr now \" + str(learn_rate))\n",
    "    sess.run(train_op, feed_dict={X: batch_x, y: batch_y, lr: learn_rate})\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        # Calculate batch loss and accuracy\n",
    "        loss, pred = sess.run([loss_op, tf.round(prediction)], feed_dict={X: batch_x, y: batch_y})\n",
    "        \n",
    "        test_out = sess.run(tf.round(prediction), feed_dict={X: X_test})\n",
    "        print(\"Step \" + str(step) + \", Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss) + \", Train Acc= \" + \\\n",
    "              \"{:.3f}\".format(evaluate_multilabel(y_pred=pred, y_true=batch_y)) +  \", Test Acc = \" + \\\n",
    "              \"{:.3f}\".format(evaluate_multilabel(y_pred=test_out, y_true=y_test)))\n",
    "\n",
    "    if step % checkpoint_step == 0:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, '../models/simple_nn/mlp', global_step=step)\n",
    "        \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    save = tf.train.Saver()\n",
    "    save.restore(session, '../models/lstm_big_dropout/lstm_100_classes-40000')\n",
    "    pred = tf.round(tf.nn.sigmoid(logits))\n",
    "    test_preds = session.run(pred, feed_dict={X: X_test})\n",
    "    for num in range(0, test_preds.shape[0]):\n",
    "        real = y_test[num]\n",
    "\n",
    "        real = np.nonzero(real)[0].tolist()\n",
    "        right_num = len(real)\n",
    "        pred = test_preds[num]\n",
    "        pred = np.nonzero(pred)[0].tolist()\n",
    "        print('=================================\\nreal_classes', mlb.classes_[np.array(real)])\n",
    "        if len(pred) > 0:\n",
    "            print('predicted classes', mlb.classes_[np.array(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['seat']\n",
    "def filter_categories(cat_list):\n",
    "    return set(cat_list).intersection(query) == set(query)\n",
    "data[data['products'].apply(filter_categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/categ_preprocessed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
