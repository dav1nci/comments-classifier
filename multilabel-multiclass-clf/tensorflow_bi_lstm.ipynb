{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83897, 18)\n",
      "(47516, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pandas.read_parquet('../data/to_send.pq')\n",
    "print(data.shape)\n",
    "data = data[['description', 'products']][pandas.notnull(data['products'])].copy().reset_index(drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique categories 66682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(23413, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "stopwords_cached = stopwords.words('english')\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "st = LancasterStemmer()\n",
    "\n",
    "# lemmatize categories to make 'detail' and 'details' as one category\n",
    "def clean_categories(x):\n",
    "    result = []\n",
    "    for category in x:\n",
    "        tmp_ = ' '.join([lemma.lemmatize(word)\n",
    "                         for word in tokenizer.tokenize(category.lower())\n",
    "                         if word not in stopwords_cached])\n",
    "        if tmp_ != '':\n",
    "            result.append(tmp_)\n",
    "    if len(result) == 0:\n",
    "        return np.nan\n",
    "    return np.array(result)\n",
    "\n",
    "data['products'] = data['products'].apply(clean_categories)\n",
    "data = data[data['products'].notnull()].copy().reset_index(drop=True)\n",
    "\n",
    "all_products = []\n",
    "for prod_list in data['products'].values:\n",
    "    all_products += [' '.join(tokenizer.tokenize(product))\n",
    "                     for product in prod_list.tolist()]\n",
    "    \n",
    "counter = Counter(all_products)\n",
    "print('unique categories', len(counter.most_common()))\n",
    "\n",
    "most_common = [product[0] for product in counter.most_common(100)]\n",
    "\n",
    "# remove all categories which is not in top 100\n",
    "def filter_categories(x):\n",
    "    new_categories = np.array([product \n",
    "                               for product in x.tolist()\n",
    "                               if product in most_common])\n",
    "    if new_categories.shape[0] == 0:\n",
    "        return np.nan\n",
    "    return new_categories\n",
    "\n",
    "stopwords_cached = stopwords.words('english')\n",
    "\n",
    "# basic text processing on descriptions\n",
    "def filter_descriptions(text):\n",
    "    cleaned_text = [lemma.lemmatize(token)\n",
    "                      for token in tokenizer.tokenize(text.lower())\n",
    "                      if token not in stopwords_cached]\n",
    "    if len(cleaned_text) == 0:\n",
    "        return np.nan\n",
    "    return ' '.join(cleaned_text)\n",
    "\n",
    "data['products'] = data['products'].apply(filter_categories)\n",
    "data['description'] = data['description'].apply(filter_descriptions)\n",
    "\n",
    "data = data[(data['products'].notnull()) & (data['description'].notnull())].reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welch allyn combine practical understanding cl...</td>\n",
       "      <td>[power supply, medical]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>line company intention support international g...</td>\n",
       "      <td>[imo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>service redaelli ricambi offer ability produce...</td>\n",
       "      <td>[auto spare part, auto spare, spare part]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>established tien jiang enterprise co ltd one s...</td>\n",
       "      <td>[rubber, polyester]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>songwei dedicate become benchmark manufacturin...</td>\n",
       "      <td>[electric]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  welch allyn combine practical understanding cl...   \n",
       "1  line company intention support international g...   \n",
       "2  service redaelli ricambi offer ability produce...   \n",
       "3  established tien jiang enterprise co ltd one s...   \n",
       "4  songwei dedicate become benchmark manufacturin...   \n",
       "\n",
       "                                    products  \n",
       "0                    [power supply, medical]  \n",
       "1                                      [imo]  \n",
       "2  [auto spare part, auto spare, spare part]  \n",
       "3                        [rubber, polyester]  \n",
       "4                                 [electric]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers.fasttext import FastText\n",
    "\n",
    "model = FastText.load_fasttext_format('../models/wiki.simple.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in word zc\n",
      "error in word zc\n",
      "error in word vz\n",
      "error in word vz\n",
      "error in word vz\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer = RegexpTokenizer(r'[a-z]+')\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "stopwords_cached = stopwords.words('english')\n",
    "\n",
    "def get_embedding_matrix(text, first_n=50):\n",
    "#     print(text)\n",
    "    matrix = []\n",
    "    text = [token#lemma.lemmatize(token)\n",
    "            for token in tokenizer.tokenize(text)]\n",
    "    for word in text[:first_n]:\n",
    "        try:\n",
    "            word_embedding = model.wv[word]\n",
    "            matrix.append(model.wv[word])\n",
    "        except KeyError:\n",
    "            print(\"error in word\", word)\n",
    "            matrix.append(np.zeros(300))\n",
    "    \n",
    "    matrix = np.array(matrix)\n",
    "    # fill text embeddings with seq_len < first_n with zeros\n",
    "#     print(matrix.shape[0])\n",
    "    if matrix.shape[0] < first_n:\n",
    "        matrix = np.vstack((matrix, np.zeros((first_n - matrix.shape[0], 300))))\n",
    "    return matrix\n",
    "\n",
    "embeddings = []\n",
    "for text in data['description'].values:\n",
    "    embeddings.append(get_embedding_matrix(text))\n",
    "\n",
    "train_data = np.array(embeddings)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([item.tolist() for item in data['products'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aluminium', 'aluminum', 'assembly', 'auto part', 'auto spare',\n",
       "       'auto spare part', 'automotive part', 'automotive spare',\n",
       "       'automotive spare part', 'ball', 'battery', 'bearing', 'book',\n",
       "       'bottle', 'brake', 'cable', 'cap', 'car', 'car part', 'ceramic',\n",
       "       'ceramic tile', 'chair', 'coil', 'component', 'compressor',\n",
       "       'computer', 'computer part', 'connector', 'cotton', 'cover',\n",
       "       'cylinder', 'display', 'electric', 'electrical', 'engine',\n",
       "       'fabric', 'film', 'filter', 'fitting', 'frame', 'furniture',\n",
       "       'gear', 'glass', 'granite', 'hand', 'hand tool', 'hose',\n",
       "       'hydraulic', 'imo', 'industrial', 'lamp', 'leather', 'led',\n",
       "       'light', 'lighting', 'machinery', 'machinery part', 'medical',\n",
       "       'motor', 'motor vehicle', 'nut', 'oil', 'panel', 'pipe',\n",
       "       'plastic part', 'plate', 'polyester', 'powder', 'power',\n",
       "       'power supply', 'pump', 'pvc', 'ring', 'rubber', 'screw', 'sheet',\n",
       "       'shoe', 'slab', 'software', 'solid wood', 'spare part',\n",
       "       'stainless', 'stainless steel', 'steel', 'system', 'table', 'tile',\n",
       "       'tire', 'tool', 'toy', 'truck', 'tube', 'tyre', 'valve', 'vehicle',\n",
       "       'water', 'wheel', 'wine', 'wire', 'x'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Clusterize for example \"aluminium\" and \"aluminum\";  'spare part' and 'spare parts' and others into one category.\n",
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class DataBatcher():\n",
    "    def __init__(self, _X, _y, _batch_size=30):\n",
    "        self._X = _X\n",
    "        self._y = _y\n",
    "        self._batch_size = _batch_size\n",
    "        self._resplit = True\n",
    "        self._num_examples = self._y.shape[0]\n",
    "    \n",
    "    def next_batch(self):\n",
    "        if self._resplit:\n",
    "            perm0 = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm0)\n",
    "            self._batches_indexes = np.array_split(perm0, math.ceil(perm0.shape[0] / self._batch_size))\n",
    "            self._batch_counter = -1\n",
    "            self._resplit = False\n",
    "\n",
    "        self._batch_counter += 1\n",
    "        if self._batches_indexes[self._batch_counter].shape[0] < self._batch_size:\n",
    "            self._resplit = True\n",
    "            ind = self._batch_counter\n",
    "            missing_num = self._batch_size - self._batches_indexes[ind].shape[0]\n",
    "            return self._X[np.hstack((self._batches_indexes[ind], self._batches_indexes[0][:missing_num]))],\\\n",
    "                   self._y[np.hstack((self._batches_indexes[ind], self._batches_indexes[0][:missing_num]))]\n",
    "        \n",
    "        return self._X[self._batches_indexes[self._batch_counter]], self._y[self._batches_indexes[self._batch_counter]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss= 0.9236, Train Acc= 0.452, Test Acc = 0.482\n",
      "Step 1000, Loss= 0.0747, Train Acc= 0.000, Test Acc = 0.000\n",
      "Step 2000, Loss= 0.0694, Train Acc= 0.000, Test Acc = 0.000\n",
      "Step 3000, Loss= 0.0697, Train Acc= 0.000, Test Acc = 0.001\n",
      "Step 4000, Loss= 0.0641, Train Acc= 0.016, Test Acc = 0.021\n",
      "Step 5000, Loss= 0.0651, Train Acc= 0.040, Test Acc = 0.036\n",
      "Step 6000, Loss= 0.0562, Train Acc= 0.042, Test Acc = 0.041\n",
      "Step 7000, Loss= 0.0598, Train Acc= 0.051, Test Acc = 0.056\n",
      "Step 8000, Loss= 0.0584, Train Acc= 0.096, Test Acc = 0.060\n",
      "Step 9000, Loss= 0.0555, Train Acc= 0.124, Test Acc = 0.069\n",
      "Step 10000, Loss= 0.0537, Train Acc= 0.148, Test Acc = 0.072\n",
      "Step 11000, Loss= 0.0569, Train Acc= 0.118, Test Acc = 0.073\n",
      "Step 12000, Loss= 0.0482, Train Acc= 0.182, Test Acc = 0.086\n",
      "Step 13000, Loss= 0.0502, Train Acc= 0.141, Test Acc = 0.085\n",
      "Step 14000, Loss= 0.0522, Train Acc= 0.209, Test Acc = 0.097\n",
      "Step 15000, Loss= 0.0556, Train Acc= 0.161, Test Acc = 0.092\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Training Parameters\n",
    "    learn_rate = 0.001\n",
    "    training_steps = 220000\n",
    "    batch_size = 128\n",
    "    display_step = 1000\n",
    "    checkpoint_step = 10000\n",
    "    \n",
    "    # Network Parameters\n",
    "    num_input = 300 \n",
    "    timesteps = 50 \n",
    "    num_hidden = 40 \n",
    "    num_classes = 100 \n",
    "    num_layers = 3\n",
    "    cell = tf.contrib.rnn.LSTMCell\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    lr = tf.placeholder(\"float\")\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([2 * num_hidden, num_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "    }\n",
    "\n",
    "    def RNN(x, weights, biases):\n",
    "        x = tf.unstack(x, timesteps, 1)\n",
    "        initializer = tf.truncated_normal_initializer(stddev=0.18632568, dtype=tf.float32)\n",
    "\n",
    "        # Cerate forward and backward cells\n",
    "        cells_fw = [cell(num_hidden, activation=tf.tanh, \n",
    "                         initializer=initializer) for _ in range(num_layers)]\n",
    "        cells_bw = [cell(num_hidden, activation=tf.tanh,\n",
    "                         initializer=initializer) for _ in range(num_layers)]\n",
    "        \n",
    "        keep_probs = [0.75, 0.65, 0.65]\n",
    "        # Add dropout\n",
    "        cells_fw = [tf.contrib.rnn.DropoutWrapper(cell, \n",
    "                                                  input_keep_prob=keep_prob_\n",
    "                                                 ) for cell, keep_prob_ in zip(cells_fw, keep_probs)]\n",
    "        cells_bw = [tf.contrib.rnn.DropoutWrapper(cell, \n",
    "                                                  input_keep_prob=keep_prob_\n",
    "                                                 ) for cell, keep_prob_ in zip(cells_bw, keep_probs)]\n",
    "        \n",
    "        outputs, _, _ = tf.contrib.rnn.stack_bidirectional_rnn(\n",
    "            cells_fw=cells_fw,\n",
    "            cells_bw=cells_bw,\n",
    "            inputs=x,\n",
    "            dtype=tf.float32)\n",
    "        \n",
    "        return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "    logits = RNN(X, weights, biases)\n",
    "    prediction = tf.nn.sigmoid(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=logits, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "def evaluate_multilabel(y_pred, y_true):\n",
    "    acc = []\n",
    "    for y_pred_tmp, y_true_tmp in zip(y_pred, y_true):\n",
    "        real_ = np.nonzero(y_true_tmp)[0].tolist()\n",
    "        pred_ = np.nonzero(y_pred_tmp)[0].tolist()\n",
    "        if len(real_) == 0:\n",
    "            #means 0 right answers\n",
    "            acc.append(0.0)\n",
    "            continue\n",
    "        acc.append(len(set(real_).intersection(set(pred_))) / len(real_))\n",
    "    return(np.array(acc).mean())\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4)\n",
    "sess = tf.InteractiveSession(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "# with tf.Session(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "batcher = DataBatcher(X_train, y_train, _batch_size=batch_size)\n",
    "\n",
    "for step in range(1, training_steps+1):\n",
    "    batch_x, batch_y = batcher.next_batch()\n",
    "    if step == int(training_steps * 0.75):\n",
    "        learn_rate /= 2\n",
    "        print(\"lr now \" + str(learn_rate))\n",
    "    if step == int(training_steps * 0.90):\n",
    "        learn_rate /= 2\n",
    "        print(\"lr now \" + str(learn_rate))\n",
    "    sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn_rate})\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        # Calculate batch loss and accuracy\n",
    "        loss, pred = sess.run([loss_op, tf.round(prediction)], feed_dict={X: batch_x, Y: batch_y})\n",
    "        \n",
    "        test_out = sess.run(tf.round(prediction), feed_dict={X: X_test})\n",
    "        print(\"Step \" + str(step) + \", Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss) + \", Train Acc= \" + \\\n",
    "              \"{:.3f}\".format(evaluate_multilabel(y_pred=pred, y_true=batch_y)) +  \", Test Acc = \" + \\\n",
    "              \"{:.3f}\".format(evaluate_multilabel(y_pred=test_out, y_true=y_test)))\n",
    "        \n",
    "    if step % checkpoint_step == 0:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, '../models/lstm_big_dropout/lstm_100_classes', global_step=step)\n",
    "        \n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, '../models/lstm_big_dropout/lstm_100_classes', global_step=step)\n",
    "        \n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    save = tf.train.Saver()\n",
    "    save.restore(session, '../models/lstm_big_dropout/lstm_100_classes-40000')\n",
    "    pred = tf.round(tf.nn.sigmoid(logits))\n",
    "    test_preds = session.run(pred, feed_dict={X: X_test})\n",
    "    for num in range(0, test_preds.shape[0]):\n",
    "        real = y_test[num]\n",
    "\n",
    "        real = np.nonzero(real)[0].tolist()\n",
    "        right_num = len(real)\n",
    "        pred = test_preds[num]\n",
    "        pred = np.nonzero(pred)[0].tolist()\n",
    "#         print(real, pred)\n",
    "        print('=================================\\nreal_classes', mlb.classes_[np.array(real)])\n",
    "        if len(pred) > 0:\n",
    "            print('predicted classes', mlb.classes_[np.array(pred)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
