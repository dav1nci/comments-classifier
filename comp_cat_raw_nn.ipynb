{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83897, 18)\n",
      "(47516, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pandas.read_parquet('../data/to_send.pq')\n",
    "print(data.shape)\n",
    "data = data[['description', 'products']][pandas.notnull(data['products'])].copy().reset_index(drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Welch allyn combines its practical understandi...</td>\n",
       "      <td>[power supply, body sub assy, medical, valve b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In  line  with  the  company  s intention  to ...</td>\n",
       "      <td>[imo, advertising materials, point, imo label,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Services redaelli ricambi offers the ability t...</td>\n",
       "      <td>[auto spare parts, tie rod, tie rod end, auto ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STROTHMANN not only delivers suitable mechanic...</td>\n",
       "      <td>[covers non automated, demurrage rules form, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Established\\nin 1991, tien jiang enterprise co...</td>\n",
       "      <td>[rubber, polyester, nylon, boot, support]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  Welch allyn combines its practical understandi...   \n",
       "1  In  line  with  the  company  s intention  to ...   \n",
       "2  Services redaelli ricambi offers the ability t...   \n",
       "3  STROTHMANN not only delivers suitable mechanic...   \n",
       "4  Established\\nin 1991, tien jiang enterprise co...   \n",
       "\n",
       "                                            products  \n",
       "0  [power supply, body sub assy, medical, valve b...  \n",
       "1  [imo, advertising materials, point, imo label,...  \n",
       "2  [auto spare parts, tie rod, tie rod end, auto ...  \n",
       "3  [covers non automated, demurrage rules form, r...  \n",
       "4          [rubber, polyester, nylon, boot, support]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique categories 62194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24213, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "st = LancasterStemmer()\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "stopwords_cached = stopwords.words('english')\n",
    "\n",
    "def clean_categories(x):\n",
    "    result = []\n",
    "    for category in x:\n",
    "        tmp_ = ' '.join([st.stem(word)\n",
    "                         for word in tokenizer.tokenize(category.lower())\n",
    "                         if word not in stopwords_cached])\n",
    "        if tmp_ != '':\n",
    "            result.append(tmp_)\n",
    "    if len(result) == 0:\n",
    "        return np.nan\n",
    "    return np.array(result)\n",
    "\n",
    "data['products'] = data['products'].apply(clean_categories)\n",
    "data = data[data['products'].notnull()].copy().reset_index(drop=True)\n",
    "\n",
    "all_products = []\n",
    "for prod_list in data['products'].values:\n",
    "    all_products += [' '.join(tokenizer.tokenize(product))\n",
    "                     for product in prod_list.tolist()]\n",
    "    \n",
    "counter = Counter(all_products)\n",
    "print('unique categories', len(counter.most_common()))\n",
    "\n",
    "most_common = [product[0] for product in counter.most_common(100)]\n",
    "\n",
    "def filter_categories(x):\n",
    "    new_categories = np.array([product \n",
    "                               for product in x.tolist()\n",
    "                               if product in most_common])\n",
    "    if new_categories.shape[0] == 0:\n",
    "        return np.nan\n",
    "    return new_categories\n",
    "\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "stopwords_cached = stopwords.words('english')\n",
    "\n",
    "def filter_descriptions(text):\n",
    "    cleaned_text = [lemma.lemmatize(token)\n",
    "                      for token in tokenizer.tokenize(text.lower())\n",
    "                      if token not in stopwords_cached]\n",
    "    if len(cleaned_text) == 0:\n",
    "        return np.nan\n",
    "    return ' '.join(cleaned_text)\n",
    "\n",
    "data['products'] = data['products'].apply(filter_categories)\n",
    "data['description'] = data['description'].apply(filter_descriptions)\n",
    "\n",
    "data = data[(data['products'].notnull()) & (data['description'].notnull())].copy().reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welch allyn combine practical understanding cl...</td>\n",
       "      <td>[pow supply, med]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>line company intention support international g...</td>\n",
       "      <td>[imo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>service redaelli ricambi offer ability produce...</td>\n",
       "      <td>[auto spar part, auto spar, spar part]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strothmann delivers suitable mechanical system...</td>\n",
       "      <td>[lin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>established tien jiang enterprise co ltd one s...</td>\n",
       "      <td>[rub, polyest]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>songwei dedicate become benchmark manufacturin...</td>\n",
       "      <td>[elect, med]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>salespider medium inc leading digital medium c...</td>\n",
       "      <td>[gear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>insight trend including weekly round industry ...</td>\n",
       "      <td>[hos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>since behl precision fabricating provided prec...</td>\n",
       "      <td>[car part]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nidec minster corporation world class supplier...</td>\n",
       "      <td>[print, mot]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>qualified efficient structure able affix chall...</td>\n",
       "      <td>[rol, automot part, steel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>offer precision part manufacturing analytical ...</td>\n",
       "      <td>[filt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>come moving r ncommercial load standard tracto...</td>\n",
       "      <td>[machinery]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>established headquartered kiyosu aichi prefect...</td>\n",
       "      <td>[mot vehic, vehic, auto part, mot vehic, mot]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lite co ltd taiwan established may started tak...</td>\n",
       "      <td>[light]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>always detail improvement plan facility change...</td>\n",
       "      <td>[alumin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>codan long proud tradition rubber production m...</td>\n",
       "      <td>[automot spar part, spar part, automot spar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>big as fan ideal air moving machine commercial...</td>\n",
       "      <td>[machin part]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>since ksr industry leader designing engineerin...</td>\n",
       "      <td>[auto part]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>mardec fully absorbed subsidiary pol become mi...</td>\n",
       "      <td>[rub]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>heat treatment wheel product percent cnc machi...</td>\n",
       "      <td>[alumin, wheel, wheel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>kamag transporttechnik located ulm kamag activ...</td>\n",
       "      <td>[spar part]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>whether new car driving year basic car care es...</td>\n",
       "      <td>[cotton, toy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>global presence lister petter iconic british b...</td>\n",
       "      <td>[engin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>long established multi plant group specializin...</td>\n",
       "      <td>[mot vehic, mot, auto part]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>story begin spring today see global partner au...</td>\n",
       "      <td>[wir, steel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>mission number lighting supplier quality innov...</td>\n",
       "      <td>[mot vehic, compon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>automotive aftermarket aa division offer deale...</td>\n",
       "      <td>[brak]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>formed eight chinese factory producing differe...</td>\n",
       "      <td>[pump]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>alfa laval today world leader within key techn...</td>\n",
       "      <td>[wat, oil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24183</th>\n",
       "      <td>since wesem company provided lighting agricult...</td>\n",
       "      <td>[lamp, light]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24184</th>\n",
       "      <td>experience dynamism flexibility determination ...</td>\n",
       "      <td>[tool]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24185</th>\n",
       "      <td>mr founded mr radomir bata rajkovic course tra...</td>\n",
       "      <td>[engin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24186</th>\n",
       "      <td>pleasure present official spare part distribut...</td>\n",
       "      <td>[auto part, spar part, car part]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24187</th>\n",
       "      <td>professional manufacturer automobile engine tr...</td>\n",
       "      <td>[engin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24188</th>\n",
       "      <td>focus developing v v lithium ion starter batte...</td>\n",
       "      <td>[battery]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24189</th>\n",
       "      <td>u u u company carry wholesale trade distributi...</td>\n",
       "      <td>[car part, spar part]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24190</th>\n",
       "      <td>top world auto part pte ltd founded one leadin...</td>\n",
       "      <td>[brak, tool]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24191</th>\n",
       "      <td>challenging situation require inspired action ...</td>\n",
       "      <td>[battery, battery]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24192</th>\n",
       "      <td>interpump group largest manufacturer professio...</td>\n",
       "      <td>[pump]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24193</th>\n",
       "      <td>ag company american grease stick company locat...</td>\n",
       "      <td>[fit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24194</th>\n",
       "      <td>dorukmak llc established year bursa turkey tot...</td>\n",
       "      <td>[rub]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24195</th>\n",
       "      <td>boncraft boncraft major manufacturer trading s...</td>\n",
       "      <td>[hand tool]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24196</th>\n",
       "      <td>gameroil l based merida extremadura capital so...</td>\n",
       "      <td>[oil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24197</th>\n",
       "      <td>fujikoki global supplier refrigerant control v...</td>\n",
       "      <td>[valv]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24198</th>\n",
       "      <td>huanyu hose co ltd established mr jang yuan fa...</td>\n",
       "      <td>[hos, tub]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24199</th>\n",
       "      <td>autoforniture bipa srl started business local ...</td>\n",
       "      <td>[spar part]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24200</th>\n",
       "      <td>company new generation bio developed new vibra...</td>\n",
       "      <td>[pow, filt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24201</th>\n",
       "      <td>elke italian company established specialized p...</td>\n",
       "      <td>[oil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24202</th>\n",
       "      <td>krasnobrodzka racing one biggest factory polan...</td>\n",
       "      <td>[engin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24203</th>\n",
       "      <td>w u fcrth u cfell love u vehicle diagnosticsan...</td>\n",
       "      <td>[connect]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24204</th>\n",
       "      <td>indurapower leader r engineering manufacturing...</td>\n",
       "      <td>[battery, pow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24205</th>\n",
       "      <td>e guan known expert importing exporting car pa...</td>\n",
       "      <td>[auto part]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24206</th>\n",
       "      <td>rubatech mfg co pvt ltd premiere provider auto...</td>\n",
       "      <td>[rub]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24207</th>\n",
       "      <td>hifi filter partner filtration hifi filter gro...</td>\n",
       "      <td>[filt, filt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24208</th>\n",
       "      <td>rolling industrial co ltd formed supply qualit...</td>\n",
       "      <td>[lamp]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24209</th>\n",
       "      <td>since company expanded range becoming part ind...</td>\n",
       "      <td>[pump]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24210</th>\n",
       "      <td>quite simply best property tire wholesaler rim...</td>\n",
       "      <td>[softw]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24211</th>\n",
       "      <td>company fogang xin tongshi auto radiator co lt...</td>\n",
       "      <td>[heat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24212</th>\n",
       "      <td>air top italia company specialized cabin air f...</td>\n",
       "      <td>[filt, filt]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24213 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             description  \\\n",
       "0      welch allyn combine practical understanding cl...   \n",
       "1      line company intention support international g...   \n",
       "2      service redaelli ricambi offer ability produce...   \n",
       "3      strothmann delivers suitable mechanical system...   \n",
       "4      established tien jiang enterprise co ltd one s...   \n",
       "5      songwei dedicate become benchmark manufacturin...   \n",
       "6      salespider medium inc leading digital medium c...   \n",
       "7      insight trend including weekly round industry ...   \n",
       "8      since behl precision fabricating provided prec...   \n",
       "9      nidec minster corporation world class supplier...   \n",
       "10     qualified efficient structure able affix chall...   \n",
       "11     offer precision part manufacturing analytical ...   \n",
       "12     come moving r ncommercial load standard tracto...   \n",
       "13     established headquartered kiyosu aichi prefect...   \n",
       "14     lite co ltd taiwan established may started tak...   \n",
       "15     always detail improvement plan facility change...   \n",
       "16     codan long proud tradition rubber production m...   \n",
       "17     big as fan ideal air moving machine commercial...   \n",
       "18     since ksr industry leader designing engineerin...   \n",
       "19     mardec fully absorbed subsidiary pol become mi...   \n",
       "20     heat treatment wheel product percent cnc machi...   \n",
       "21     kamag transporttechnik located ulm kamag activ...   \n",
       "22     whether new car driving year basic car care es...   \n",
       "23     global presence lister petter iconic british b...   \n",
       "24     long established multi plant group specializin...   \n",
       "25     story begin spring today see global partner au...   \n",
       "26     mission number lighting supplier quality innov...   \n",
       "27     automotive aftermarket aa division offer deale...   \n",
       "28     formed eight chinese factory producing differe...   \n",
       "29     alfa laval today world leader within key techn...   \n",
       "...                                                  ...   \n",
       "24183  since wesem company provided lighting agricult...   \n",
       "24184  experience dynamism flexibility determination ...   \n",
       "24185  mr founded mr radomir bata rajkovic course tra...   \n",
       "24186  pleasure present official spare part distribut...   \n",
       "24187  professional manufacturer automobile engine tr...   \n",
       "24188  focus developing v v lithium ion starter batte...   \n",
       "24189  u u u company carry wholesale trade distributi...   \n",
       "24190  top world auto part pte ltd founded one leadin...   \n",
       "24191  challenging situation require inspired action ...   \n",
       "24192  interpump group largest manufacturer professio...   \n",
       "24193  ag company american grease stick company locat...   \n",
       "24194  dorukmak llc established year bursa turkey tot...   \n",
       "24195  boncraft boncraft major manufacturer trading s...   \n",
       "24196  gameroil l based merida extremadura capital so...   \n",
       "24197  fujikoki global supplier refrigerant control v...   \n",
       "24198  huanyu hose co ltd established mr jang yuan fa...   \n",
       "24199  autoforniture bipa srl started business local ...   \n",
       "24200  company new generation bio developed new vibra...   \n",
       "24201  elke italian company established specialized p...   \n",
       "24202  krasnobrodzka racing one biggest factory polan...   \n",
       "24203  w u fcrth u cfell love u vehicle diagnosticsan...   \n",
       "24204  indurapower leader r engineering manufacturing...   \n",
       "24205  e guan known expert importing exporting car pa...   \n",
       "24206  rubatech mfg co pvt ltd premiere provider auto...   \n",
       "24207  hifi filter partner filtration hifi filter gro...   \n",
       "24208  rolling industrial co ltd formed supply qualit...   \n",
       "24209  since company expanded range becoming part ind...   \n",
       "24210  quite simply best property tire wholesaler rim...   \n",
       "24211  company fogang xin tongshi auto radiator co lt...   \n",
       "24212  air top italia company specialized cabin air f...   \n",
       "\n",
       "                                            products  \n",
       "0                                  [pow supply, med]  \n",
       "1                                              [imo]  \n",
       "2             [auto spar part, auto spar, spar part]  \n",
       "3                                              [lin]  \n",
       "4                                     [rub, polyest]  \n",
       "5                                       [elect, med]  \n",
       "6                                             [gear]  \n",
       "7                                              [hos]  \n",
       "8                                         [car part]  \n",
       "9                                       [print, mot]  \n",
       "10                        [rol, automot part, steel]  \n",
       "11                                            [filt]  \n",
       "12                                       [machinery]  \n",
       "13     [mot vehic, vehic, auto part, mot vehic, mot]  \n",
       "14                                           [light]  \n",
       "15                                          [alumin]  \n",
       "16      [automot spar part, spar part, automot spar]  \n",
       "17                                     [machin part]  \n",
       "18                                       [auto part]  \n",
       "19                                             [rub]  \n",
       "20                            [alumin, wheel, wheel]  \n",
       "21                                       [spar part]  \n",
       "22                                     [cotton, toy]  \n",
       "23                                           [engin]  \n",
       "24                       [mot vehic, mot, auto part]  \n",
       "25                                      [wir, steel]  \n",
       "26                               [mot vehic, compon]  \n",
       "27                                            [brak]  \n",
       "28                                            [pump]  \n",
       "29                                        [wat, oil]  \n",
       "...                                              ...  \n",
       "24183                                  [lamp, light]  \n",
       "24184                                         [tool]  \n",
       "24185                                        [engin]  \n",
       "24186               [auto part, spar part, car part]  \n",
       "24187                                        [engin]  \n",
       "24188                                      [battery]  \n",
       "24189                          [car part, spar part]  \n",
       "24190                                   [brak, tool]  \n",
       "24191                             [battery, battery]  \n",
       "24192                                         [pump]  \n",
       "24193                                          [fit]  \n",
       "24194                                          [rub]  \n",
       "24195                                    [hand tool]  \n",
       "24196                                          [oil]  \n",
       "24197                                         [valv]  \n",
       "24198                                     [hos, tub]  \n",
       "24199                                    [spar part]  \n",
       "24200                                    [pow, filt]  \n",
       "24201                                          [oil]  \n",
       "24202                                        [engin]  \n",
       "24203                                      [connect]  \n",
       "24204                                 [battery, pow]  \n",
       "24205                                    [auto part]  \n",
       "24206                                          [rub]  \n",
       "24207                                   [filt, filt]  \n",
       "24208                                         [lamp]  \n",
       "24209                                         [pump]  \n",
       "24210                                        [softw]  \n",
       "24211                                         [heat]  \n",
       "24212                                   [filt, filt]  \n",
       "\n",
       "[24213 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim\n",
    "\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "stopwords_cached = stopwords.words('english')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def read_corpus(tokens_only=False):\n",
    "#     with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "    for line, id_ in zip(data['description'].values, \n",
    "                         data.index.values):\n",
    "        \n",
    "        # For training data, add tags\n",
    "        yield gensim.models.doc2vec.TaggedDocument([token\n",
    "                                                    for token in tokenizer.tokenize(line)], \n",
    "                                                   ['_*' + str(id_)])\n",
    "        \n",
    "train_corpus = list(read_corpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PV-DBOW \n",
    "\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(dm=0, dbow_words=1, size=200, window=8, min_count=19, iter=10, workers=7)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "model.save('../models/company_descriptions.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.doc2vec.Doc2Vec.load('../models/company_descriptions.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = []\n",
    "y_ = []\n",
    "\n",
    "for i in data.index.values:\n",
    "    X_.append(model.docvecs['_*' + str(i)])\n",
    "    y_.append(data.loc[i]['products'])\n",
    "X_ = np.array(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_ = mlb.fit_transform(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['alumin', 'assembl', 'auto part', 'auto spar', 'auto spar part',\n",
       "       'automot part', 'automot spar', 'automot spar part', 'bal',\n",
       "       'battery', 'bear', 'book', 'bottl', 'brak', 'cabl', 'cap', 'car',\n",
       "       'car part', 'cast', 'ceram', 'ceram til', 'chair', 'coil',\n",
       "       'compon', 'compress', 'comput', 'connect', 'cotton', 'cov',\n",
       "       'cylind', 'display', 'elect', 'electron', 'engin', 'fabr', 'film',\n",
       "       'filt', 'fit', 'flo', 'furnit', 'gear', 'glass', 'hand',\n",
       "       'hand tool', 'heat', 'hos', 'hous', 'hydra', 'imo', 'indust',\n",
       "       'lamp', 'light', 'lin', 'machin part', 'machinery',\n",
       "       'machinery part', 'med', 'mold', 'mot', 'mot vehic', 'nut', 'oil',\n",
       "       'pip', 'plast part', 'plat', 'polyest', 'pow', 'pow supply',\n",
       "       'press', 'print', 'pump', 'pvc', 'ring', 'rol', 'rub', 'screw',\n",
       "       'seat', 'sheet', 'softw', 'solid wood', 'spar part', 'stainless',\n",
       "       'stainless steel', 'steel', 'system', 'tabl', 'til', 'tir', 'tool',\n",
       "       'toy', 'tub', 'tyr', 'valv', 'vehic', 'wat', 'wheel', 'win', 'wir',\n",
       "       'wood furnit', 'x'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class DataBatcher():\n",
    "    def __init__(self, _X, _y, _batch_size=30):\n",
    "        self._X = _X\n",
    "        self._y = _y\n",
    "        self._batch_size = _batch_size\n",
    "        self._resplit = True\n",
    "        self._num_examples = self._y.shape[0]\n",
    "    \n",
    "    def next_batch(self):\n",
    "        if self._resplit:\n",
    "#             print('splitting')\n",
    "            perm0 = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm0)\n",
    "            self._batches_indexes = np.array_split(perm0, math.ceil(perm0.shape[0] / self._batch_size))\n",
    "#             print(self._batches_indexes)\n",
    "            self._batch_counter = -1\n",
    "            self._resplit = False\n",
    "\n",
    "        self._batch_counter += 1\n",
    "        if self._batches_indexes[self._batch_counter].shape[0] < self._batch_size:\n",
    "#             print('hstacking')\n",
    "            self._resplit = True\n",
    "            ind = self._batch_counter\n",
    "#             self._batch_counter = -1\n",
    "            missing_num = self._batch_size - self._batches_indexes[ind].shape[0]\n",
    "            return self._X[np.hstack((self._batches_indexes[ind], self._batches_indexes[0][:missing_num]))],\\\n",
    "                   self._y[np.hstack((self._batches_indexes[ind], self._batches_indexes[0][:missing_num]))]\n",
    "        \n",
    "        return self._X[self._batches_indexes[self._batch_counter]], self._y[self._batches_indexes[self._batch_counter]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss= 0.7124, Train Acc= 0.467, Test Acc = 0.426\n",
      "Step 1000, Loss= 0.0705, Train Acc= 0.000, Test Acc = 0.000\n",
      "Step 2000, Loss= 0.0678, Train Acc= 0.008, Test Acc = 0.005\n",
      "Step 3000, Loss= 0.0623, Train Acc= 0.066, Test Acc = 0.021\n",
      "Step 4000, Loss= 0.0657, Train Acc= 0.031, Test Acc = 0.036\n",
      "Step 5000, Loss= 0.0581, Train Acc= 0.094, Test Acc = 0.042\n",
      "Step 6000, Loss= 0.0585, Train Acc= 0.062, Test Acc = 0.047\n",
      "Step 7000, Loss= 0.0619, Train Acc= 0.076, Test Acc = 0.054\n",
      "Step 8000, Loss= 0.0552, Train Acc= 0.148, Test Acc = 0.060\n",
      "Step 9000, Loss= 0.0534, Train Acc= 0.115, Test Acc = 0.065\n",
      "Step 10000, Loss= 0.0543, Train Acc= 0.098, Test Acc = 0.071\n",
      "Step 11000, Loss= 0.0516, Train Acc= 0.105, Test Acc = 0.071\n",
      "Step 12000, Loss= 0.0529, Train Acc= 0.123, Test Acc = 0.072\n",
      "Step 13000, Loss= 0.0511, Train Acc= 0.118, Test Acc = 0.074\n",
      "Step 14000, Loss= 0.0478, Train Acc= 0.168, Test Acc = 0.078\n",
      "Step 15000, Loss= 0.0492, Train Acc= 0.171, Test Acc = 0.079\n",
      "Step 16000, Loss= 0.0512, Train Acc= 0.167, Test Acc = 0.079\n",
      "Step 17000, Loss= 0.0469, Train Acc= 0.224, Test Acc = 0.079\n",
      "Step 18000, Loss= 0.0437, Train Acc= 0.170, Test Acc = 0.083\n",
      "Step 19000, Loss= 0.0476, Train Acc= 0.247, Test Acc = 0.082\n",
      "Step 20000, Loss= 0.0507, Train Acc= 0.125, Test Acc = 0.083\n",
      "Step 21000, Loss= 0.0463, Train Acc= 0.226, Test Acc = 0.083\n",
      "Step 22000, Loss= 0.0464, Train Acc= 0.215, Test Acc = 0.085\n",
      "Step 23000, Loss= 0.0477, Train Acc= 0.201, Test Acc = 0.084\n",
      "Step 24000, Loss= 0.0386, Train Acc= 0.236, Test Acc = 0.086\n",
      "Step 25000, Loss= 0.0402, Train Acc= 0.248, Test Acc = 0.086\n",
      "Step 26000, Loss= 0.0392, Train Acc= 0.250, Test Acc = 0.083\n",
      "Step 27000, Loss= 0.0415, Train Acc= 0.294, Test Acc = 0.090\n",
      "Step 28000, Loss= 0.0418, Train Acc= 0.217, Test Acc = 0.085\n",
      "Step 29000, Loss= 0.0456, Train Acc= 0.171, Test Acc = 0.089\n",
      "Step 30000, Loss= 0.0393, Train Acc= 0.281, Test Acc = 0.086\n",
      "Step 31000, Loss= 0.0393, Train Acc= 0.266, Test Acc = 0.086\n",
      "Step 32000, Loss= 0.0368, Train Acc= 0.275, Test Acc = 0.088\n",
      "Step 33000, Loss= 0.0398, Train Acc= 0.285, Test Acc = 0.087\n",
      "Step 34000, Loss= 0.0390, Train Acc= 0.288, Test Acc = 0.089\n",
      "Step 35000, Loss= 0.0366, Train Acc= 0.269, Test Acc = 0.088\n",
      "Step 36000, Loss= 0.0338, Train Acc= 0.346, Test Acc = 0.088\n",
      "Step 37000, Loss= 0.0416, Train Acc= 0.279, Test Acc = 0.087\n",
      "Step 38000, Loss= 0.0383, Train Acc= 0.340, Test Acc = 0.090\n",
      "Step 39000, Loss= 0.0407, Train Acc= 0.285, Test Acc = 0.086\n",
      "Step 40000, Loss= 0.0360, Train Acc= 0.345, Test Acc = 0.089\n",
      "Step 41000, Loss= 0.0373, Train Acc= 0.302, Test Acc = 0.089\n",
      "Step 42000, Loss= 0.0361, Train Acc= 0.332, Test Acc = 0.088\n",
      "Step 43000, Loss= 0.0378, Train Acc= 0.308, Test Acc = 0.090\n",
      "Step 44000, Loss= 0.0358, Train Acc= 0.305, Test Acc = 0.089\n",
      "Step 45000, Loss= 0.0322, Train Acc= 0.406, Test Acc = 0.089\n",
      "Step 46000, Loss= 0.0327, Train Acc= 0.359, Test Acc = 0.088\n",
      "Step 47000, Loss= 0.0409, Train Acc= 0.327, Test Acc = 0.090\n",
      "Step 48000, Loss= 0.0354, Train Acc= 0.350, Test Acc = 0.089\n",
      "Step 49000, Loss= 0.0384, Train Acc= 0.322, Test Acc = 0.090\n",
      "Step 50000, Loss= 0.0352, Train Acc= 0.319, Test Acc = 0.087\n",
      "Step 51000, Loss= 0.0319, Train Acc= 0.424, Test Acc = 0.088\n",
      "Step 52000, Loss= 0.0336, Train Acc= 0.363, Test Acc = 0.091\n",
      "Step 53000, Loss= 0.0354, Train Acc= 0.352, Test Acc = 0.090\n",
      "Step 54000, Loss= 0.0346, Train Acc= 0.382, Test Acc = 0.089\n",
      "Step 55000, Loss= 0.0329, Train Acc= 0.413, Test Acc = 0.090\n",
      "Step 56000, Loss= 0.0322, Train Acc= 0.443, Test Acc = 0.092\n",
      "Step 57000, Loss= 0.0404, Train Acc= 0.374, Test Acc = 0.089\n",
      "Step 58000, Loss= 0.0331, Train Acc= 0.375, Test Acc = 0.091\n",
      "Step 59000, Loss= 0.0321, Train Acc= 0.430, Test Acc = 0.092\n",
      "Step 60000, Loss= 0.0321, Train Acc= 0.382, Test Acc = 0.091\n",
      "Step 61000, Loss= 0.0321, Train Acc= 0.427, Test Acc = 0.091\n",
      "Step 62000, Loss= 0.0303, Train Acc= 0.375, Test Acc = 0.091\n",
      "Step 63000, Loss= 0.0337, Train Acc= 0.405, Test Acc = 0.088\n",
      "Step 64000, Loss= 0.0333, Train Acc= 0.406, Test Acc = 0.091\n",
      "Step 65000, Loss= 0.0311, Train Acc= 0.417, Test Acc = 0.090\n",
      "Step 66000, Loss= 0.0349, Train Acc= 0.423, Test Acc = 0.090\n",
      "Step 67000, Loss= 0.0348, Train Acc= 0.391, Test Acc = 0.091\n",
      "Step 68000, Loss= 0.0340, Train Acc= 0.380, Test Acc = 0.090\n",
      "Step 69000, Loss= 0.0349, Train Acc= 0.397, Test Acc = 0.092\n",
      "Step 70000, Loss= 0.0324, Train Acc= 0.454, Test Acc = 0.093\n",
      "Step 71000, Loss= 0.0322, Train Acc= 0.433, Test Acc = 0.092\n",
      "Step 72000, Loss= 0.0359, Train Acc= 0.449, Test Acc = 0.090\n",
      "Step 73000, Loss= 0.0385, Train Acc= 0.408, Test Acc = 0.091\n",
      "Step 74000, Loss= 0.0325, Train Acc= 0.410, Test Acc = 0.092\n",
      "Step 75000, Loss= 0.0287, Train Acc= 0.451, Test Acc = 0.090\n",
      "Step 76000, Loss= 0.0317, Train Acc= 0.452, Test Acc = 0.090\n",
      "Step 77000, Loss= 0.0329, Train Acc= 0.414, Test Acc = 0.092\n",
      "Step 78000, Loss= 0.0323, Train Acc= 0.361, Test Acc = 0.093\n",
      "Step 79000, Loss= 0.0292, Train Acc= 0.469, Test Acc = 0.091\n",
      "Step 80000, Loss= 0.0358, Train Acc= 0.407, Test Acc = 0.094\n",
      "Step 81000, Loss= 0.0319, Train Acc= 0.375, Test Acc = 0.095\n",
      "Step 82000, Loss= 0.0321, Train Acc= 0.388, Test Acc = 0.095\n",
      "Step 83000, Loss= 0.0277, Train Acc= 0.521, Test Acc = 0.094\n",
      "Step 84000, Loss= 0.0325, Train Acc= 0.410, Test Acc = 0.093\n",
      "Step 85000, Loss= 0.0282, Train Acc= 0.427, Test Acc = 0.093\n",
      "Step 86000, Loss= 0.0337, Train Acc= 0.380, Test Acc = 0.094\n",
      "Step 87000, Loss= 0.0336, Train Acc= 0.386, Test Acc = 0.094\n",
      "Step 88000, Loss= 0.0277, Train Acc= 0.502, Test Acc = 0.095\n",
      "Step 89000, Loss= 0.0327, Train Acc= 0.430, Test Acc = 0.097\n",
      "Step 90000, Loss= 0.0256, Train Acc= 0.466, Test Acc = 0.095\n",
      "Step 91000, Loss= 0.0308, Train Acc= 0.483, Test Acc = 0.094\n",
      "Step 92000, Loss= 0.0314, Train Acc= 0.512, Test Acc = 0.093\n",
      "Step 93000, Loss= 0.0319, Train Acc= 0.464, Test Acc = 0.096\n",
      "Step 94000, Loss= 0.0263, Train Acc= 0.514, Test Acc = 0.096\n",
      "Step 95000, Loss= 0.0289, Train Acc= 0.507, Test Acc = 0.098\n",
      "Step 96000, Loss= 0.0321, Train Acc= 0.469, Test Acc = 0.094\n",
      "Step 97000, Loss= 0.0260, Train Acc= 0.503, Test Acc = 0.094\n",
      "Step 98000, Loss= 0.0312, Train Acc= 0.434, Test Acc = 0.091\n",
      "Step 99000, Loss= 0.0320, Train Acc= 0.382, Test Acc = 0.096\n",
      "Step 100000, Loss= 0.0300, Train Acc= 0.467, Test Acc = 0.095\n",
      "Step 101000, Loss= 0.0300, Train Acc= 0.476, Test Acc = 0.095\n",
      "Step 102000, Loss= 0.0310, Train Acc= 0.439, Test Acc = 0.094\n",
      "Step 103000, Loss= 0.0320, Train Acc= 0.411, Test Acc = 0.097\n",
      "Step 104000, Loss= 0.0315, Train Acc= 0.470, Test Acc = 0.096\n",
      "Step 105000, Loss= 0.0284, Train Acc= 0.393, Test Acc = 0.094\n",
      "Step 106000, Loss= 0.0331, Train Acc= 0.415, Test Acc = 0.095\n",
      "Step 107000, Loss= 0.0296, Train Acc= 0.427, Test Acc = 0.095\n",
      "Step 108000, Loss= 0.0330, Train Acc= 0.467, Test Acc = 0.097\n",
      "Step 109000, Loss= 0.0304, Train Acc= 0.500, Test Acc = 0.096\n",
      "Step 110000, Loss= 0.0311, Train Acc= 0.452, Test Acc = 0.096\n",
      "Step 111000, Loss= 0.0329, Train Acc= 0.377, Test Acc = 0.096\n",
      "Step 112000, Loss= 0.0267, Train Acc= 0.445, Test Acc = 0.096\n",
      "Step 113000, Loss= 0.0285, Train Acc= 0.516, Test Acc = 0.097\n",
      "Step 114000, Loss= 0.0357, Train Acc= 0.429, Test Acc = 0.097\n",
      "Step 115000, Loss= 0.0299, Train Acc= 0.393, Test Acc = 0.096\n",
      "Step 116000, Loss= 0.0293, Train Acc= 0.482, Test Acc = 0.096\n",
      "Step 117000, Loss= 0.0244, Train Acc= 0.529, Test Acc = 0.098\n",
      "Step 118000, Loss= 0.0291, Train Acc= 0.505, Test Acc = 0.097\n",
      "Step 119000, Loss= 0.0345, Train Acc= 0.405, Test Acc = 0.099\n",
      "Step 120000, Loss= 0.0305, Train Acc= 0.447, Test Acc = 0.095\n",
      "Step 121000, Loss= 0.0300, Train Acc= 0.499, Test Acc = 0.097\n",
      "Step 122000, Loss= 0.0324, Train Acc= 0.385, Test Acc = 0.096\n",
      "Step 123000, Loss= 0.0286, Train Acc= 0.504, Test Acc = 0.097\n",
      "Step 124000, Loss= 0.0312, Train Acc= 0.436, Test Acc = 0.096\n",
      "Step 125000, Loss= 0.0264, Train Acc= 0.482, Test Acc = 0.096\n",
      "Step 126000, Loss= 0.0285, Train Acc= 0.516, Test Acc = 0.098\n",
      "Step 127000, Loss= 0.0290, Train Acc= 0.475, Test Acc = 0.098\n",
      "Step 128000, Loss= 0.0267, Train Acc= 0.568, Test Acc = 0.098\n",
      "Step 129000, Loss= 0.0257, Train Acc= 0.560, Test Acc = 0.098\n",
      "Step 130000, Loss= 0.0297, Train Acc= 0.500, Test Acc = 0.099\n",
      "Step 131000, Loss= 0.0339, Train Acc= 0.427, Test Acc = 0.100\n",
      "Step 132000, Loss= 0.0234, Train Acc= 0.543, Test Acc = 0.096\n",
      "Step 133000, Loss= 0.0247, Train Acc= 0.572, Test Acc = 0.099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 134000, Loss= 0.0252, Train Acc= 0.520, Test Acc = 0.097\n",
      "Step 135000, Loss= 0.0275, Train Acc= 0.457, Test Acc = 0.097\n",
      "Step 136000, Loss= 0.0296, Train Acc= 0.530, Test Acc = 0.099\n",
      "Step 137000, Loss= 0.0310, Train Acc= 0.451, Test Acc = 0.098\n",
      "Step 138000, Loss= 0.0289, Train Acc= 0.521, Test Acc = 0.100\n",
      "Step 139000, Loss= 0.0284, Train Acc= 0.484, Test Acc = 0.097\n",
      "Step 140000, Loss= 0.0278, Train Acc= 0.496, Test Acc = 0.099\n",
      "Step 141000, Loss= 0.0295, Train Acc= 0.491, Test Acc = 0.095\n",
      "Step 142000, Loss= 0.0265, Train Acc= 0.528, Test Acc = 0.099\n",
      "Step 143000, Loss= 0.0333, Train Acc= 0.495, Test Acc = 0.097\n",
      "Step 144000, Loss= 0.0288, Train Acc= 0.462, Test Acc = 0.096\n",
      "Step 145000, Loss= 0.0255, Train Acc= 0.565, Test Acc = 0.096\n",
      "Step 146000, Loss= 0.0255, Train Acc= 0.410, Test Acc = 0.097\n",
      "Step 147000, Loss= 0.0282, Train Acc= 0.472, Test Acc = 0.099\n",
      "Step 148000, Loss= 0.0260, Train Acc= 0.507, Test Acc = 0.096\n",
      "Step 149000, Loss= 0.0309, Train Acc= 0.403, Test Acc = 0.095\n",
      "lr now 0.0005\n",
      "Step 150000, Loss= 0.0297, Train Acc= 0.488, Test Acc = 0.095\n",
      "Step 151000, Loss= 0.0281, Train Acc= 0.452, Test Acc = 0.099\n",
      "Step 152000, Loss= 0.0285, Train Acc= 0.495, Test Acc = 0.098\n",
      "Step 153000, Loss= 0.0239, Train Acc= 0.555, Test Acc = 0.097\n",
      "Step 154000, Loss= 0.0268, Train Acc= 0.512, Test Acc = 0.099\n",
      "Step 155000, Loss= 0.0276, Train Acc= 0.571, Test Acc = 0.099\n",
      "Step 156000, Loss= 0.0277, Train Acc= 0.486, Test Acc = 0.100\n",
      "Step 157000, Loss= 0.0262, Train Acc= 0.509, Test Acc = 0.097\n",
      "Step 158000, Loss= 0.0251, Train Acc= 0.560, Test Acc = 0.097\n",
      "Step 159000, Loss= 0.0298, Train Acc= 0.507, Test Acc = 0.098\n",
      "Step 160000, Loss= 0.0276, Train Acc= 0.493, Test Acc = 0.100\n",
      "Step 161000, Loss= 0.0273, Train Acc= 0.553, Test Acc = 0.099\n",
      "Step 162000, Loss= 0.0296, Train Acc= 0.534, Test Acc = 0.097\n",
      "Step 163000, Loss= 0.0290, Train Acc= 0.567, Test Acc = 0.101\n",
      "Step 164000, Loss= 0.0302, Train Acc= 0.438, Test Acc = 0.098\n",
      "Step 165000, Loss= 0.0280, Train Acc= 0.532, Test Acc = 0.099\n",
      "Step 166000, Loss= 0.0238, Train Acc= 0.604, Test Acc = 0.099\n",
      "Step 167000, Loss= 0.0230, Train Acc= 0.566, Test Acc = 0.098\n",
      "Step 168000, Loss= 0.0265, Train Acc= 0.553, Test Acc = 0.099\n",
      "Step 169000, Loss= 0.0293, Train Acc= 0.509, Test Acc = 0.098\n",
      "Step 170000, Loss= 0.0263, Train Acc= 0.556, Test Acc = 0.098\n",
      "Step 171000, Loss= 0.0303, Train Acc= 0.436, Test Acc = 0.099\n",
      "Step 172000, Loss= 0.0249, Train Acc= 0.544, Test Acc = 0.100\n",
      "Step 173000, Loss= 0.0253, Train Acc= 0.533, Test Acc = 0.099\n",
      "Step 174000, Loss= 0.0227, Train Acc= 0.582, Test Acc = 0.099\n",
      "Step 175000, Loss= 0.0264, Train Acc= 0.456, Test Acc = 0.097\n",
      "Step 176000, Loss= 0.0221, Train Acc= 0.591, Test Acc = 0.097\n",
      "Step 177000, Loss= 0.0263, Train Acc= 0.516, Test Acc = 0.098\n",
      "Step 178000, Loss= 0.0278, Train Acc= 0.488, Test Acc = 0.098\n",
      "Step 179000, Loss= 0.0254, Train Acc= 0.573, Test Acc = 0.099\n",
      "lr now 0.00025\n",
      "Step 180000, Loss= 0.0297, Train Acc= 0.521, Test Acc = 0.100\n",
      "Step 181000, Loss= 0.0248, Train Acc= 0.533, Test Acc = 0.101\n",
      "Step 182000, Loss= 0.0274, Train Acc= 0.452, Test Acc = 0.100\n",
      "Step 183000, Loss= 0.0262, Train Acc= 0.495, Test Acc = 0.101\n",
      "Step 184000, Loss= 0.0227, Train Acc= 0.617, Test Acc = 0.099\n",
      "Step 185000, Loss= 0.0253, Train Acc= 0.625, Test Acc = 0.099\n",
      "Step 186000, Loss= 0.0266, Train Acc= 0.558, Test Acc = 0.099\n",
      "Step 187000, Loss= 0.0289, Train Acc= 0.558, Test Acc = 0.099\n",
      "Step 188000, Loss= 0.0293, Train Acc= 0.458, Test Acc = 0.101\n",
      "Step 189000, Loss= 0.0293, Train Acc= 0.507, Test Acc = 0.099\n",
      "Step 190000, Loss= 0.0229, Train Acc= 0.618, Test Acc = 0.099\n",
      "Step 191000, Loss= 0.0270, Train Acc= 0.641, Test Acc = 0.099\n",
      "Step 192000, Loss= 0.0299, Train Acc= 0.547, Test Acc = 0.101\n",
      "Step 193000, Loss= 0.0244, Train Acc= 0.593, Test Acc = 0.100\n",
      "Step 194000, Loss= 0.0279, Train Acc= 0.540, Test Acc = 0.099\n",
      "Step 195000, Loss= 0.0253, Train Acc= 0.601, Test Acc = 0.100\n",
      "Step 196000, Loss= 0.0255, Train Acc= 0.477, Test Acc = 0.100\n",
      "Step 197000, Loss= 0.0247, Train Acc= 0.524, Test Acc = 0.099\n",
      "Step 198000, Loss= 0.0278, Train Acc= 0.521, Test Acc = 0.099\n",
      "Step 199000, Loss= 0.0256, Train Acc= 0.533, Test Acc = 0.101\n",
      "Step 200000, Loss= 0.0285, Train Acc= 0.494, Test Acc = 0.100\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Parameters\n",
    "    learn_rate = 0.001\n",
    "    num_steps = 200000\n",
    "    batch_size = 128\n",
    "    display_step = 1000\n",
    "\n",
    "    # Network Parameters\n",
    "#     n_hidden_1 = 60 # 1st layer number of neurons\n",
    "    n_hidden_2 = 70 # 2nd layer number of neurons\n",
    "    num_input = 200 # MNIST data input (img shape: 28*28)\n",
    "    num_classes = 100 # MNIST total classes (0-9 digits)\n",
    "\n",
    "    # Graph input\n",
    "    X = tf.placeholder(\"float\", [None, num_input])\n",
    "    y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    lr = tf.placeholder(\"float\")\n",
    "\n",
    "    # Define the neural network\n",
    "    def simple_nn(x):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "#         initializer = tf.contrib.layers.xavier_initializer(uniform=False, seed=None, dtype=tf.float32)\n",
    "        initializer = tf.truncated_normal_initializer(stddev=0.18632568, dtype=tf.float32)\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=0.01)\n",
    "        layer_1 = tf.layers.dense(inputs=x, units=n_hidden_2, activation=tf.tanh, \n",
    "                                  kernel_initializer=initializer, kernel_regularizer=regularizer)\n",
    "        layer_2 = tf.layers.dense(inputs=layer_1, units=n_hidden_2, activation=tf.tanh, \n",
    "                                  kernel_initializer=initializer, kernel_regularizer=regularizer)\n",
    "        layer_3 = tf.layers.dense(inputs=layer_2, units=n_hidden_2, activation=tf.tanh, \n",
    "                                  kernel_initializer=initializer, kernel_regularizer=regularizer)\n",
    "        layer_4 = tf.layers.dense(inputs=layer_3, units=n_hidden_2, activation=tf.tanh, \n",
    "                                  kernel_initializer=initializer, kernel_regularizer=regularizer)\n",
    "        # Output fully connected layer with a neuron for each class\n",
    "        out_layer = tf.layers.dense(layer_4, num_classes, activation=None, \n",
    "                                  kernel_initializer=initializer, kernel_regularizer=regularizer)\n",
    "        return out_layer\n",
    "\n",
    "    logits = simple_nn(X)\n",
    "    prediction = tf.nn.sigmoid(logits)\n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "def evaluate_multilabel(y_pred, y_true):\n",
    "    acc = []\n",
    "    for y_pred_tmp, y_true_tmp in zip(y_pred, y_true):\n",
    "        real_ = np.nonzero(y_true_tmp)[0].tolist()\n",
    "        pred_ = np.nonzero(y_pred_tmp)[0].tolist()\n",
    "        if len(real_) == 0:\n",
    "            #means 0 right answers\n",
    "            acc.append(0.0)\n",
    "            continue\n",
    "        acc.append(len(set(real_).intersection(set(pred_))) / len(real_))\n",
    "    return(np.array(acc).mean())\n",
    "\n",
    "# Start training\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4)\n",
    "sess = tf.InteractiveSession(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.2, random_state=42)\n",
    "batcher = DataBatcher(X_train, y_train, _batch_size=batch_size)\n",
    "\n",
    "\n",
    "for step in range(1, num_steps + 1):\n",
    "    batch_x, batch_y = batcher.next_batch()\n",
    "    # Run optimization op (backprop)\n",
    "    if step == int(num_steps * 0.75):\n",
    "        learn_rate /= 2\n",
    "        print(\"lr now \" + str(learn_rate))\n",
    "    if step == int(num_steps * 0.90):\n",
    "        learn_rate /= 2\n",
    "        print(\"lr now \" + str(learn_rate))\n",
    "#     print(\"before run train_op\")\n",
    "    sess.run(train_op, feed_dict={X: batch_x, y: batch_y, lr: learn_rate})\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        # Calculate batch loss and accuracy\n",
    "        loss, pred = sess.run([loss_op, tf.round(prediction)], feed_dict={X: batch_x, y: batch_y})\n",
    "        \n",
    "        test_out = sess.run(tf.round(prediction), feed_dict={X: X_test})\n",
    "        print(\"Step \" + str(step) + \", Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss) + \", Train Acc= \" + \\\n",
    "              \"{:.3f}\".format(evaluate_multilabel(y_pred=pred, y_true=batch_y)) +  \", Test Acc = \" + \\\n",
    "              \"{:.3f}\".format(evaluate_multilabel(y_pred=test_out, y_true=y_test)))\n",
    "        \n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ = sess.run(tf.round(prediction), feed_dict={X: X_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[1]['products']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['seat']\n",
    "def filter_categories(cat_list):\n",
    "    return set(cat_list).intersection(query) == set(query)\n",
    "data[data['products'].apply(filter_categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/categ_preprocessed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
